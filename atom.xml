<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Notes</title>
  
  <subtitle>little notes</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-06-12T13:44:14.075Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>秋水一色</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hbase-shell2</title>
    <link href="http://example.com/2022/06/11/Hbase-shell2/"/>
    <id>http://example.com/2022/06/11/Hbase-shell2/</id>
    <published>2022-06-10T16:00:00.000Z</published>
    <updated>2022-06-12T13:44:14.075Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Hbase-shell"><a href="#一、Hbase-shell" class="headerlink" title="一、Hbase shell"></a>一、Hbase shell</h2><h3 id="1、Region信息观察"><a href="#1、Region信息观察" class="headerlink" title="1、Region信息观察"></a>1、Region信息观察</h3><h4 id="创建表指定命名空间"><a href="#创建表指定命名空间" class="headerlink" title="创建表指定命名空间"></a>创建表指定命名空间</h4><blockquote><p>在创建表的时候可以选择创建到bigdata17这个namespace中，如何实现呢？<br>使用这种格式即可：‘命名空间名称:表名’<br>针对default这个命名空间，在使用的时候可以省略不写</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">create &#x27;bigdata17:t1&#x27;,&#x27;info&#x27;,&#x27;level&#x27;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/Dow6xBpHlRzQ8PG.png" alt="image-20220612212944281"></p><blockquote><p>此时使用list查看所有的表</p></blockquote><p><img src="https://s2.loli.net/2022/06/12/CxKkf4Wui2Smh1A.png" alt="image-20220612212956679"></p><blockquote><p>如果只想查看bigdata17这个命名空间中的表，如何实现呢？<br>可以使用命令list_namespace_tables</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">list_namespace_tables &#x27;n1&#x27;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/hnEFiVldA1Ksr6W.png" alt="image-20220612213004703"></p><blockquote><p>查看region中的某列簇数据</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase hfile -p -f /hbase/data/default/tbl_user/92994712513a45baaa12b72117dda5e5/info/d84e2013791845968917d876e2b438a5</span><br></pre></td></tr></table></figure><h4 id="1-1-查看表的所有region"><a href="#1-1-查看表的所有region" class="headerlink" title="1.1    查看表的所有region"></a>1.1    查看表的所有region</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">list_regions &#x27;表名&#x27;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/bY5Oj38yBf4mnwL.png" alt="image-20220612213103879"></p><h4 id="1-2-强制将表切分出来一个region"><a href="#1-2-强制将表切分出来一个region" class="headerlink" title="1.2    强制将表切分出来一个region"></a>1.2    强制将表切分出来一个region</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">split &#x27;表名&#x27;,&#x27;行键&#x27;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/yaqm2QU1oNlZRLE.png" alt="image-20220612213116364"></p><blockquote><p>但是在页面上可以看到三个：过一会会自动的把原来的删除</p></blockquote><p><img src="D:/bigdata%25E5%259F%25B9%25E8%25AE%25AD/Hbase/day02/Hbase%25E5%25AD%25A6%25E4%25B9%25A0%25EF%25BC%2588%25E4%25BA%258C%25EF%25BC%2589.assets/image-20220609215721140.png" alt="image-20220609215721140"></p><h4 id="1-2-查看某一行在哪个region中"><a href="#1-2-查看某一行在哪个region中" class="headerlink" title="1.2    查看某一行在哪个region中"></a>1.2    查看某一行在哪个region中</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">locate_region &#x27;表名&#x27;,&#x27;行键&#x27;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/GAjNmPi9SRoqJWU.png" alt="image-20220612213129838"></p><blockquote><p>可以hbase hfile -p -f xxxx 查看一下</p></blockquote><h3 id="2、预分region解决热点问题"><a href="#2、预分region解决热点问题" class="headerlink" title="2、预分region解决热点问题"></a>2、预分region解决热点问题</h3><blockquote><p>row设计的一个关键点是查询维度</p><p>(在建表的时候根据具体的查询业务  设计rowkey   预拆分)</p><p>在默认的拆分策略中 ,region的大小达到一定的阈值以后才会进行拆分,并且拆分的region在同一个regionserver中 ,只有达到负载均衡的时机时才会进行region重分配!并且开始如果有大量的数据进行插入操作,那么并发就会集中在单个RS中, 形成热点问题,所以如果有并发插入的时候尽量避免热点问题 ,应当预划分 Region的rowkeyRange范围 ,在建表的时候就指定预region范围 </p></blockquote><blockquote><p>查看命令使用(指定4个切割点，就会有5个region)</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">help &#x27;create&#x27;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/onVxkTuJROsG2hE.png" alt="image-20220612213148593"></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">create &#x27;tb_split&#x27;,&#x27;cf&#x27;,SPLITS =&gt; [&#x27;e&#x27;,&#x27;h&#x27;,&#x27;l&#x27;,&#x27;r&#x27;]</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">list_regions &#x27;tb_split&#x27;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/sS8UjNJrlLAb5TQ.png" alt="image-20220612213201806"></p><blockquote><p>添加数据试试</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">put &#x27;tb_split&#x27;,&#x27;c001&#x27;,&#x27;cf:name&#x27;,&#x27;first&#x27;</span><br><span class="line">put &#x27;tb_split&#x27;,&#x27;f001&#x27;,&#x27;cf:name&#x27;,&#x27;second&#x27;</span><br><span class="line">put &#x27;tb_split&#x27;,&#x27;z001&#x27;,&#x27;cf:name&#x27;,&#x27;last&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>hbase hfile -p –f xxxx 查看数据</p></blockquote><blockquote><p>如果没有数据，因为数据还在内存中，需要手动刷新内存到HDFS中，以HFile的形式存储</p></blockquote><h3 id="3、总结（写一个文档总结回顾）"><a href="#3、总结（写一个文档总结回顾）" class="headerlink" title="3、总结（写一个文档总结回顾）"></a>3、总结（写一个文档总结回顾）</h3><h3 id="4、日志查看"><a href="#4、日志查看" class="headerlink" title="4、日志查看"></a>4、日志查看</h3><blockquote><p>演示不启动hdfs 就启动hbase</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">日志目录：</span><br><span class="line">/usr/local/soft/hbase-1.7.1/logs</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/KIdYakN6uzUgGTb.png" alt="image-20220612213213973"></p><blockquote><p>start-all.sh发现HMaster没启动，hbase shell客户端也可以正常访问</p><p>再启动hbase就好了</p></blockquote><h3 id="5、scan进阶使用"><a href="#5、scan进阶使用" class="headerlink" title="5、scan进阶使用"></a>5、scan进阶使用</h3><blockquote><p>查看所有的命名空间</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">list_namespace</span><br></pre></td></tr></table></figure><blockquote><p>查看某个命名空间下的所有表</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">list_namespace_tables &#x27;default&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>修改命名空间,设置一个属性</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alter_namespace &#x27;bigdata17&#x27;,&#123;METHOD=&gt;&#x27;set&#x27;,&#x27;author&#x27;=&gt;&#x27;wyh&#x27;&#125;</span><br></pre></td></tr></table></figure><blockquote><p>查看命名空间属性</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">describe_namespace &#x27;bigdata17&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>删除一个属性</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alter_namespace &#x27;bigdata17&#x27;,&#123;METHOD=&gt;&#x27;unset&#x27;, NAME=&gt;&#x27;author&#x27;&#125;</span><br></pre></td></tr></table></figure><blockquote><p>删除一个命名空间</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">drop_namespace &#x27;bigdata17&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>创建一张表</p></blockquote><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">create <span class="string">&#x27;teacher&#x27;</span>,<span class="string">&#x27;cf&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>添加数据</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">put &#x27;teacher&#x27;,&#x27;tid0001&#x27;,&#x27;cf:tid&#x27;,1</span><br><span class="line">put &#x27;teacher&#x27;,&#x27;tid0002&#x27;,&#x27;cf:tid&#x27;,2</span><br><span class="line">put &#x27;teacher&#x27;,&#x27;tid0003&#x27;,&#x27;cf:tid&#x27;,3</span><br><span class="line">put &#x27;teacher&#x27;,&#x27;tid0004&#x27;,&#x27;cf:tid&#x27;,4</span><br><span class="line">put &#x27;teacher&#x27;,&#x27;tid0005&#x27;,&#x27;cf:tid&#x27;,5</span><br><span class="line">put &#x27;teacher&#x27;,&#x27;tid0006&#x27;,&#x27;cf:tid&#x27;,6</span><br></pre></td></tr></table></figure><blockquote><p>显示三行数据</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan &#x27;teacher&#x27;,&#123;LIMIT=&gt;3&#125;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">put &#x27;teacher&#x27;,&#x27;tid00001&#x27;,&#x27;cf:name&#x27;,&#x27;wyh&#x27;</span><br><span class="line">scan &#x27;teacher&#x27;,&#123;LIMIT=&gt;3&#125;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/lE4SaAX3FCNtGOj.png" alt="image-20220612213223835"></p><blockquote><p>从后查三行</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan &#x27;teacher&#x27;,&#123;LIMIT=&gt;3,REVERSED=&gt;true&#125;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/ky7KZhD36bYxpCI.png" alt="image-20220612213231718"></p><blockquote><p>查看包含指定列的行</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan &#x27;teacher&#x27;,&#123;LIMIT=&gt;3,COLUMNS=&gt;[&#x27;cf:name&#x27;]&#125;</span><br></pre></td></tr></table></figure><blockquote><p>简化写法：</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan &#x27;teacher&#x27;,LIMIT=&gt;3</span><br></pre></td></tr></table></figure><blockquote><p>在已有的值后面追加值</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">append &#x27;teacher&#x27;,&#x27;tid0006&#x27;,&#x27;cf:name&#x27;,&#x27;123&#x27;</span><br></pre></td></tr></table></figure><h3 id="6、get进阶使用"><a href="#6、get进阶使用" class="headerlink" title="6、get进阶使用"></a>6、get进阶使用</h3><blockquote><p>简单使用，获取某一行数据</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">get &#x27;teacher&#x27;,&#x27;tid0001&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>获取某一行的某个列簇</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">get &#x27;teacher&#x27;,&#x27;tid0001&#x27;,&#x27;cf&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>获取某一行的某一列（属性 ）</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">get &#x27;teacher&#x27;,&#x27;tid0001&#x27;,&#x27;cf:name&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>可以新增一个列簇数据测试</p></blockquote><blockquote><p><strong>查看历史版本</strong></p><p>1、修改表可以存储多个版本</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alter &#x27;teacher&#x27;,NAME=&gt;&#x27;cf&#x27;,VERSIONS=&gt;3</span><br></pre></td></tr></table></figure><blockquote><p>2、put四次相同rowkey和列的数据</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">put &#x27;teacher&#x27;,&#x27;tid0001&#x27;,&#x27;cf:name&#x27;,&#x27;xiaohu1&#x27;</span><br><span class="line">put &#x27;teacher&#x27;,&#x27;tid0001&#x27;,&#x27;cf:name&#x27;,&#x27;xiaohu2&#x27;</span><br><span class="line">put &#x27;teacher&#x27;,&#x27;tid0001&#x27;,&#x27;cf:name&#x27;,&#x27;xiaohu3&#x27;</span><br><span class="line">put &#x27;teacher&#x27;,&#x27;tid0001&#x27;,&#x27;cf:name&#x27;,&#x27;xiaohu4&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>3、查看历史数据，默认是最新的</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">get &#x27;teacher&#x27;,&#x27;tid0001&#x27;,&#123;COLUMN=&gt;&#x27;cf:name&#x27;,VERSIONS=&gt;2&#125;</span><br></pre></td></tr></table></figure><blockquote><p>修改列簇的过期时间 TTL单位是秒，这个时间是与插入的时间比较，而不是现在开始60s</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alter &#x27;teacher&#x27;,&#123;NAME=&gt;&#x27;cf2&#x27;,TTL=&gt;&#x27;60&#x27;&#125;</span><br></pre></td></tr></table></figure><h3 id="7、插入时间指定时间戳"><a href="#7、插入时间指定时间戳" class="headerlink" title="7、插入时间指定时间戳"></a>7、插入时间指定时间戳</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">put &#x27;teacher&#x27;,&#x27;tid0007&#x27;,&#x27;cf2:job&#x27;,&#x27;bigdata17&#x27;,1654845442790</span><br></pre></td></tr></table></figure><blockquote><p>画图理解这个操作在实际生产的作用</p></blockquote><h3 id="8、delete-只能删除一个单元格，不能删除列簇"><a href="#8、delete-只能删除一个单元格，不能删除列簇" class="headerlink" title="8、delete(只能删除一个单元格，不能删除列簇)"></a>8、delete(只能删除一个单元格，不能删除列簇)</h3><blockquote><p>删除某一列</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">delete &#x27;teacher&#x27;,&#x27;tid0004&#x27;,&#x27;cf:tid&#x27;</span><br></pre></td></tr></table></figure><h3 id="9、deleteall-删除不了某个列簇，但是可以删除多个单元格"><a href="#9、deleteall-删除不了某个列簇，但是可以删除多个单元格" class="headerlink" title="9、deleteall(删除不了某个列簇，但是可以删除多个单元格)"></a>9、deleteall(删除不了某个列簇，但是可以删除多个单元格)</h3><blockquote><p>删除一行，如果不指定类簇，删除的是一行中的所有列簇</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">deleteall &#x27;teacher&#x27;,&#x27;tid0006&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>删除单元格</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">deleteall &#x27;teacher&#x27;,&#x27;tid0006&#x27;,&#x27;cf:name&#x27;,&#x27;cf2:job&#x27;</span><br></pre></td></tr></table></figure><h3 id="10、incr和counter"><a href="#10、incr和counter" class="headerlink" title="10、incr和counter"></a>10、incr和counter</h3><blockquote><p>统计表有多少行(<strong>统计的是行键的个数</strong>)</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">count &#x27;teacher&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>新建一个自增的一列</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">incr &#x27;teacher&#x27;,&#x27;tid0001&#x27;,&#x27;cf:cnt&#x27;,1</span><br></pre></td></tr></table></figure><blockquote><p>每操作一次，自增1</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">incr &#x27;teacher&#x27;,&#x27;tid0001&#x27;,&#x27;cf:cnt&#x27;,1</span><br><span class="line">incr &#x27;teacher&#x27;,&#x27;tid0001&#x27;,&#x27;cf:cnt&#x27;,10</span><br><span class="line">incr &#x27;teacher&#x27;,&#x27;tid0001&#x27;,&#x27;cf:cnt&#x27;,100</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/o4DQfV6mOd5gGBY.png" alt="image-20220612213526090"></p><blockquote><p>配合counter取出数据,只能去incr字段</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">get_counter &#x27;teacher&#x27;,&#x27;tid0001&#x27;,&#x27;cf:cnt&#x27;</span><br></pre></td></tr></table></figure><h3 id="11、获取region的分割点，清除数据，快照"><a href="#11、获取region的分割点，清除数据，快照" class="headerlink" title="11、获取region的分割点，清除数据，快照"></a>11、获取region的分割点，清除数据，快照</h3><blockquote><p>获取region的分割点</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">get_splits &#x27;tb_split&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>清除表数据</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">truncate &#x27;teacher&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>拍摄快照</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">snapshot &#x27;tb_split&#x27;,&#x27;tb_split_20220610&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>列出所有快照</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">list_table_snapshots &#x27;tb_split&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>再添加一些数据</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">put &#x27;tb_split&#x27;,&#x27;a001&#x27;,&#x27;cf:name&#x27;,&#x27;wyh&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>恢复快照(先禁用)</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">disable &#x27;tb_split&#x27;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">restore_snapshot &#x27;tb_split_20220610&#x27;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">enable &#x27;tb_split&#x27;</span><br></pre></td></tr></table></figure><h3 id="12-修饰词"><a href="#12-修饰词" class="headerlink" title="12    修饰词"></a>12    修饰词</h3><h5 id="1、修饰词"><a href="#1、修饰词" class="headerlink" title="1、修饰词"></a>1、修饰词</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>, &#123;COLUMNS <span class="operator">=</span><span class="operator">&gt;</span> [ <span class="string">&#x27;列族名1:列名1&#x27;</span>, <span class="string">&#x27;列族名1:列名2&#x27;</span>, ...]&#125;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>, &#123;COLUMNS <span class="operator">=</span><span class="operator">&gt;</span> [ <span class="string">&#x27;info:id&#x27;</span>, <span class="string">&#x27;info:age&#x27;</span>]&#125;</span><br></pre></td></tr></table></figure><h5 id="2、TIMESTAMP-指定时间戳"><a href="#2、TIMESTAMP-指定时间戳" class="headerlink" title="2、TIMESTAMP 指定时间戳"></a>2、TIMESTAMP 指定时间戳</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>,&#123;TIMERANGE<span class="operator">=</span><span class="operator">&gt;</span>[timestamp1, timestamp2]&#125;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>,&#123;TIMERANGE<span class="operator">=</span><span class="operator">&gt;</span>[<span class="number">1551938004321</span>, <span class="number">1551938036450</span>]&#125;</span><br></pre></td></tr></table></figure><h5 id="3、VERSIONS"><a href="#3、VERSIONS" class="headerlink" title="3、VERSIONS"></a>3、VERSIONS</h5><blockquote><p>默认情况下一个列只能存储一个数据，后面如果修改数据就会将原来的覆盖掉，可以通过指定VERSIONS时HBase一列能存储多个值。</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="string">&#x27;tbl_test&#x27;</span>, <span class="string">&#x27;columnFamily1&#x27;</span></span><br><span class="line"><span class="keyword">describe</span> <span class="string">&#x27;tbl_test&#x27;</span></span><br><span class="line"></span><br><span class="line"># 修改列族版本号</span><br><span class="line"><span class="keyword">alter</span> <span class="string">&#x27;tbl_test&#x27;</span>, &#123; NAME<span class="operator">=</span><span class="operator">&gt;</span><span class="string">&#x27;columnFamily1&#x27;</span>, VERSIONS<span class="operator">=</span><span class="operator">&gt;</span><span class="number">3</span> &#125;</span><br><span class="line"></span><br><span class="line">put <span class="string">&#x27;tbl_test&#x27;</span>, <span class="string">&#x27;rowKey1&#x27;</span>, <span class="string">&#x27;columnFamily1:column1&#x27;</span>, <span class="string">&#x27;value1&#x27;</span></span><br><span class="line">put <span class="string">&#x27;tbl_test&#x27;</span>, <span class="string">&#x27;rowKey1&#x27;</span>, <span class="string">&#x27;columnFamily1:column1&#x27;</span>, <span class="string">&#x27;value2&#x27;</span></span><br><span class="line">put <span class="string">&#x27;tbl_test&#x27;</span>, <span class="string">&#x27;rowKey1&#x27;</span>, <span class="string">&#x27;columnFamily1:column1&#x27;</span>, <span class="string">&#x27;value3&#x27;</span></span><br><span class="line"></span><br><span class="line"># 默认返回最新的一条数据</span><br><span class="line"><span class="keyword">get</span> <span class="string">&#x27;tbl_test&#x27;</span>,<span class="string">&#x27;rowKey1&#x27;</span>,<span class="string">&#x27;columnFamily1:column1&#x27;</span></span><br><span class="line"></span><br><span class="line"># 返回<span class="number">3</span>个</span><br><span class="line"><span class="keyword">get</span> <span class="string">&#x27;tbl_test&#x27;</span>,<span class="string">&#x27;rowKey1&#x27;</span>,&#123;<span class="keyword">COLUMN</span><span class="operator">=</span><span class="operator">&gt;</span><span class="string">&#x27;columnFamily1:column1&#x27;</span>, VERSIONS<span class="operator">=</span><span class="operator">&gt;</span><span class="number">3</span>&#125;</span><br><span class="line"># 返回<span class="number">2</span>个</span><br><span class="line"><span class="keyword">get</span> <span class="string">&#x27;tbl_test&#x27;</span>,<span class="string">&#x27;rowKey1&#x27;</span>,&#123;<span class="keyword">COLUMN</span><span class="operator">=</span><span class="operator">&gt;</span><span class="string">&#x27;columnFamily1:column1&#x27;</span>, VERSIONS<span class="operator">=</span><span class="operator">&gt;</span><span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure><h5 id="4、STARTROW"><a href="#4、STARTROW" class="headerlink" title="4、STARTROW"></a>4、STARTROW</h5><blockquote><p>ROWKEY起始行。会先根据这个key定位到region，再向后扫描</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>, &#123; STARTROW <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;行键名&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>, &#123; STARTROW <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;vbirdbest&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><h5 id="5、STOPROW-：截止到STOPROW行，STOPROW行之前的数据，不包括STOPROW这行数据"><a href="#5、STOPROW-：截止到STOPROW行，STOPROW行之前的数据，不包括STOPROW这行数据" class="headerlink" title="5、STOPROW ：截止到STOPROW行，STOPROW行之前的数据，不包括STOPROW这行数据"></a>5、STOPROW ：截止到STOPROW行，STOPROW行之前的数据，不包括STOPROW这行数据</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>, &#123; STOPROW <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;行键名&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>, &#123; STOPROW <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;xiaoming&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><h5 id="6、LIMIT-返回的行数"><a href="#6、LIMIT-返回的行数" class="headerlink" title="6、LIMIT 返回的行数"></a>6、LIMIT 返回的行数</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>, &#123; LIMIT <span class="operator">=</span><span class="operator">&gt;</span> 行数&#125;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>, &#123; LIMIT <span class="operator">=</span><span class="operator">&gt;</span> <span class="number">2</span> &#125;</span><br></pre></td></tr></table></figure><h3 id="13-FILTER条件过滤器"><a href="#13-FILTER条件过滤器" class="headerlink" title="13    FILTER条件过滤器"></a>13    FILTER条件过滤器</h3><blockquote><p>过滤器之间可以使用AND、OR连接多个过滤器。</p></blockquote><h5 id="1、ValueFilter-值过滤器"><a href="#1、ValueFilter-值过滤器" class="headerlink" title="1、ValueFilter 值过滤器"></a>1、ValueFilter 值过滤器</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法：<span class="type">binary</span> 等于某个值</span><br><span class="line">scan <span class="string">&#x27;&#x27;</span>, <span class="keyword">FILTER</span><span class="operator">=</span><span class="operator">&gt;</span>&quot;ValueFilter(=,&#x27;binary:&#x27;)&quot;</span><br><span class="line"># 语法 substring:包含某个值</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>, <span class="keyword">FILTER</span><span class="operator">=</span><span class="operator">&gt;</span>&quot;ValueFilter(=,&#x27;substring:列值&#x27;)&quot;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>, <span class="keyword">FILTER</span><span class="operator">=</span><span class="operator">&gt;</span>&quot;ValueFilter(=, &#x27;binary:26&#x27;)&quot;</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>, <span class="keyword">FILTER</span><span class="operator">=</span><span class="operator">&gt;</span>&quot;ValueFilter(=, &#x27;substring:6&#x27;)&quot;</span><br></pre></td></tr></table></figure><h5 id="2、ColumnPrefixFilter-列名前缀过滤器"><a href="#2、ColumnPrefixFilter-列名前缀过滤器" class="headerlink" title="2、ColumnPrefixFilter 列名前缀过滤器"></a>2、ColumnPrefixFilter 列名前缀过滤器</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法 substring:包含某个值</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>, <span class="keyword">FILTER</span><span class="operator">=</span><span class="operator">&gt;</span>&quot;ColumnPrefixFilter(&#x27;列名前缀&#x27;)&quot;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>, <span class="keyword">FILTER</span><span class="operator">=</span><span class="operator">&gt;</span>&quot;ColumnPrefixFilter(&#x27;birth&#x27;)&quot;</span><br><span class="line"># 通过括号、<span class="keyword">AND</span>和<span class="keyword">OR</span>的条件组合多个过滤器</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>, <span class="keyword">FILTER</span><span class="operator">=</span><span class="operator">&gt;</span>&quot;ColumnPrefixFilter(&#x27;birth&#x27;) AND ValueFilter(=,&#x27;substring:26&#x27;)&quot;</span><br></pre></td></tr></table></figure><h5 id="3、rowKey字典排序"><a href="#3、rowKey字典排序" class="headerlink" title="3、rowKey字典排序"></a>3、rowKey字典排序</h5><blockquote><p>Table中的所有行都是按照row key的字典排序的</p></blockquote><p><img src="https://s2.loli.net/2022/06/12/lHbNsQYjU8TCnMA.png" alt="image-20220612213543115"></p><h2 id="二、JAVA-API"><a href="#二、JAVA-API" class="headerlink" title="二、JAVA API"></a>二、JAVA API</h2><blockquote><p>pom文件</p></blockquote><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">对学习hbase shell的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hbase" scheme="http://example.com/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>Hbase过滤器</title>
    <link href="http://example.com/2022/06/11/Hbase%E8%BF%87%E6%BB%A4%E5%99%A8/"/>
    <id>http://example.com/2022/06/11/Hbase%E8%BF%87%E6%BB%A4%E5%99%A8/</id>
    <published>2022-06-10T16:00:00.000Z</published>
    <updated>2022-06-12T13:43:34.353Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Hbase过滤器"><a href="#Hbase过滤器" class="headerlink" title="Hbase过滤器"></a>Hbase过滤器</h2><blockquote><p>HBase 的基本 API，包括增、删、改、查等。<br>增、删都是相对简单的操作，与传统的 RDBMS 相比，这里的查询操作略显苍白，只能根据特性的行键进行查询（Get）或者根据行键的范围来查询（Scan）。<br>HBase 不仅提供了这些简单的查询，而且提供了更加高级的过滤器（Filter）来查询。</p><p>过滤器可以根据列族、列、版本等更多的条件来对数据进行过滤，</p><p>基于 HBase 本身提供的三维有序（行键，列，版本有序），这些过滤器可以高效地完成查询过滤的任务，带有过滤器条件的 RPC 查询请求会把过滤器分发到各个 RegionServer（这是一个服务端过滤器），这样也可以降低网络传输的压力。</p><p>使用过滤器至少需要两类参数：</p><p><strong>一类是抽象的操作符，另一类是比较器</strong></p></blockquote><h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><ul><li>过滤器的作用是在<strong>服务端</strong>判断数据是否满足条件，然后只将满足条件的数据返回给<strong>客户端</strong></li><li>过滤器的类型很多，但是可以分为三大类：<ul><li>比较过滤器：可应用于rowkey、列簇、列、列值过滤器</li><li>专用过滤器：只能适用于特定的过滤器</li><li>包装过滤器：包装过滤器就是通过包装其他过滤器以实现某些拓展的功能。</li></ul></li></ul><h4 id="比较过滤器"><a href="#比较过滤器" class="headerlink" title="比较过滤器"></a>比较过滤器</h4><blockquote><p>所有比较过滤器均继承自 <code>CompareFilter</code>。创建一个比较过滤器需要两个参数，分别是<strong>比较运算符</strong>和<strong>比较器实例</strong>。</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">CompareFilter</span><span class="params">(<span class="keyword">final</span> CompareOp compareOp,<span class="keyword">final</span> ByteArrayComparable comparator)</span> &#123;</span><br><span class="line">   <span class="built_in">this</span>.compareOp = compareOp;</span><br><span class="line">   <span class="built_in">this</span>.comparator = comparator;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h5 id="比较运算符"><a href="#比较运算符" class="headerlink" title="比较运算符"></a>比较运算符</h5><ul><li><p>LESS  &lt;</p></li><li><p>LESS_OR_EQUAL &lt;&#x3D;</p></li><li><p>EQUAL &#x3D;</p></li><li><p>NOT_EQUAL &lt;&gt;</p></li><li><p>GREATER_OR_EQUAL &gt;&#x3D;</p></li><li><p>GREATER &gt;</p></li><li><p>NO_OP 排除所有</p></li></ul><h5 id="常见的六大比较过滤器"><a href="#常见的六大比较过滤器" class="headerlink" title="常见的六大比较过滤器"></a>常见的六大比较过滤器</h5><h6 id="BinaryComparator"><a href="#BinaryComparator" class="headerlink" title="BinaryComparator"></a>BinaryComparator</h6><blockquote><p>按字节索引顺序比较指定字节数组，采用Bytes.compareTo(byte[])</p></blockquote><h6 id="BinaryPrefixComparator"><a href="#BinaryPrefixComparator" class="headerlink" title="BinaryPrefixComparator"></a>BinaryPrefixComparator</h6><blockquote><p>通BinaryComparator，只是比较左端前缀的数据是否相同</p></blockquote><h6 id="NullComparator"><a href="#NullComparator" class="headerlink" title="NullComparator"></a>NullComparator</h6><blockquote><p>判断给定的是否为空</p></blockquote><h6 id="BitComparator"><a href="#BitComparator" class="headerlink" title="BitComparator"></a>BitComparator</h6><blockquote><p>按位比较</p></blockquote><h6 id="RegexStringComparator"><a href="#RegexStringComparator" class="headerlink" title="RegexStringComparator"></a>RegexStringComparator</h6><blockquote><p>提供一个正则的比较器，仅支持 EQUAL 和非EQUAL</p></blockquote><h6 id="SubstringComparator"><a href="#SubstringComparator" class="headerlink" title="SubstringComparator"></a>SubstringComparator</h6><blockquote><p>判断提供的子串是否出现在中</p></blockquote><h5 id="代码演示"><a href="#代码演示" class="headerlink" title="代码演示"></a>代码演示</h5><h6 id="rowKey过滤器：RowFilter-行键过滤器"><a href="#rowKey过滤器：RowFilter-行键过滤器" class="headerlink" title="rowKey过滤器：RowFilter  行键过滤器"></a>rowKey过滤器：RowFilter  行键过滤器</h6><blockquote><p>通过RowFilter与BinaryComparator过滤比rowKey 1500100010小的所有值出来</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *  行键过滤器</span></span><br><span class="line"><span class="comment">     *  通过RowFilter与BinaryComparator过滤比rowKey 1500100010小的所有值出来</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">RowFilter1</span><span class="params">()</span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//获取表的实例</span></span><br><span class="line">            <span class="type">HTableInterface</span> <span class="variable">students</span> <span class="operator">=</span> conn.getTable(<span class="string">&quot;students&quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="type">BinaryComparator</span> <span class="variable">binaryComparator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BinaryComparator</span>(<span class="string">&quot;1500100010&quot;</span>.getBytes());</span><br><span class="line"></span><br><span class="line">            <span class="comment">//创建一个行键过滤器的对象</span></span><br><span class="line">            <span class="type">RowFilter</span> <span class="variable">rowFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RowFilter</span>(CompareFilter.CompareOp.LESS, binaryComparator);</span><br><span class="line"></span><br><span class="line">            <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">            scan.setFilter(rowFilter);</span><br><span class="line"></span><br><span class="line">            <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> students.getScanner(scan);</span><br><span class="line">            print(scanner);</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *      专门用来打印数据的方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">print</span><span class="params">(ResultScanner scanner)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="type">Result</span> <span class="variable">rs</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="keyword">while</span> ((rs = scanner.next()) != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">id</span> <span class="operator">=</span> Bytes.toString(rs.getRow());</span><br><span class="line"><span class="comment">//                List&lt;Cell&gt; cells = rs.listCells();</span></span><br><span class="line"><span class="comment">//                System.out.print(&quot;id:&quot; + id);</span></span><br><span class="line"><span class="comment">//                System.out.print(&quot;\t&quot;);</span></span><br><span class="line"><span class="comment">//                for (Cell cell : cells) &#123;</span></span><br><span class="line"><span class="comment">////                String s = Bytes.toString(cell.getValue());</span></span><br><span class="line"><span class="comment">//                    String col = Bytes.toString(CellUtil.cloneQualifier(cell));</span></span><br><span class="line"><span class="comment">//                    String s = Bytes.toString(CellUtil.cloneValue(cell));</span></span><br><span class="line"><span class="comment">//                    System.out.print(col + &quot;:&quot; + s);</span></span><br><span class="line"><span class="comment">//                    System.out.print(&quot;\t&quot;);</span></span><br><span class="line"><span class="comment">//                &#125;</span></span><br><span class="line"><span class="comment">//                System.out.println();</span></span><br><span class="line"></span><br><span class="line">            <span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span> Bytes.toString(rs.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;name&quot;</span>.getBytes()));</span><br><span class="line">            <span class="type">String</span> <span class="variable">age</span> <span class="operator">=</span> Bytes.toString(rs.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;age&quot;</span>.getBytes()));</span><br><span class="line">            <span class="type">String</span> <span class="variable">gender</span> <span class="operator">=</span> Bytes.toString(rs.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;gender&quot;</span>.getBytes()));</span><br><span class="line">            <span class="type">String</span> <span class="variable">clazz</span> <span class="operator">=</span> Bytes.toString(rs.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;clazz&quot;</span>.getBytes()));</span><br><span class="line">            System.out.println(<span class="string">&quot;学号：&quot;</span> + id + <span class="string">&quot;,姓名：&quot;</span> + name + <span class="string">&quot;,年龄：&quot;</span> + age + <span class="string">&quot;,性别：&quot;</span> + gender + <span class="string">&quot;,班级：&quot;</span> + clazz);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h6 id="列簇过滤器：FamilyFilter"><a href="#列簇过滤器：FamilyFilter" class="headerlink" title="列簇过滤器：FamilyFilter"></a>列簇过滤器：FamilyFilter</h6><blockquote><p>通过FamilyFilter与SubstringComparator查询列簇名包含in的所有列簇下面的数据</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *      列簇过滤器案例1：通过FamilyFilter与SubstringComparator查询列簇名包含in的所有列簇下面的数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">FamilyFilter1</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//获取表的实例</span></span><br><span class="line">        <span class="type">HTableInterface</span> <span class="variable">students</span> <span class="operator">=</span> conn.getTable(<span class="string">&quot;students&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个比较器对象</span></span><br><span class="line">        <span class="comment">//只要列簇名中包含了in，就把该列簇下的所有列查询出来</span></span><br><span class="line">        <span class="type">SubstringComparator</span> <span class="variable">substringComparator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SubstringComparator</span>(<span class="string">&quot;in&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建列簇过滤器</span></span><br><span class="line">        <span class="type">FamilyFilter</span> <span class="variable">familyFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FamilyFilter</span>(CompareFilter.CompareOp.EQUAL, substringComparator);</span><br><span class="line"></span><br><span class="line">        <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">        scan.setFilter(familyFilter);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取数据</span></span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> students.getScanner(scan);</span><br><span class="line">        print(scanner);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>通过FamilyFilter与 BinaryPrefixComparator 过滤出列簇以i开头的列簇下的所有数据</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 列簇过滤器案例2：通过FamilyFilter与 BinaryPrefixComparator 过滤出列簇以i开头的列簇下的所有数据</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">FamilyFilter2</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//获取表的实例</span></span><br><span class="line">        <span class="type">HTableInterface</span> <span class="variable">students</span> <span class="operator">=</span> conn.getTable(<span class="string">&quot;students&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建前缀比较器</span></span><br><span class="line">        <span class="type">BinaryPrefixComparator</span> <span class="variable">binaryPrefixComparator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BinaryPrefixComparator</span>(<span class="string">&quot;i&quot;</span>.getBytes());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建列簇过滤器</span></span><br><span class="line">        <span class="type">FamilyFilter</span> <span class="variable">familyFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FamilyFilter</span>(CompareFilter.CompareOp.EQUAL, binaryPrefixComparator);</span><br><span class="line"></span><br><span class="line">        <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">        scan.setFilter(familyFilter);</span><br><span class="line"></span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> students.getScanner(scan);</span><br><span class="line">        print(scanner);</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="列过滤器：QualifierFilter"><a href="#列过滤器：QualifierFilter" class="headerlink" title="列过滤器：QualifierFilter"></a>列过滤器：QualifierFilter</h6><blockquote><p>通过QualifierFilter与SubstringComparator查询列名包含ge的列的值</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 列过滤器案例1：通过QualifierFilter与SubstringComparator查询列名包含ge的列的值</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">QualifierFilter1</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//获取表的实例</span></span><br><span class="line">        <span class="type">HTableInterface</span> <span class="variable">students</span> <span class="operator">=</span> conn.getTable(<span class="string">&quot;students&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建包含比较器</span></span><br><span class="line">        <span class="comment">//age</span></span><br><span class="line">        <span class="comment">//gender</span></span><br><span class="line">        <span class="type">SubstringComparator</span> <span class="variable">substringComparator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SubstringComparator</span>(<span class="string">&quot;ge&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个列过滤器</span></span><br><span class="line">        <span class="type">QualifierFilter</span> <span class="variable">qualifierFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">QualifierFilter</span>(CompareFilter.CompareOp.EQUAL, substringComparator);</span><br><span class="line"></span><br><span class="line">        <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">        scan.setFilter(qualifierFilter);</span><br><span class="line"></span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> students.getScanner(scan);</span><br><span class="line">        print(scanner);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>过滤出列的名字中 包含 “am” 所有的列 及列的值</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 列过滤器案例2：通过QualifierFilter与SubstringComparator查询列名包含ge的列的值</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">QualifierFilter2</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//获取表的实例</span></span><br><span class="line">        <span class="type">HTableInterface</span> <span class="variable">students</span> <span class="operator">=</span> conn.getTable(<span class="string">&quot;students&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">SubstringComparator</span> <span class="variable">substringComparator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SubstringComparator</span>(<span class="string">&quot;am&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建列过滤器</span></span><br><span class="line">        <span class="type">QualifierFilter</span> <span class="variable">qualifierFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">QualifierFilter</span>(CompareFilter.CompareOp.EQUAL, substringComparator);</span><br><span class="line"></span><br><span class="line">        <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">        scan.setFilter(qualifierFilter);</span><br><span class="line"></span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> students.getScanner(scan);</span><br><span class="line">        print(scanner);</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="列值过滤器：ValueFilter"><a href="#列值过滤器：ValueFilter" class="headerlink" title="列值过滤器：ValueFilter"></a>列值过滤器：ValueFilter</h6><blockquote><p>通过ValueFilter与BinaryPrefixComparator过滤出所有的cell中值以 “张” 开头的学生</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 列值过滤器案例1:通过ValueFilter与BinaryPrefixComparator过滤出所有的cell中值以 &quot;张&quot; 开头的学生</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">ValueFilter1</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//获取表的实例</span></span><br><span class="line">            <span class="type">HTableInterface</span> <span class="variable">students</span> <span class="operator">=</span> conn.getTable(<span class="string">&quot;students&quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//创建前缀比较器</span></span><br><span class="line">            <span class="type">BinaryPrefixComparator</span> <span class="variable">binaryPrefixComparator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BinaryPrefixComparator</span>(<span class="string">&quot;张&quot;</span>.getBytes());</span><br><span class="line"></span><br><span class="line">            <span class="comment">//创建列值过滤器的对象</span></span><br><span class="line">            <span class="type">ValueFilter</span> <span class="variable">valueFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ValueFilter</span>(CompareFilter.CompareOp.EQUAL, binaryPrefixComparator);</span><br><span class="line"></span><br><span class="line">            <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">            scan.setFilter(valueFilter);</span><br><span class="line"></span><br><span class="line">            <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> students.getScanner(scan);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//因为ResultScanner类继承了迭代器</span></span><br><span class="line">            <span class="comment">//使用增强for循环遍历</span></span><br><span class="line"><span class="comment">//            for (Result rs : scanner) &#123;</span></span><br><span class="line"><span class="comment">//                String id = Bytes.toString(rs.getRow());</span></span><br><span class="line"><span class="comment">//                System.out.println(&quot;当前行的rowkey为：&quot; + id);</span></span><br><span class="line"><span class="comment">//                //继续增强for循环得到每一行中的每一个单元格（列）</span></span><br><span class="line"><span class="comment">//                //获取一行中的所有单元格</span></span><br><span class="line"><span class="comment">//                for (Cell cell : rs.listCells()) &#123;</span></span><br><span class="line"><span class="comment">//                    //获取该单元格属于的列簇</span></span><br><span class="line"><span class="comment">//                    String family = Bytes.toString(CellUtil.cloneFamily(cell));</span></span><br><span class="line"><span class="comment">//                    //获取该单元格的列名</span></span><br><span class="line"><span class="comment">//                    String colName = Bytes.toString(CellUtil.cloneQualifier(cell));</span></span><br><span class="line"><span class="comment">//                    //获取该单元格的列值</span></span><br><span class="line"><span class="comment">//                    String value = Bytes.toString(CellUtil.cloneValue(cell));</span></span><br><span class="line"><span class="comment">//                    System.out.println(family + &quot;:&quot; + colName + &quot;的值为：&quot; + value);</span></span><br><span class="line"><span class="comment">//                &#125;</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"></span><br><span class="line">            print(scanner);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><blockquote><p>过滤出文科的学生，只会返回以文科开头的数据列，其他列的数据不符合条件，不会返回</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *      列值过滤器案例2：&gt; 过滤出文科的学生，只会返回以文科开头的数据列，其他列的数据不符合条件，不会返回</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">ValueFilter12</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//获取表的实例</span></span><br><span class="line">        <span class="type">HTableInterface</span> <span class="variable">students</span> <span class="operator">=</span> conn.getTable(<span class="string">&quot;students&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建正则比较器</span></span><br><span class="line">        <span class="type">RegexStringComparator</span> <span class="variable">regexStringComparator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RegexStringComparator</span>(<span class="string">&quot;^文科.*&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建列值过滤器</span></span><br><span class="line">        <span class="type">ValueFilter</span> <span class="variable">valueFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ValueFilter</span>(CompareFilter.CompareOp.EQUAL, regexStringComparator);</span><br><span class="line"></span><br><span class="line">        <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">        scan.setFilter(valueFilter);</span><br><span class="line"></span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> students.getScanner(scan);</span><br><span class="line">        print(scanner);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="专用过滤器"><a href="#专用过滤器" class="headerlink" title="专用过滤器"></a>专用过滤器</h4><h6 id="单列值过滤器：SingleColumnValueFilter"><a href="#单列值过滤器：SingleColumnValueFilter" class="headerlink" title="单列值过滤器：SingleColumnValueFilter"></a>单列值过滤器：SingleColumnValueFilter</h6><blockquote><p>SingleColumnValueFilter会返回满足条件的cell所在行的所有cell的值（即会返回一行数据）</p><p>通过SingleColumnValueFilter与查询文科班所有学生信息</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 单列值过滤器</span></span><br><span class="line"><span class="comment">     * SingleColumnValueFilter会返回满足条件的cell所在行的所有cell的值（即会返回一行数据）</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 通过SingleColumnValueFilter与查询文科班所有学生信息</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">SingleColumnValueFilter</span><span class="params">()</span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//获取表的实例</span></span><br><span class="line">            <span class="type">HTableInterface</span> <span class="variable">students</span> <span class="operator">=</span> conn.getTable(<span class="string">&quot;students&quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//创建一个正则比较器</span></span><br><span class="line">            <span class="type">RegexStringComparator</span> <span class="variable">regexStringComparator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RegexStringComparator</span>(<span class="string">&quot;^文科.*&quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//创建单列值过滤器对象</span></span><br><span class="line">            <span class="type">SingleColumnValueFilter</span> <span class="variable">singleColumnValueFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SingleColumnValueFilter</span>(</span><br><span class="line">                    <span class="string">&quot;info&quot;</span>.getBytes(),</span><br><span class="line">                    <span class="string">&quot;clazz&quot;</span>.getBytes(),</span><br><span class="line">                    CompareFilter.CompareOp.EQUAL,</span><br><span class="line">                    regexStringComparator</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">            <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">            scan.setFilter(singleColumnValueFilter);</span><br><span class="line"></span><br><span class="line">            <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> students.getScanner(scan);</span><br><span class="line">            print(scanner);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 专门用来打印数据的方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">print</span><span class="params">(ResultScanner scanner)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="type">Result</span> <span class="variable">rs</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="keyword">while</span> ((rs = scanner.next()) != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">id</span> <span class="operator">=</span> Bytes.toString(rs.getRow());</span><br><span class="line">            List&lt;Cell&gt; cells = rs.listCells();</span><br><span class="line">            System.out.print(<span class="string">&quot;id:&quot;</span> + id);</span><br><span class="line">            System.out.print(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            <span class="keyword">for</span> (Cell cell : cells) &#123;</span><br><span class="line"><span class="comment">//                String s = Bytes.toString(cell.getValue());</span></span><br><span class="line">                <span class="type">String</span> <span class="variable">col</span> <span class="operator">=</span> Bytes.toString(CellUtil.cloneQualifier(cell));</span><br><span class="line">                <span class="type">String</span> <span class="variable">s</span> <span class="operator">=</span> Bytes.toString(CellUtil.cloneValue(cell));</span><br><span class="line">                System.out.print(col + <span class="string">&quot;:&quot;</span> + s);</span><br><span class="line">                System.out.print(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println();</span><br><span class="line"></span><br><span class="line"><span class="comment">//            String name = Bytes.toString(rs.getValue(&quot;info&quot;.getBytes(), &quot;name&quot;.getBytes()));</span></span><br><span class="line"><span class="comment">//            String age = Bytes.toString(rs.getValue(&quot;info&quot;.getBytes(), &quot;age&quot;.getBytes()));</span></span><br><span class="line"><span class="comment">//            String gender = Bytes.toString(rs.getValue(&quot;info&quot;.getBytes(), &quot;gender&quot;.getBytes()));</span></span><br><span class="line"><span class="comment">//            String clazz = Bytes.toString(rs.getValue(&quot;info&quot;.getBytes(), &quot;clazz&quot;.getBytes()));</span></span><br><span class="line"><span class="comment">//            System.out.println(&quot;学号：&quot; + id + &quot;,姓名：&quot; + name + &quot;,年龄：&quot; + age + &quot;,性别：&quot; + gender + &quot;,班级：&quot; + clazz);</span></span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h6 id="列值排除过滤器：SingleColumnValueExcludeFilter"><a href="#列值排除过滤器：SingleColumnValueExcludeFilter" class="headerlink" title="列值排除过滤器：SingleColumnValueExcludeFilter"></a>列值排除过滤器：SingleColumnValueExcludeFilter</h6><blockquote><p>与SingleColumnValueFilter相反，会排除掉指定的列，其他的列全部返回</p><p>通过SingleColumnValueExcludeFilter与BinaryComparator查询文科一班所有学生信息，最终不返回clazz列</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 列值排除过滤器</span></span><br><span class="line"><span class="comment"> * 与SingleColumnValueFilter相反，会排除掉指定的列，其他的列全部返回</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 通过SingleColumnValueExcludeFilter与BinaryComparator查询文科一班所有学生信息，最终不返回clazz列</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">SingleColumnValueExcludeFilter</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//获取表的实例</span></span><br><span class="line">        <span class="type">HTableInterface</span> <span class="variable">students</span> <span class="operator">=</span> conn.getTable(<span class="string">&quot;students&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个二进制比较器</span></span><br><span class="line">        <span class="type">BinaryComparator</span> <span class="variable">binaryComparator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BinaryComparator</span>(<span class="string">&quot;文科一班&quot;</span>.getBytes());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个列值排除过滤器</span></span><br><span class="line">        <span class="type">SingleColumnValueExcludeFilter</span> <span class="variable">singleColumnValueExcludeFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SingleColumnValueExcludeFilter</span>(</span><br><span class="line">                <span class="string">&quot;info&quot;</span>.getBytes(),</span><br><span class="line">                <span class="string">&quot;clazz&quot;</span>.getBytes(),</span><br><span class="line">                CompareFilter.CompareOp.EQUAL,</span><br><span class="line">                binaryComparator</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">        scan.setFilter(singleColumnValueExcludeFilter);</span><br><span class="line"></span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> students.getScanner(scan);</span><br><span class="line">        print(scanner);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="rowkey前缀过滤器：PrefixFilter"><a href="#rowkey前缀过滤器：PrefixFilter" class="headerlink" title="rowkey前缀过滤器：PrefixFilter"></a>rowkey前缀过滤器：PrefixFilter</h6><blockquote><p>通过PrefixFilter查询以150010008开头的所有前缀的rowkey</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * rowkey前缀过滤器</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 通过PrefixFilter查询以150010008开头的所有前缀的rowkey</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">PrefixFilter</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//获取表的实例</span></span><br><span class="line">        <span class="type">HTableInterface</span> <span class="variable">students</span> <span class="operator">=</span> conn.getTable(<span class="string">&quot;students&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建rowkey前缀过滤器</span></span><br><span class="line">        <span class="type">PrefixFilter</span> <span class="variable">prefixFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PrefixFilter</span>(<span class="string">&quot;150010008&quot;</span>.getBytes());</span><br><span class="line">        <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line"></span><br><span class="line">        scan.setFilter(prefixFilter);</span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> students.getScanner(scan);</span><br><span class="line">        print2(scanner);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="分页过滤器PageFilter"><a href="#分页过滤器PageFilter" class="headerlink" title="分页过滤器PageFilter"></a>分页过滤器PageFilter</h6><blockquote><p>通过PageFilter查询三页的数据，每页10条</p><p>使用PageFilter分页效率比较低，每次都需要扫描前面的数据，直到扫描到所需要查的数据</p><p>可设计一个合理的rowkey来实现分页需求</p></blockquote><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="section"># 注意事项：</span></span><br><span class="line">客户端进行分页查询，需要传递 startRow(起始 RowKey)，知道起始 startRow 后，就可以返回对应的 pageSize 行数据。这里唯一的问题就是，对于第一次查询，显然 startRow 就是表格的第一行数据，但是之后第二次、第三次查询我们并不知道 startRow，只能知道上一次查询的最后一条数据的 RowKey（简单称之为 lastRow）。</span><br><span class="line"></span><br><span class="line">我们不能将 lastRow 作为新一次查询的 startRow 传入，因为 scan 的查询区间是[startRow，endRow) ，即前开后闭区间，这样 startRow 在新的查询也会被返回，这条数据就重复了。</span><br><span class="line"></span><br><span class="line">同时在不使用第三方数据库存储 RowKey 的情况下，我们是无法通过知道 lastRow 的下一个 RowKey 的，因为 RowKey 的设计可能是连续的也有可能是不连续的。</span><br><span class="line"></span><br><span class="line">由于 Hbase 的 RowKey 是按照字典序进行排序的。这种情况下，就可以在 lastRow 后面加上 0 ，作为 startRow 传入，因为按照字典序的规则，某个值加上 0 后的新值，在字典序上一定是这个值的下一个值，对于 HBase 来说下一个 RowKey 在字典序上一定也是等于或者大于这个新值的。</span><br><span class="line"></span><br><span class="line">所以最后传入 lastRow+0，如果等于这个值的 RowKey 存在就从这个值开始 scan,否则从字典序的下一个 RowKey 开始 scan。</span><br><span class="line"></span><br><span class="line">25 个字母以及数字字符，字典排序如下:</span><br><span class="line"></span><br><span class="line">&#x27;0&#x27; &lt; &#x27;1&#x27; &lt; &#x27;2&#x27; &lt; ... &lt; &#x27;9&#x27; &lt; &#x27;a&#x27; &lt; &#x27;b&#x27; &lt; ... &lt; &#x27;z&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>需要注意的是在多台 Regin Services 上执行分页过滤的时候，由于并行执行的过滤器不能共享它们的状态和边界，所以有可能每个过滤器都会在完成扫描前获取了 PageCount 行的结果，这种情况下会返回比分页条数更多的数据，分页过滤器就有失效的可能。</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 分页过滤器</span></span><br><span class="line"><span class="comment"> * 通过PageFilter查询三页的数据，每页10条</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">PageFilter</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//获取表的实例</span></span><br><span class="line">        <span class="type">HTableInterface</span> <span class="variable">students</span> <span class="operator">=</span> conn.getTable(<span class="string">&quot;students&quot;</span>);</span><br><span class="line">        <span class="comment">//定义要查询的页数</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">pageNum</span> <span class="operator">=</span> <span class="number">3</span>;</span><br><span class="line">        <span class="comment">//定义每页的条数</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">pageSize</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">        <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//定义一开始的行</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">current_page_start_row</span> <span class="operator">=</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= pageNum; i++) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;====================当前是第&quot;</span> + i + <span class="string">&quot;页===========================&quot;</span>);</span><br><span class="line">            <span class="comment">//创建一个分页过滤器</span></span><br><span class="line">            <span class="type">PageFilter</span> <span class="variable">pageFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PageFilter</span>(pageSize);</span><br><span class="line">            scan.setFilter(pageFilter);</span><br><span class="line">            <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> students.getScanner(scan);</span><br><span class="line">            <span class="keyword">for</span> (Result rs : scanner) &#123;</span><br><span class="line">                current_page_start_row = Bytes.toString(rs.getRow());</span><br><span class="line">                <span class="comment">//告诉扫描器是从哪一行开始获取数据</span></span><br><span class="line">                scan.withStartRow((current_page_start_row + <span class="number">0</span>).getBytes());</span><br><span class="line">                <span class="type">PageFilter</span> <span class="variable">pageFilter1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PageFilter</span>(pageSize);</span><br><span class="line">                scan.setFilter(pageFilter1);</span><br><span class="line">                <span class="comment">//获取id</span></span><br><span class="line">                <span class="type">String</span> <span class="variable">id</span> <span class="operator">=</span> Bytes.toString(rs.getRow());</span><br><span class="line">                <span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span> Bytes.toString(rs.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;name&quot;</span>.getBytes()));</span><br><span class="line">                <span class="type">String</span> <span class="variable">age</span> <span class="operator">=</span> Bytes.toString(rs.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;age&quot;</span>.getBytes()));</span><br><span class="line">                <span class="type">String</span> <span class="variable">gender</span> <span class="operator">=</span> Bytes.toString(rs.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;gender&quot;</span>.getBytes()));</span><br><span class="line">                <span class="type">String</span> <span class="variable">clazz</span> <span class="operator">=</span> Bytes.toString(rs.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;clazz&quot;</span>.getBytes()));</span><br><span class="line">                System.out.println(<span class="string">&quot;学号：&quot;</span> + id + <span class="string">&quot;,姓名：&quot;</span> + name + <span class="string">&quot;,年龄：&quot;</span> + age + <span class="string">&quot;,性别：&quot;</span> + gender + <span class="string">&quot;,班级：&quot;</span> + clazz);</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="包装过滤器"><a href="#包装过滤器" class="headerlink" title="包装过滤器"></a>包装过滤器</h4><h6 id="SkipFilter过滤器"><a href="#SkipFilter过滤器" class="headerlink" title="SkipFilter过滤器"></a>SkipFilter过滤器</h6><blockquote><p>SkipFilter包装一个过滤器，当被包装的过滤器遇到一个需要过滤的 KeyValue 实例时，则拓展过滤整行数据。下面是一个使用示例：</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义 ValueFilter 过滤器</span></span><br><span class="line"><span class="type">Filter</span> <span class="variable">filter1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ValueFilter</span>(CompareOperator.NOT_EQUAL,</span><br><span class="line">      <span class="keyword">new</span> <span class="title class_">BinaryComparator</span>(Bytes.toBytes(<span class="string">&quot;xxx&quot;</span>)));</span><br><span class="line"><span class="comment">// 使用 SkipFilter 进行包装</span></span><br><span class="line"><span class="type">Filter</span> <span class="variable">filter2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SkipFilter</span>(filter1);</span><br></pre></td></tr></table></figure><h6 id="WhileMatchFilter过滤器"><a href="#WhileMatchFilter过滤器" class="headerlink" title="WhileMatchFilter过滤器"></a>WhileMatchFilter过滤器</h6><blockquote><p>WhileMatchFilter 包装一个过滤器，当被包装的过滤器遇到一个需要过滤的 KeyValue 实例时，WhileMatchFilter 则结束本次扫描，返回已经扫描到的结果。下面是其使用示例：</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">baoZhuang1</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="type">HTableInterface</span> <span class="variable">students</span> <span class="operator">=</span> conn.getTable(<span class="string">&quot;students&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">Filter</span> <span class="variable">filter1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RowFilter</span>(CompareFilter.CompareOp.NOT_EQUAL,<span class="keyword">new</span> <span class="title class_">BinaryComparator</span>(<span class="string">&quot;1500100009&quot;</span>.getBytes()));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//不做包装</span></span><br><span class="line">        <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">        scan.setFilter(filter1);</span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">scanner1</span> <span class="operator">=</span> students.getScanner(scan);</span><br><span class="line">        <span class="type">Result</span> <span class="variable">rs</span> <span class="operator">=</span> scanner1.next();</span><br><span class="line">        <span class="keyword">while</span> (rs != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">id</span> <span class="operator">=</span> Bytes.toString(rs.getRow());</span><br><span class="line">            <span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span> Bytes.toString(rs.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;name&quot;</span>.getBytes()));</span><br><span class="line">            <span class="type">String</span> <span class="variable">age</span> <span class="operator">=</span> Bytes.toString(rs.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;age&quot;</span>.getBytes()));</span><br><span class="line">            <span class="type">String</span> <span class="variable">gender</span> <span class="operator">=</span> Bytes.toString(rs.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;gender&quot;</span>.getBytes()));</span><br><span class="line">            <span class="type">String</span> <span class="variable">clazz</span> <span class="operator">=</span> Bytes.toString(rs.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;clazz&quot;</span>.getBytes()));</span><br><span class="line"></span><br><span class="line">            System.out.println(id + <span class="string">&quot;\t&quot;</span> + name + <span class="string">&quot;\t&quot;</span> + age + <span class="string">&quot;\t&quot;</span> + gender + <span class="string">&quot;\t&quot;</span> + clazz + <span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">            rs = scanner1.next();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;--------------------&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用 WhileMatchFilter 进行包装</span></span><br><span class="line">        <span class="type">Filter</span> <span class="variable">filter2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">WhileMatchFilter</span>(filter1);</span><br><span class="line">        scan.setFilter(filter2);</span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> students.getScanner(scan);</span><br><span class="line">        <span class="type">Result</span> <span class="variable">rs2</span> <span class="operator">=</span> scanner.next();</span><br><span class="line">        <span class="keyword">while</span> (rs2 != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">id</span> <span class="operator">=</span> Bytes.toString(rs2.getRow());</span><br><span class="line">            <span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span> Bytes.toString(rs2.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;name&quot;</span>.getBytes()));</span><br><span class="line">            <span class="type">String</span> <span class="variable">age</span> <span class="operator">=</span> Bytes.toString(rs2.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;age&quot;</span>.getBytes()));</span><br><span class="line">            <span class="type">String</span> <span class="variable">gender</span> <span class="operator">=</span> Bytes.toString(rs2.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;gender&quot;</span>.getBytes()));</span><br><span class="line">            <span class="type">String</span> <span class="variable">clazz</span> <span class="operator">=</span> Bytes.toString(rs2.getValue(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;clazz&quot;</span>.getBytes()));</span><br><span class="line"></span><br><span class="line">            System.out.println(id + <span class="string">&quot;\t&quot;</span> + name + <span class="string">&quot;\t&quot;</span> + age + <span class="string">&quot;\t&quot;</span> + gender + <span class="string">&quot;\t&quot;</span> + clazz + <span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">            rs2 = scanner.next();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="多过滤器综合查询"><a href="#多过滤器综合查询" class="headerlink" title="多过滤器综合查询"></a>多过滤器综合查询</h4><p>以上都是讲解单个过滤器的作用，当需要多个过滤器共同作用于一次查询的时候，就需要使用 <code>FilterList</code>。<code>FilterList</code> 支持通过构造器或者 <code>addFilter</code> 方法传入多个过滤器。</p><blockquote><p>通过运用4种比较器过滤出姓于，年纪大于23岁，性别为女，且是理科的学生。</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：1 通过运用4种比较器过滤出姓于，年纪大于23岁，性别为女，且是理科的学生。</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 正则比较器   RegexStringComparator</span></span><br><span class="line"><span class="comment"> * 包含比较器   SubstringComparator</span></span><br><span class="line"><span class="comment"> * 二进制前缀比较器   BinaryPrefixComparator</span></span><br><span class="line"><span class="comment"> * 二进制比较器      BinaryComparator</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">FilterData1</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//获取表的实例</span></span><br><span class="line">        <span class="type">HTableInterface</span> <span class="variable">students</span> <span class="operator">=</span> conn.getTable(<span class="string">&quot;students&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         *  第一个过滤器，过滤出是理科开头的班级</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">RegexStringComparator</span> <span class="variable">regexStringComparator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RegexStringComparator</span>(<span class="string">&quot;^理科.*&quot;</span>);</span><br><span class="line">        <span class="comment">//单列值过滤器</span></span><br><span class="line">        <span class="type">SingleColumnValueFilter</span> <span class="variable">singleColumnValueFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SingleColumnValueFilter</span>(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;clazz&quot;</span>.getBytes(),</span><br><span class="line">                CompareFilter.CompareOp.EQUAL, regexStringComparator);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 第二个过滤器，过滤出性别是女生的</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="type">SubstringComparator</span> <span class="variable">substringComparator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SubstringComparator</span>(<span class="string">&quot;女&quot;</span>);</span><br><span class="line">        <span class="type">SingleColumnValueFilter</span> <span class="variable">singleColumnValueFilter1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SingleColumnValueFilter</span>(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;gender&quot;</span>.getBytes(),</span><br><span class="line">                CompareFilter.CompareOp.EQUAL, substringComparator);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 第三个过滤器，过滤出年龄大于23岁的</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">BinaryComparator</span> <span class="variable">binaryComparator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BinaryComparator</span>(<span class="string">&quot;20&quot;</span>.getBytes());</span><br><span class="line">        <span class="type">SingleColumnValueFilter</span> <span class="variable">singleColumnValueFilter2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SingleColumnValueFilter</span>(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;age&quot;</span>.getBytes(),</span><br><span class="line">                CompareFilter.CompareOp.GREATER, binaryComparator);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 第四个过滤器，过滤出姓于的学生</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">BinaryPrefixComparator</span> <span class="variable">binaryPrefixComparator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BinaryPrefixComparator</span>(<span class="string">&quot;于&quot;</span>.getBytes());</span><br><span class="line">        <span class="type">SingleColumnValueFilter</span> <span class="variable">singleColumnValueFilter3</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SingleColumnValueFilter</span>(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;name&quot;</span>.getBytes(),</span><br><span class="line">                CompareFilter.CompareOp.EQUAL, binaryPrefixComparator);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//要想实现多个需求同时过滤，就需要创建多个过滤器，添加到一个过滤器列表中</span></span><br><span class="line">        <span class="comment">//然后将过滤器列表传给扫描器scan</span></span><br><span class="line">        <span class="type">FilterList</span> <span class="variable">filterList</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FilterList</span>();</span><br><span class="line">        filterList.addFilter(singleColumnValueFilter);</span><br><span class="line">        filterList.addFilter(singleColumnValueFilter1);</span><br><span class="line">        filterList.addFilter(singleColumnValueFilter2);</span><br><span class="line">        filterList.addFilter(singleColumnValueFilter3);</span><br><span class="line"></span><br><span class="line">        scan.setFilter(filterList);</span><br><span class="line"></span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> students.getScanner(scan);</span><br><span class="line"></span><br><span class="line">        print2(scanner);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>过滤出学号是以15001001开头的文科学生</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 过滤出学号是以15001001开头的文科学生</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">filterData2</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//获取表的实例</span></span><br><span class="line">        <span class="type">HTableInterface</span> <span class="variable">students</span> <span class="operator">=</span> conn.getTable(<span class="string">&quot;students&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         *  创建第一个过滤器，过滤是以15001001开头的rowkey</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">BinaryPrefixComparator</span> <span class="variable">binaryPrefixComparator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BinaryPrefixComparator</span>(<span class="string">&quot;15001001&quot;</span>.getBytes());</span><br><span class="line">        <span class="comment">//创建行键过滤器</span></span><br><span class="line">        <span class="type">RowFilter</span> <span class="variable">rowFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RowFilter</span>(CompareFilter.CompareOp.EQUAL, binaryPrefixComparator);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 创建第二个过滤器，过滤出文科的学生</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">RegexStringComparator</span> <span class="variable">regexStringComparator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RegexStringComparator</span>(<span class="string">&quot;^文科.*&quot;</span>);</span><br><span class="line">        <span class="type">SingleColumnValueFilter</span> <span class="variable">singleColumnValueFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SingleColumnValueFilter</span>(<span class="string">&quot;info&quot;</span>.getBytes(), <span class="string">&quot;clazz&quot;</span>.getBytes(),</span><br><span class="line">                CompareFilter.CompareOp.EQUAL,</span><br><span class="line">                regexStringComparator);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="type">FilterList</span> <span class="variable">filterList</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FilterList</span>();</span><br><span class="line">        filterList.addFilter(rowFilter);</span><br><span class="line">        filterList.addFilter(singleColumnValueFilter);</span><br><span class="line"></span><br><span class="line">        <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">        scan.setFilter(filterList);</span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> students.getScanner(scan);</span><br><span class="line">        print2(scanner);</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>作业：查询文科一班学生总分排名前10的学生（输出：学号，姓名，班级，总分）结果写到hbase</p></blockquote><h4 id="布隆过滤器"><a href="#布隆过滤器" class="headerlink" title="布隆过滤器"></a>布隆过滤器</h4><blockquote><p>本质上布隆过滤器是一种数据结构，比较巧妙的概率型数据结构，特点是高效地插入和查询，可以用来告诉你 “<strong>某样东西一定不存在或者可能存在</strong>”。</p><p>相比于传统的 List、Set、Map 等数据结构，它更高效、占用空间更少，但是缺点是其返回的结果是概率性的，而不是确切的。</p><p>实际上，布隆过滤器广泛应用于<strong>网页黑名单系统</strong>、<strong>垃圾邮件过滤系统</strong>、<strong>爬虫网址判重系统</strong>等，Google 著名的分布式数据库 Bigtable 使用了布隆过滤器来查找不存在的行或列，以减少磁盘查找的 IO 次数，Google Chrome 浏览器使用了布隆过滤器加速安全浏览服务。</p><p>在很多 Key-Value 系统中也使用了布隆过滤器来加快查询过程，如 Hbase，Accumulo，Leveldb，一般而言，Value 保存在磁盘中，访问磁盘需要花费大量时间，然而使用布隆过滤器可以快速判断某个 Key 对应的 Value 是否存在，因此可以避免很多不必要的磁盘 IO 操作。</p><p>通过一个 Hash 函数将一个元素映射成一个位阵列（Bit Array）中的一个点。这样一来，我们只要看看这个点是不是 1 就知道可以集合中有没有它了。这就是布隆过滤器的基本思想。</p></blockquote><h5 id="运用场景"><a href="#运用场景" class="headerlink" title="运用场景"></a>运用场景</h5><blockquote><p>1、目前有 10 亿数量的自然数，乱序排列，需要对其排序。限制条件在 32 位机器上面完成，内存限制为 2G。如何完成？</p><p>2、如何快速在亿级黑名单中快速定位 URL 地址是否在黑名单中？(每条 URL 平均 64 字节)</p><p>3、需要进行用户登陆行为分析，来确定用户的活跃情况？</p><p>4、网络爬虫-如何判断 URL 是否被爬过？</p><p>5、快速定位用户属性（黑名单、白名单等）？</p><p>6、数据存储在磁盘中，如何避免大量的无效 IO？</p><p>7、判断一个元素在亿级数据中是否存在？</p><p>8、缓存穿透。</p></blockquote><h5 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h5><blockquote><p>假设我们有个集合 A，A 中有 n 个元素。利用<strong>k个哈希散列</strong>函数，将A中的每个元素<strong>映射</strong>到一个长度为 a 位的数组 B中的不同位置上，这些位置上的二进制数均设置为 1。如果待检查的元素，经过这 k个哈希散列函数的映射后，发现其 k 个位置上的二进制数<strong>全部为 1</strong>，这个元素很可能属于集合A，反之，<strong>一定不属于集合A</strong>。</p><p>比如我们有 3 个 URL <code>&#123;URL1,URL2,URL3&#125;</code>，通过一个hash 函数把它们映射到一个长度为 16 的数组上，如下：</p></blockquote><p><img src="https://s2.loli.net/2022/06/12/qAcJ4GLkDvZgjzt.png" alt="image-20220612214242813"></p><p>若当前哈希函数为 <code>Hash1()</code>，通过哈希运算映射到数组中，假设<code>Hash1(URL1) = 3</code>，<code>Hash1(URL2) = 6</code>，<code>Hash1(URL3) = 6</code>，如下：</p><p><img src="https://s2.loli.net/2022/06/12/NAO6aL8KT2isjZG.png" alt="image-20220612214254594"></p><blockquote><p>因此，如果我们需要判断<code>URL1</code>是否在这个集合中，则通过<code>Hash(urL1)</code>计算出其下标，并得到其值若为 1 则说明存在。</p><p>由于 Hash 存在哈希冲突，如上面<code>URL2,URL3</code>都定位到一个位置上，假设 Hash 函数是良好的，如果我们的数组长度为 m 个点，那么如果我们想将冲突率降低到例如 <strong>1%<strong>， 这个散列表就只能容纳 <code>m/100</code> 个元素，显然空间利用率就变低了，也就是没法做到</strong>空间有效</strong>（space-efficient）。</p><p>解决方法也简单，就是使用多个 Hash 算法，如果它们有一个说元素不在集合中，那肯定就不在，如下：</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Hash1(URL1) = 3,Hash2(URL1) = 5,Hash3(URL1) = 6</span><br><span class="line">Hash1(URL2) = 5,Hash2(URL2) = 8,Hash3(URL2) = 14</span><br><span class="line">Hash1(URL3) = 4,Hash2(URL3) = 7,Hash3(URL3) = 10</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/2y6qDXUfvg5Ic8k.png" alt="image-20220612214308660"></p><blockquote><p>以上就是布隆过滤器做法，使用了<strong>k个哈希函数</strong>，每个字符串跟 k 个 bit 对应，从而降低了冲突的概率。</p></blockquote><h5 id="误判现象"><a href="#误判现象" class="headerlink" title="误判现象"></a>误判现象</h5><blockquote><p>上面的做法同样存在问题，因为随着增加的值越来越多，被置为 1 的 bit 位也会越来越多，这样某个值即使没有被存储过，但是万一哈希函数返回的三个 bit 位都被其他值置位了 1 ，那么程序还是会判断这个值存在。比如此时来一个不存在的 <code>URL1000</code>，经过哈希计算后，发现 bit 位为下：</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Hash1(URL1000) = 7,Hash2(URL1000) = 8,Hash3(URL1000) = 14</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/vkUtQ6bqMLwFj7c.png" alt="image-20220612214320802"></p><blockquote><p>但是上面这些 bit 位已经被<code>URL1,URL2,URL3</code>置为 1 了，此时程序就会判断 <code>URL1000</code> 值存在。</p><p>这就是布隆过滤器的误判现象，所以，<strong>布隆过滤器判断存在的不一定存在，但是，判断不存在的一定不存在。</strong></p><p>布隆过滤器可精确的代表一个集合，可精确判断某一元素是否在此集合中，精确程度由用户的具体设计决定，达到 100% 的正确是不可能的。但是布隆过滤器的优势在于，<strong>利用很少的空间可以达到较高的精确率</strong>。</p></blockquote><h5 id="控制粒度"><a href="#控制粒度" class="headerlink" title="控制粒度"></a>控制粒度</h5><blockquote><p><strong>a）ROW</strong><br>    根据KeyValue中的行来过滤storefile<br>    举例：假设有2个storefile文件sf1和sf2，<br>        sf1包含kv1（r1 cf：q1 v），kv2（r2 cf：q1 v）<br>        sf2包含kv3（r3 cf：q1 v），kv4（r4 cf：q1 v）<br>        若是设置了CF属性中的bloomfilter为ROW，那么得（r1）时就会过滤sf2，get（r3）就会过滤sf1<br><strong>b）ROWCOL</strong><br>    根据KeyValue中的行+限定符来过滤storefile<br>    举例：假设有2个storefile文件sf1和sf2，<br>        sf1包含kv1（r1 cf：q1 v），kv2（r2 cf：q1 v）<br>        sf2包含kv3（r1 cf：q2 v），kv4（r2 cf：q2 v）<br>        若是设置了CF属性中的布隆过滤器为ROW，不管获得（R1，Q1）仍是获得（R1，Q2），都会读取SF1 + SF2;而若是设置了CF属性中的布隆过滤器为        ROWCOL，那么GET（R1， q1）就会过滤sf2，get（r1，q2）就会过滤sf1<br><strong>c）NO</strong><br>    默认的值，默认不开启布隆过滤器</p></blockquote><h5 id="实现："><a href="#实现：" class="headerlink" title="实现："></a>实现：</h5><blockquote><p>在建立表时加入一个参数就能够了</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">//使用HTableDescriptor类创建一个表对象</span></span><br><span class="line">          <span class="type">HTableDescriptor</span> <span class="variable">students</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HTableDescriptor</span>(<span class="string">&quot;students&quot;</span>);</span><br><span class="line"></span><br><span class="line">          <span class="comment">//在创建表的时候，至少指定一个列簇</span></span><br><span class="line">          <span class="type">HColumnDescriptor</span> <span class="variable">info</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HColumnDescriptor</span>(<span class="string">&quot;info&quot;</span>);</span><br><span class="line">          info.setBloomFilterType(BloomType.ROW); <span class="comment">//&lt;===========================================</span></span><br><span class="line"></span><br><span class="line">          <span class="comment">//将列簇添加到表中</span></span><br><span class="line">          students.addFamily(info);</span><br><span class="line">          <span class="comment">//真正的执行，是由HMaster</span></span><br><span class="line">          <span class="comment">//hAdmin</span></span><br><span class="line">          hAdmin.createTable(students);</span><br><span class="line">          System.out.println(Bytes.toString(students.getName()) + <span class="string">&quot;表 创建成功。。。&quot;</span>);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">          e.printStackTrace();</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">对学习Hbase过滤器的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hbase" scheme="http://example.com/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>Hbase shell1</title>
    <link href="http://example.com/2022/06/10/Hbase-shell1/"/>
    <id>http://example.com/2022/06/10/Hbase-shell1/</id>
    <published>2022-06-09T16:00:00.000Z</published>
    <updated>2022-06-12T13:44:20.182Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、hbase-shell"><a href="#一、hbase-shell" class="headerlink" title="一、hbase shell"></a>一、hbase shell</h2><table><thead><tr><th>命名</th><th>描述</th><th>语法</th></tr></thead><tbody><tr><td>help ‘命名名’</td><td>查看命令的使用描述</td><td>help ‘命令名’</td></tr><tr><td>whoami</td><td>我是谁</td><td>whoami</td></tr><tr><td>version</td><td>返回hbase版本信息</td><td>version</td></tr><tr><td>status</td><td>返回hbase集群的状态信息</td><td>status</td></tr><tr><td>table_help</td><td>查看如何操作表</td><td>table_help</td></tr><tr><td><strong>create</strong></td><td>创建表</td><td>create ‘表名’, ‘列族名1’, ‘列族名2’, ‘列族名N’</td></tr><tr><td><strong>alter</strong></td><td>修改列族</td><td>添加一个列族：alter ‘表名’, ‘列族名’ <br />删除列族：alter ‘表名’, {NAME&#x3D;&gt; ‘列族名’, METHOD&#x3D;&gt; ‘delete’}</td></tr><tr><td>describe</td><td>显示表相关的详细信息</td><td>describe ‘表名’</td></tr><tr><td><strong>list</strong></td><td>列出hbase中存在的所有表</td><td>list</td></tr><tr><td>exists</td><td>测试表是否存在</td><td>exists ‘表名’</td></tr><tr><td><strong>put</strong></td><td>添加或修改的表的值</td><td>put ‘表名’, ‘行键’, ‘列族名’, ‘列值’ <br />put ‘表名’, ‘行键’, ‘列族名:列名’, ‘列值’</td></tr><tr><td><strong>scan</strong></td><td>通过对表的扫描来获取对用的值</td><td>scan ‘表名’<br/>扫描某个列族： scan ‘表名’, {COLUMN&#x3D;&gt;‘列族名’}<br/>扫描某个列族的某个列： scan ‘表名’, {COLUMN&#x3D;&gt;‘列族名:列名’}<br/>查询同一个列族的多个列： scan ‘表名’, {COLUMNS &#x3D;&gt; [ ‘列族名1:列名1’, ‘列族名1:列名2’, …]}</td></tr><tr><td><strong>get</strong></td><td>获取行或单元（cell）的值</td><td>get ‘表名’, ‘行键’ <br />get ‘表名’, ‘行键’, ‘列族名’</td></tr><tr><td>count</td><td>统计表中行的数量</td><td>count ‘表名’</td></tr><tr><td>incr</td><td>增加指定表行或列的值</td><td>incr ‘表名’, ‘行键’, ‘列族:列名’, 步长值</td></tr><tr><td>get_counter</td><td>获取计数器</td><td>get_counter ‘表名’, ‘行键’, ‘列族:列名’</td></tr><tr><td><strong>delete</strong></td><td>删除指定对象的值（可以为表，行，列对应的值，另外也可以指定时间戳的值）</td><td>删除列族的某个列： delete ‘表名’, ‘行键’, ‘列族名:列名’</td></tr><tr><td>deleteall</td><td>删除指定行的所有元素值</td><td>deleteall ‘表名’, ‘行键’</td></tr><tr><td><strong>truncate</strong></td><td>重新创建指定表</td><td>truncate ‘表名’</td></tr><tr><td><strong>enable</strong></td><td>使表有效</td><td>enable ‘表名’</td></tr><tr><td>is_enabled</td><td>是否启用</td><td>is_enabled ‘表名’</td></tr><tr><td><strong>disable</strong></td><td>使表无效</td><td>disable ‘表名’</td></tr><tr><td><strong>is_disabled</strong></td><td>是否无效</td><td>is_disabled ‘表名’</td></tr><tr><td><strong>drop</strong></td><td>删除表</td><td>drop的表必须是disable的 <br />disable ‘表名’ <br />drop ‘表名’</td></tr><tr><td>shutdown</td><td>关闭hbase集群（与exit不同）</td><td></td></tr><tr><td>tools</td><td>列出hbase所支持的工具</td><td></td></tr><tr><td><strong>exit</strong></td><td>退出hbase shell</td><td></td></tr></tbody></table><p>HBase Shell 是官方提供的一组命令，用于操作HBase。如果配置了HBase的<strong>环境变量</strong>了，就可以知己在命令行中输入hbase shell 命令进入命令行。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase shell</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/SXT6AIdLqjEc7CV.png" alt="image-20220608225844374"></p><h3 id="help命令"><a href="#help命令" class="headerlink" title="help命令"></a>help命令</h3><blockquote><p>可以通过 <code>help &#39;命名名称&#39;</code>来查看<strong>命令行</strong>的具体使用，包括命令的作用和用法。<br>通过help ‘hbase’ 命名来查看hbase shell 支持的所有命令，hbase将命令进行分组，其中ddl、dml使用较多。</p></blockquote><p><img src="https://s2.loli.net/2022/06/12/hOgVHLT9FPvBnf6.png" alt="image-20220608230009607"></p><h3 id="1-2-general-类"><a href="#1-2-general-类" class="headerlink" title="1.2    general 类"></a>1.2    general 类</h3><h4 id="1-2-1-显示集群状态status"><a href="#1-2-1-显示集群状态status" class="headerlink" title="1.2.1    显示集群状态status"></a>1.2.1    显示集群状态status</h4><p><img src="https://s2.loli.net/2022/06/12/dCGBNJEuMOrcHI8.png" alt="image-20220612211516897"></p><h4 id="1-2-2-查询数据库版本version"><a href="#1-2-2-查询数据库版本version" class="headerlink" title="1.2.2     查询数据库版本version"></a>1.2.2     查询数据库版本version</h4><p><img src="https://s2.loli.net/2022/06/12/Tgupqb7tkDCvf5J.png" alt="image-20220612211524520"></p><h4 id="1-2-3-显示当前用户与组-whoami"><a href="#1-2-3-显示当前用户与组-whoami" class="headerlink" title="1.2.3    显示当前用户与组 whoami"></a>1.2.3    显示当前用户与组 whoami</h4><p><img src="https://s2.loli.net/2022/06/12/hBgJPd16uKf98Ck.png" alt="image-20220612211550680"></p><h4 id="1-2-4-查看操作表的命令table-help"><a href="#1-2-4-查看操作表的命令table-help" class="headerlink" title="1.2.4    查看操作表的命令table_help"></a>1.2.4    查看操作表的命令table_help</h4><p><img src="https://s2.loli.net/2022/06/12/zDu9nGlk46CLEs8.png" alt="image-20220612211739667"></p><h4 id="1-2-5-退出HBase-Shell-exit"><a href="#1-2-5-退出HBase-Shell-exit" class="headerlink" title="1.2.5    退出HBase Shell exit"></a>1.2.5    退出HBase Shell exit</h4><p><img src="https://s2.loli.net/2022/06/12/DcuwQTr8WmKxkIO.png" alt="image-20220612211758289"></p><h3 id="1-3-DDL相关"><a href="#1-3-DDL相关" class="headerlink" title="1.3    DDL相关"></a>1.3    DDL相关</h3><h4 id="1-3-1-创建表create"><a href="#1-3-1-创建表create" class="headerlink" title="1.3.1. 创建表create"></a>1.3.1. 创建表create</h4><blockquote><p>注意：创建表时只需要指定列族名称，不需要指定列名。</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line"><span class="keyword">create</span> <span class="string">&#x27;表名&#x27;</span>, &#123;NAME <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;列族名1&#x27;</span>&#125;, &#123;NAME <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;列族名2&#x27;</span>&#125;, &#123;NAME <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;列族名3&#x27;</span>&#125;</span><br><span class="line"># 此种方式是上上面的简写方式，使用上面方式可以为列族指定更多的属性，如VERSIONS、TTL、BLOCKCACHE、CONFIGURATION等属性</span><br><span class="line"><span class="keyword">create</span> <span class="string">&#x27;表名&#x27;</span>, <span class="string">&#x27;列族名1&#x27;</span>, <span class="string">&#x27;列族名2&#x27;</span>, <span class="string">&#x27;列族名3&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="string">&#x27;表名&#x27;</span>, &#123;NAME <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;列族名1&#x27;</span>, VERSIONS <span class="operator">=</span><span class="operator">&gt;</span> 版本号, TTL <span class="operator">=</span><span class="operator">&gt;</span> 过期时间, BLOCKCACHE <span class="operator">=</span><span class="operator">&gt;</span> <span class="literal">true</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line"><span class="keyword">create</span> <span class="string">&#x27;tbl_user&#x27;</span>, <span class="string">&#x27;info&#x27;</span>, <span class="string">&#x27;detail&#x27;</span></span><br><span class="line"><span class="keyword">create</span> <span class="string">&#x27;t1&#x27;</span>, &#123;NAME <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;f1&#x27;</span>, VERSIONS <span class="operator">=</span><span class="operator">&gt;</span> <span class="number">1</span>, TTL <span class="operator">=</span><span class="operator">&gt;</span> <span class="number">2592000</span>, BLOCKCACHE <span class="operator">=</span><span class="operator">&gt;</span> <span class="literal">true</span>&#125;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/wUgQbz1Lpu6MKao.png" alt="image-20220612211844485"></p><h4 id="1-3-2-修改-添加、删除-表结构Schema-alter"><a href="#1-3-2-修改-添加、删除-表结构Schema-alter" class="headerlink" title="1.3.2    修改(添加、删除)表结构Schema alter"></a>1.3.2    修改(添加、删除)表结构Schema alter</h4><h5 id="1-3-2-1-添加一个列簇"><a href="#1-3-2-1-添加一个列簇" class="headerlink" title="1.3.2.1    添加一个列簇"></a>1.3.2.1    添加一个列簇</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法 </span><br><span class="line"><span class="keyword">alter</span> <span class="string">&#x27;表名&#x27;</span>, <span class="string">&#x27;列族名&#x27;</span></span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line"><span class="keyword">alter</span> <span class="string">&#x27;tbl_user&#x27;</span>, <span class="string">&#x27;address&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/sp2T48CcUBNaH6x.png" alt="image-20220612211857310"></p><h5 id="1-3-2-2-删除一个列簇"><a href="#1-3-2-2-删除一个列簇" class="headerlink" title="1.3.2.2    删除一个列簇"></a>1.3.2.2    删除一个列簇</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法 </span><br><span class="line"><span class="keyword">alter</span> <span class="string">&#x27;表名&#x27;</span>, &#123;NAME<span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;列族名&#x27;</span>, <span class="keyword">METHOD</span><span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;delete&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line"><span class="keyword">alter</span> <span class="string">&#x27;tbl_user&#x27;</span>, &#123;NAME<span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;address&#x27;</span>, <span class="keyword">METHOD</span><span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;delete&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/GVo9jqBUNhpvgnm.png" alt="image-20220612211912064"></p><h5 id="1-3-2-3-修改列族的属性"><a href="#1-3-2-3-修改列族的属性" class="headerlink" title="1.3.2.3    修改列族的属性"></a>1.3.2.3    修改列族的属性</h5><blockquote><p>可以修改列族的VERSIONS、IN_MEMORY</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 修改f1列族的版本为<span class="number">5</span></span><br><span class="line"><span class="keyword">alter</span> <span class="string">&#x27;t1&#x27;</span>, NAME <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;f1&#x27;</span>, VERSIONS <span class="operator">=</span><span class="operator">&gt;</span> <span class="number">5</span></span><br><span class="line"></span><br><span class="line"># 修改多个列族，修改f2为内存，版本号为<span class="number">5</span></span><br><span class="line"><span class="keyword">alter</span> <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;f1&#x27;</span>, &#123;NAME <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;f2&#x27;</span>, IN_MEMORY <span class="operator">=</span><span class="operator">&gt;</span> <span class="literal">true</span>&#125;, &#123;NAME <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;f3&#x27;</span>, VERSIONS <span class="operator">=</span><span class="operator">&gt;</span> <span class="number">5</span>&#125;</span><br><span class="line"></span><br><span class="line"># 也可以修改<span class="keyword">table</span><span class="operator">-</span><span class="keyword">scope</span>属性，例如MAX_FILESIZE, READONLY,MEMSTORE_FLUSHSIZE, DEFERRED_LOG_FLUSH等。</span><br><span class="line"># 例如，修改region的最大大小为<span class="number">128</span>MB：</span><br><span class="line"><span class="keyword">alter</span> <span class="string">&#x27;t1&#x27;</span>, MAX_FILESIZE <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;134217728&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="1-3-3-获取表的描述describe"><a href="#1-3-3-获取表的描述describe" class="headerlink" title="1.3.3    获取表的描述describe"></a>1.3.3    获取表的描述describe</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法 </span><br><span class="line"><span class="keyword">describe</span> <span class="string">&#x27;表名&#x27;</span></span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line"><span class="keyword">describe</span> <span class="string">&#x27;tbl_user&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/KeTgHt3YSZy8Aox.png" alt="image-20220612211927537"></p><h4 id="1-3-4-列举所有表list"><a href="#1-3-4-列举所有表list" class="headerlink" title="1.3.4    列举所有表list"></a>1.3.4    列举所有表list</h4><p><img src="https://s2.loli.net/2022/06/12/X4H17ytvEKNBbSp.png" alt="image-20220612211939477"></p><h4 id="1-3-5-表是否存在exists"><a href="#1-3-5-表是否存在exists" class="headerlink" title="1.3.5    表是否存在exists"></a>1.3.5    表是否存在exists</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法 </span><br><span class="line"><span class="keyword">exists</span> <span class="string">&#x27;表名&#x27;</span></span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line"><span class="keyword">exists</span> <span class="string">&#x27;tbl_user&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/HcSG3VxYrj7mfno.png" alt="image-20220612211957306"></p><h4 id="1-3-6-启用表enable和禁用表disable"><a href="#1-3-6-启用表enable和禁用表disable" class="headerlink" title="1.3.6    启用表enable和禁用表disable"></a>1.3.6    启用表enable和禁用表disable</h4><blockquote><p>通过enable和disable来启用&#x2F;禁用这个表,相应的可以通过is_enabled和is_disabled来检查表是否被禁用。</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">enable <span class="string">&#x27;表名&#x27;</span></span><br><span class="line">is_enabled <span class="string">&#x27;表名&#x27;</span></span><br><span class="line"></span><br><span class="line">disable <span class="string">&#x27;表名&#x27;</span></span><br><span class="line">is_disabled <span class="string">&#x27;表名&#x27;</span></span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">disable <span class="string">&#x27;tbl_user&#x27;</span></span><br><span class="line">is_disabled <span class="string">&#x27;tbl_user&#x27;</span></span><br><span class="line"></span><br><span class="line">enable <span class="string">&#x27;tbl_user&#x27;</span></span><br><span class="line">is_enabled <span class="string">&#x27;tbl_user&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="1-3-7-禁用满足正则表达式的所有表disable-all"><a href="#1-3-7-禁用满足正则表达式的所有表disable-all" class="headerlink" title="1.3.7    禁用满足正则表达式的所有表disable_all"></a>1.3.7    禁用满足正则表达式的所有表disable_all</h4><ul><li><code>.</code>匹配除“\n”和”\r”之外的任何单个字符</li><li><code>*</code>匹配前面的子表达式任意次</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 匹配以t开头的表名</span><br><span class="line">disable_all <span class="string">&#x27;t.*&#x27;</span></span><br><span class="line"># 匹配指定命名空间ns下的以t开头的所有表</span><br><span class="line">disable_all <span class="string">&#x27;ns:t.*&#x27;</span></span><br><span class="line"># 匹配ns命名空间下的所有表</span><br><span class="line">disable_all <span class="string">&#x27;ns:.*&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="1-3-8-启用满足正则表达式的所有表enable-all"><a href="#1-3-8-启用满足正则表达式的所有表enable-all" class="headerlink" title="1.3.8    启用满足正则表达式的所有表enable_all"></a>1.3.8    启用满足正则表达式的所有表enable_all</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">enable_all &#x27;t.*&#x27;</span><br><span class="line">enable_all &#x27;ns:t.*&#x27;</span><br><span class="line">enable_all &#x27;ns:.*&#x27;</span><br></pre></td></tr></table></figure><h4 id="1-3-9-删除表drop"><a href="#1-3-9-删除表drop" class="headerlink" title="1.3.9    删除表drop"></a>1.3.9    删除表drop</h4><blockquote><p>需要先禁用表，然后再删除表，启用的表是不允许删除的</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">disable <span class="string">&#x27;表名&#x27;</span></span><br><span class="line"><span class="keyword">drop</span> <span class="string">&#x27;表名&#x27;</span></span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">disable <span class="string">&#x27;tbl_user&#x27;</span></span><br><span class="line"><span class="keyword">drop</span> <span class="string">&#x27;tbl_user&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>直接删除报错：</p></blockquote><p><img src="https://s2.loli.net/2022/06/12/Ki8HoPqw1UCjSsX.png" alt="image-20220612212011296"></p><p>先禁用后删除</p><p><img src="https://s2.loli.net/2022/06/12/VP5UMTDBEg9LNop.png" alt="image-20220612212023082"></p><h4 id="1-3-10-删除满足正则表达式的所有表drop-all"><a href="#1-3-10-删除满足正则表达式的所有表drop-all" class="headerlink" title="1.3.10    删除满足正则表达式的所有表drop_all"></a>1.3.10    删除满足正则表达式的所有表drop_all</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">drop_all &#x27;t.*&#x27;</span><br><span class="line">drop_all &#x27;ns:t.*&#x27;</span><br><span class="line">drop_all &#x27;ns:.*&#x27;</span><br></pre></td></tr></table></figure><h4 id="1-3-11-获取某个表赋值给一个变量-get-table"><a href="#1-3-11-获取某个表赋值给一个变量-get-table" class="headerlink" title="1.3.11    获取某个表赋值给一个变量 get_table"></a>1.3.11    获取某个表赋值给一个变量 get_table</h4><blockquote><p>通过 var &#x3D; get_table ‘表名’ 赋值给一个变量对象，然后对象.来调用，就像面向对象编程一样，通过对象.方法来调用，这种方式在操作某个表时就不必每次列举表名了。</p></blockquote><p><img src="https://s2.loli.net/2022/06/12/YhZ5EpCQijf2d1k.png" alt="image-20220612212036726"></p><h4 id="1-3-12-获取rowKey所在的区-locate-region"><a href="#1-3-12-获取rowKey所在的区-locate-region" class="headerlink" title="1.3.12    获取rowKey所在的区 locate_region"></a>1.3.12    获取rowKey所在的区 locate_region</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">locate_region &#x27;表名&#x27;, &#x27;行键&#x27;</span><br></pre></td></tr></table></figure><h4 id="1-3-13-显示hbase所支持的所有过滤器show-filters"><a href="#1-3-13-显示hbase所支持的所有过滤器show-filters" class="headerlink" title="1.3.13    显示hbase所支持的所有过滤器show_filters"></a>1.3.13    显示hbase所支持的所有过滤器show_filters</h4><blockquote><p>过滤器用于get和scan命令中作为筛选数据的条件，类型关系型数据库中的where的作用</p></blockquote><p><img src="https://s2.loli.net/2022/06/12/dkL1wJREzvXOjxQ.png" alt="image-20220612212116009"></p><h3 id="1-4-namespace"><a href="#1-4-namespace" class="headerlink" title="1.4    namespace"></a>1.4    namespace</h3><blockquote><p><strong>hbase中没有数据库的概念 , 可以使用namespace来达到数据库分类别管理表的作用</strong></p></blockquote><h4 id="1-4-1-列举命名空间-list-namespace"><a href="#1-4-1-列举命名空间-list-namespace" class="headerlink" title="1.4.1    列举命名空间 list_namespace"></a>1.4.1    列举命名空间 list_namespace</h4><p><img src="https://s2.loli.net/2022/06/12/ndvMyJafYNrStip.png" alt="image-20220612212131363"></p><h4 id="1-4-2-获取命名空间描述-describe-namespace"><a href="#1-4-2-获取命名空间描述-describe-namespace" class="headerlink" title="1.4.2    获取命名空间描述 describe_namespace"></a>1.4.2    获取命名空间描述 describe_namespace</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">describe_namespace <span class="string">&#x27;default&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/fCoW9JQNq4YEgDG.png" alt="image-20220612212142303"></p><h4 id="1-4-3-查看命名空间下的所有表-list-namespace-tables"><a href="#1-4-3-查看命名空间下的所有表-list-namespace-tables" class="headerlink" title="1.4.3    查看命名空间下的所有表 list_namespace_tables"></a>1.4.3    查看命名空间下的所有表 list_namespace_tables</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">list_namespace_tables <span class="string">&#x27;default&#x27;</span></span><br><span class="line"></span><br><span class="line">list_namespace_tables <span class="string">&#x27;hbase&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/78yuhSa3n4HRfEU.png" alt="image-20220612212155514"></p><h4 id="1-4-4-创建命名空间create-namespace"><a href="#1-4-4-创建命名空间create-namespace" class="headerlink" title="1.4.4    创建命名空间create_namespace"></a>1.4.4    创建命名空间create_namespace</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">create_namespace <span class="string">&#x27;bigdata17&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="1-4-5-删除命名空间drop-namespace"><a href="#1-4-5-删除命名空间drop-namespace" class="headerlink" title="1.4.5    删除命名空间drop_namespace"></a>1.4.5    删除命名空间drop_namespace</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">drop_namespace <span class="string">&#x27;命名空间名称&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="1-5-DML"><a href="#1-5-DML" class="headerlink" title="1.5    DML"></a>1.5    DML</h3><h4 id="1-5-1-插入或者修改数据put"><a href="#1-5-1-插入或者修改数据put" class="headerlink" title="1.5.1    插入或者修改数据put"></a>1.5.1    插入或者修改数据put</h4><p><img src="https://s2.loli.net/2022/06/12/ItKsyROVx8p3DBg.png" alt="image-20220612212213476"></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line"># 当列族中只有一个列时<span class="string">&#x27;列族名:列名&#x27;</span>使用<span class="string">&#x27;列族名&#x27;</span></span><br><span class="line">put <span class="string">&#x27;表名&#x27;</span>, <span class="string">&#x27;行键&#x27;</span>, <span class="string">&#x27;列族名&#x27;</span>, <span class="string">&#x27;列值&#x27;</span></span><br><span class="line">put <span class="string">&#x27;表名&#x27;</span>, <span class="string">&#x27;行键&#x27;</span>, <span class="string">&#x27;列族名:列名&#x27;</span>, <span class="string">&#x27;列值&#x27;</span></span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line"></span><br><span class="line"># 创建表</span><br><span class="line"><span class="keyword">create</span> <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;info&#x27;</span>, <span class="string">&#x27;detail&#x27;</span>, <span class="string">&#x27;address&#x27;</span></span><br><span class="line"></span><br><span class="line"># 第一行数据</span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;mengday&#x27;</span>, <span class="string">&#x27;info:id&#x27;</span>, <span class="string">&#x27;1&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;mengday&#x27;</span>, <span class="string">&#x27;info:name&#x27;</span>, <span class="string">&#x27;张三&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;mengday&#x27;</span>, <span class="string">&#x27;info:age&#x27;</span>, <span class="string">&#x27;28&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;mengday&#x27;</span>, <span class="string">&#x27;detail:birthday&#x27;</span>, <span class="string">&#x27;1990-06-26&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;mengday&#x27;</span>, <span class="string">&#x27;detail:email&#x27;</span>, <span class="string">&#x27;abc@163.com&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;mengday&#x27;</span>, <span class="string">&#x27;detail:create_time&#x27;</span>, <span class="string">&#x27;2019-03-04 14:26:10&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;mengday&#x27;</span>, <span class="string">&#x27;address&#x27;</span>, <span class="string">&#x27;上海市&#x27;</span></span><br><span class="line"></span><br><span class="line"># 第二行数据</span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;vbirdbest&#x27;</span>, <span class="string">&#x27;info:id&#x27;</span>, <span class="string">&#x27;2&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;vbirdbest&#x27;</span>, <span class="string">&#x27;info:name&#x27;</span>, <span class="string">&#x27;李四&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;vbirdbest&#x27;</span>, <span class="string">&#x27;info:age&#x27;</span>, <span class="string">&#x27;27&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;vbirdbest&#x27;</span>, <span class="string">&#x27;detail:birthday&#x27;</span>, <span class="string">&#x27;1990-06-27&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;vbirdbest&#x27;</span>, <span class="string">&#x27;detail:email&#x27;</span>, <span class="string">&#x27;xxx@gmail.com&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;vbirdbest&#x27;</span>, <span class="string">&#x27;detail:create_time&#x27;</span>, <span class="string">&#x27;2019-03-05 14:26:10&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;vbirdbest&#x27;</span>, <span class="string">&#x27;address&#x27;</span>, <span class="string">&#x27;北京市&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 第三行数据</span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;info:id&#x27;</span>, <span class="string">&#x27;3&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;info:name&#x27;</span>, <span class="string">&#x27;王五&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;info:age&#x27;</span>, <span class="string">&#x27;26&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;detail:birthday&#x27;</span>, <span class="string">&#x27;1990-06-28&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;detail:email&#x27;</span>, <span class="string">&#x27;xyz@qq.com&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;detail:create_time&#x27;</span>, <span class="string">&#x27;2019-03-06 14:26:10&#x27;</span></span><br><span class="line">put <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;address&#x27;</span>, <span class="string">&#x27;杭州市&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="1-5-2-全表扫描scan"><a href="#1-5-2-全表扫描scan" class="headerlink" title="1.5.2    全表扫描scan"></a>1.5.2    全表扫描scan</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span></span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;users&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/MnSmobUu6JAG5Qd.png" alt="image-20220612212319590"></p><blockquote><p>扫描整个列簇</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>, &#123;<span class="keyword">COLUMN</span><span class="operator">=</span><span class="operator">&gt;</span><span class="string">&#x27;列族名&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;users&#x27;</span>, &#123;<span class="keyword">COLUMN</span><span class="operator">=</span><span class="operator">&gt;</span><span class="string">&#x27;info&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>扫描整个列簇的某个列</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>, &#123;<span class="keyword">COLUMN</span><span class="operator">=</span><span class="operator">&gt;</span><span class="string">&#x27;列族名:列名&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;users&#x27;</span>, &#123;<span class="keyword">COLUMN</span><span class="operator">=</span><span class="operator">&gt;</span><span class="string">&#x27;info:age&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><h4 id="1-5-3-获取数据get"><a href="#1-5-3-获取数据get" class="headerlink" title="1.5.3     获取数据get"></a>1.5.3     获取数据get</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line"><span class="keyword">get</span> <span class="string">&#x27;表名&#x27;</span>, <span class="string">&#x27;行键&#x27;</span></span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line"><span class="keyword">get</span> <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;xiaoming&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>根据某一行某列族的数据</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line"><span class="keyword">get</span> <span class="string">&#x27;表名&#x27;</span>, <span class="string">&#x27;行键&#x27;</span>, <span class="string">&#x27;列族名&#x27;</span></span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line"><span class="keyword">get</span> <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;info&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 创建表，c1版本为<span class="number">4</span>， 元数据mykey<span class="operator">=</span>myvalue</span><br><span class="line">hbase(main):<span class="number">009</span>:<span class="number">0</span><span class="operator">&gt;</span> <span class="keyword">create</span> <span class="string">&#x27;t1&#x27;</span>, &#123;NAME <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;c1&#x27;</span>, VERSIONS <span class="operator">=</span><span class="operator">&gt;</span> <span class="number">4</span>&#125;, METADATA <span class="operator">=</span><span class="operator">&gt;</span> &#123; <span class="string">&#x27;mykey&#x27;</span> <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;myvalue&#x27;</span> &#125;</span><br><span class="line"><span class="number">0</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">2.2810</span> seconds</span><br><span class="line"></span><br><span class="line"><span class="operator">=</span><span class="operator">&gt;</span> Hbase::<span class="keyword">Table</span> <span class="operator">-</span> t1</span><br><span class="line"># 添加列族c2, c3</span><br><span class="line">hbase(main):<span class="number">010</span>:<span class="number">0</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;c2&#x27;</span>, <span class="string">&#x27;c3&#x27;</span></span><br><span class="line">Updating <span class="keyword">all</span> regions <span class="keyword">with</span> the <span class="keyword">new</span> schema...</span><br><span class="line"><span class="number">1</span><span class="operator">/</span><span class="number">1</span> regions updated.</span><br><span class="line">Done.</span><br><span class="line">Updating <span class="keyword">all</span> regions <span class="keyword">with</span> the <span class="keyword">new</span> schema...</span><br><span class="line"><span class="number">1</span><span class="operator">/</span><span class="number">1</span> regions updated.</span><br><span class="line">Done.</span><br><span class="line"><span class="number">0</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">3.8320</span> seconds</span><br><span class="line"></span><br><span class="line"># 出入数据，c1 插入<span class="number">4</span>个版本的值</span><br><span class="line">hbase(main):<span class="number">011</span>:<span class="number">0</span><span class="operator">&gt;</span> put <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span>, <span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;v1&#x27;</span></span><br><span class="line"><span class="number">0</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.1000</span> seconds</span><br><span class="line"></span><br><span class="line">hbase(main):<span class="number">012</span>:<span class="number">0</span><span class="operator">&gt;</span> put <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span>, <span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;v11&#x27;</span></span><br><span class="line"><span class="number">0</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.0180</span> seconds</span><br><span class="line"></span><br><span class="line">hbase(main):<span class="number">013</span>:<span class="number">0</span><span class="operator">&gt;</span> put <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span>, <span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;v111&#x27;</span></span><br><span class="line"><span class="number">0</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.0140</span> seconds</span><br><span class="line"></span><br><span class="line">hbase(main):<span class="number">014</span>:<span class="number">0</span><span class="operator">&gt;</span> put <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span>, <span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;v1111&#x27;</span></span><br><span class="line"><span class="number">0</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.0140</span> seconds</span><br><span class="line"></span><br><span class="line"># 插入c2、c3的值</span><br><span class="line">hbase(main):<span class="number">015</span>:<span class="number">0</span><span class="operator">&gt;</span> put <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span>, <span class="string">&#x27;c2&#x27;</span>, <span class="string">&#x27;v2&#x27;</span></span><br><span class="line"><span class="number">0</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.0140</span> seconds</span><br><span class="line"></span><br><span class="line">hbase(main):<span class="number">016</span>:<span class="number">0</span><span class="operator">&gt;</span> put <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span>, <span class="string">&#x27;c3&#x27;</span>, <span class="string">&#x27;v3&#x27;</span></span><br><span class="line"><span class="number">0</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.0210</span> seconds</span><br><span class="line"></span><br><span class="line"># 获取rowKey<span class="operator">=</span>r1的一行记录</span><br><span class="line">hbase(main):<span class="number">017</span>:<span class="number">0</span><span class="operator">&gt;</span> <span class="keyword">get</span> <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span></span><br><span class="line"><span class="keyword">COLUMN</span>                                              CELL</span><br><span class="line"> c1:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819382575</span>, <span class="keyword">value</span><span class="operator">=</span>v1111</span><br><span class="line"> c2:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819392398</span>, <span class="keyword">value</span><span class="operator">=</span>v2</span><br><span class="line"> c3:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819398244</span>, <span class="keyword">value</span><span class="operator">=</span>v3</span><br><span class="line"><span class="number">3</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.0550</span> seconds</span><br><span class="line"></span><br><span class="line"># 获取rowKey<span class="operator">=</span>r1并且 <span class="number">1552819392398</span> <span class="operator">&lt;=</span> 时间戳范围 <span class="operator">&lt;</span> <span class="number">1552819398244</span></span><br><span class="line">hbase(main):<span class="number">018</span>:<span class="number">0</span><span class="operator">&gt;</span> <span class="keyword">get</span> <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span>, &#123;TIMERANGE <span class="operator">=</span><span class="operator">&gt;</span> [<span class="number">1552819392398</span>, <span class="number">1552819398244</span>]&#125;</span><br><span class="line"><span class="keyword">COLUMN</span>                                              CELL</span><br><span class="line"> c2:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819392398</span>, <span class="keyword">value</span><span class="operator">=</span>v2</span><br><span class="line"><span class="number">1</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.0090</span> seconds</span><br><span class="line"></span><br><span class="line"># 获取指定列的值</span><br><span class="line">hbase(main):<span class="number">019</span>:<span class="number">0</span><span class="operator">&gt;</span> <span class="keyword">get</span> <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span>, &#123;<span class="keyword">COLUMN</span> <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;c1&#x27;</span>&#125;</span><br><span class="line"><span class="keyword">COLUMN</span>                                              CELL</span><br><span class="line"> c1:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819382575</span>, <span class="keyword">value</span><span class="operator">=</span>v1111</span><br><span class="line"><span class="number">1</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.0160</span> seconds</span><br><span class="line"></span><br><span class="line"># 获取指定列的值，多个值使用数组表示</span><br><span class="line">hbase(main):<span class="number">020</span>:<span class="number">0</span><span class="operator">&gt;</span> <span class="keyword">get</span> <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span>, &#123;<span class="keyword">COLUMN</span> <span class="operator">=</span><span class="operator">&gt;</span> [<span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;c2&#x27;</span>, <span class="string">&#x27;c3&#x27;</span>]&#125;</span><br><span class="line"><span class="keyword">COLUMN</span>                                              CELL</span><br><span class="line"> c1:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819382575</span>, <span class="keyword">value</span><span class="operator">=</span>v1111</span><br><span class="line"> c2:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819392398</span>, <span class="keyword">value</span><span class="operator">=</span>v2</span><br><span class="line"> c3:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819398244</span>, <span class="keyword">value</span><span class="operator">=</span>v3</span><br><span class="line"><span class="number">3</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.0170</span> seconds</span><br><span class="line"></span><br><span class="line"># 获取c1的值，获取<span class="number">4</span>个版本的值，默认是按照时间戳降续排序的</span><br><span class="line">hbase(main):<span class="number">021</span>:<span class="number">0</span><span class="operator">&gt;</span> <span class="keyword">get</span> <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span>, &#123;<span class="keyword">COLUMN</span> <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;c1&#x27;</span>, VERSIONS <span class="operator">=</span><span class="operator">&gt;</span> <span class="number">4</span>&#125;</span><br><span class="line"><span class="keyword">COLUMN</span>                                              CELL</span><br><span class="line"> c1:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819382575</span>, <span class="keyword">value</span><span class="operator">=</span>v1111</span><br><span class="line"> c1:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819376343</span>, <span class="keyword">value</span><span class="operator">=</span>v111</span><br><span class="line"> c1:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819368993</span>, <span class="keyword">value</span><span class="operator">=</span>v11</span><br><span class="line"> c1:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819362975</span>, <span class="keyword">value</span><span class="operator">=</span>v1</span><br><span class="line"><span class="number">4</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.0180</span> seconds</span><br><span class="line"></span><br><span class="line"># 获取c1的<span class="number">3</span>个版本值</span><br><span class="line">hbase(main):<span class="number">027</span>:<span class="number">0</span><span class="operator">*</span> <span class="keyword">get</span> <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span>, &#123;<span class="keyword">COLUMN</span> <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;c1&#x27;</span>, VERSIONS <span class="operator">=</span><span class="operator">&gt;</span> <span class="number">3</span>&#125;</span><br><span class="line"><span class="keyword">COLUMN</span>                                               CELL</span><br><span class="line"> c1:                                                 <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819382575</span>, <span class="keyword">value</span><span class="operator">=</span>v1111</span><br><span class="line"> c1:                                                 <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819376343</span>, <span class="keyword">value</span><span class="operator">=</span>v111</span><br><span class="line"> c1:                                                 <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819368993</span>, <span class="keyword">value</span><span class="operator">=</span>v11</span><br><span class="line"><span class="number">3</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.0090</span> seconds</span><br><span class="line"></span><br><span class="line"># 获取指定时间戳版本的列</span><br><span class="line">hbase(main):<span class="number">022</span>:<span class="number">0</span><span class="operator">&gt;</span> <span class="keyword">get</span> <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span>, &#123;<span class="keyword">COLUMN</span> <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;c1&#x27;</span>, <span class="type">TIMESTAMP</span> <span class="operator">=</span><span class="operator">&gt;</span> <span class="number">1552819376343</span>&#125;</span><br><span class="line"><span class="keyword">COLUMN</span>                                              CELL</span><br><span class="line"> c1:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819376343</span>, <span class="keyword">value</span><span class="operator">=</span>v111</span><br><span class="line"><span class="number">1</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.0170</span> seconds</span><br><span class="line"></span><br><span class="line">hbase(main):<span class="number">023</span>:<span class="number">0</span><span class="operator">&gt;</span> <span class="keyword">get</span> <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span>, &#123;<span class="keyword">COLUMN</span> <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;c1&#x27;</span>, <span class="type">TIMESTAMP</span> <span class="operator">=</span><span class="operator">&gt;</span> <span class="number">1552819376343</span>, VERSIONS <span class="operator">=</span><span class="operator">&gt;</span> <span class="number">4</span>&#125;</span><br><span class="line"><span class="keyword">COLUMN</span>                                              CELL</span><br><span class="line"> c1:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819376343</span>, <span class="keyword">value</span><span class="operator">=</span>v111</span><br><span class="line"><span class="number">1</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.0130</span> seconds</span><br><span class="line"></span><br><span class="line"># 获取rowKey<span class="operator">=</span>r1中的值等于v2的所有列</span><br><span class="line">hbase(main):<span class="number">024</span>:<span class="number">0</span><span class="operator">&gt;</span> <span class="keyword">get</span> <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span>, &#123;<span class="keyword">FILTER</span> <span class="operator">=</span><span class="operator">&gt;</span> &quot;ValueFilter(=, &#x27;binary:v2&#x27;)&quot;&#125;</span><br><span class="line"><span class="keyword">COLUMN</span>                                              CELL</span><br><span class="line"> c2:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819392398</span>, <span class="keyword">value</span><span class="operator">=</span>v2</span><br><span class="line"><span class="number">1</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.0510</span> seconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hbase(main):<span class="number">025</span>:<span class="number">0</span><span class="operator">&gt;</span> <span class="keyword">get</span> <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;r1&#x27;</span>, &#123;<span class="keyword">COLUMN</span> <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;c1&#x27;</span>, ATTRIBUTES <span class="operator">=</span><span class="operator">&gt;</span> &#123;<span class="string">&#x27;mykey&#x27;</span><span class="operator">=</span><span class="operator">&gt;</span><span class="string">&#x27;myvalue&#x27;</span>&#125;&#125;</span><br><span class="line"><span class="keyword">COLUMN</span>                                              CELL</span><br><span class="line"> c1:                                                <span class="type">timestamp</span><span class="operator">=</span><span class="number">1552819382575</span>, <span class="keyword">value</span><span class="operator">=</span>v1111</span><br><span class="line"><span class="number">1</span> <span class="type">row</span>(s) <span class="keyword">in</span> <span class="number">0.0100</span> seconds</span><br></pre></td></tr></table></figure><h4 id="1-5-4-删除某个列族中的某个列delete"><a href="#1-5-4-删除某个列族中的某个列delete" class="headerlink" title="1.5.4    删除某个列族中的某个列delete"></a>1.5.4    删除某个列族中的某个列delete</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">语法</span></span><br><span class="line">delete &#x27;表名&#x27;, &#x27;行键&#x27;, &#x27;列族名:列名&#x27;</span><br><span class="line"></span><br><span class="line">delete &#x27;users&#x27;,&#x27;xiaoming&#x27;,&#x27;info:age&#x27;</span><br><span class="line"></span><br><span class="line">create &#x27;tbl_test&#x27;, &#x27;columnFamily1&#x27;</span><br><span class="line"></span><br><span class="line">put &#x27;tbl_test&#x27;, &#x27;rowKey1&#x27;, &#x27;columnFamily1:column1&#x27;, &#x27;value1&#x27;</span><br><span class="line">put &#x27;tbl_test&#x27;, &#x27;rowKey1&#x27;, &#x27;columnFamily1:column2&#x27;, &#x27;value2&#x27;</span><br><span class="line"></span><br><span class="line">delete &#x27;tbl_test&#x27;, &#x27;rowKey1&#x27;, &#x27;columnFamily1:column1&#x27;</span><br></pre></td></tr></table></figure><h4 id="1-5-5-删除某行数据deleteall"><a href="#1-5-5-删除某行数据deleteall" class="headerlink" title="1.5.5     删除某行数据deleteall"></a>1.5.5     删除某行数据deleteall</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">deleteall <span class="string">&#x27;表名&#x27;</span>, <span class="string">&#x27;行键&#x27;</span></span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">deleteall <span class="string">&#x27;users&#x27;</span>, <span class="string">&#x27;xiaoming&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="1-5-6-清空整个表的数据truncate"><a href="#1-5-6-清空整个表的数据truncate" class="headerlink" title="1.5.6    清空整个表的数据truncate"></a>1.5.6    清空整个表的数据truncate</h4><blockquote><p>先disable表，然后再drop表，最后重新create表</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">truncate &#x27;表名&#x27;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/vpx594qBNIhoGcU.png" alt="image-20220612212401461"></p><h4 id="1-5-7-自增incr"><a href="#1-5-7-自增incr" class="headerlink" title="1.5.7    自增incr"></a>1.5.7    自增incr</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">incr <span class="string">&#x27;表名&#x27;</span>, <span class="string">&#x27;行键&#x27;</span>, <span class="string">&#x27;列族:列名&#x27;</span>, 步长值</span><br><span class="line"></span><br><span class="line"># 示例 </span><br><span class="line"># 注意：incr 可以对不存的行键操作，如果行键已经存在会报错，如果使用put修改了incr的值再使用incr也会报错</span><br><span class="line"># ERROR: org.apache.hadoop.hbase.DoNotRetryIOException: Field <span class="keyword">is</span> <span class="keyword">not</span> a long, it<span class="string">&#x27;s 2 bytes wide</span></span><br><span class="line"><span class="string">incr &#x27;</span>tbl_user<span class="string">&#x27;, &#x27;</span>xiaohong<span class="string">&#x27;, &#x27;</span>info:age<span class="string">&#x27;, 1</span></span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/12/lizvE4OsXNP9WjI.png" alt="image-20220612212416654"></p><h4 id="1-5-8-计数器get-counter"><a href="#1-5-8-计数器get-counter" class="headerlink" title="1.5.8    计数器get_counter"></a>1.5.8    计数器get_counter</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 点击量：日、周、月</span><br><span class="line"><span class="keyword">create</span> <span class="string">&#x27;counters&#x27;</span>, <span class="string">&#x27;daily&#x27;</span>, <span class="string">&#x27;weekly&#x27;</span>, <span class="string">&#x27;monthly&#x27;</span></span><br><span class="line">incr <span class="string">&#x27;counters&#x27;</span>, <span class="string">&#x27;20110101&#x27;</span>, <span class="string">&#x27;daily:hits&#x27;</span>, <span class="number">1</span></span><br><span class="line">incr <span class="string">&#x27;counters&#x27;</span>, <span class="string">&#x27;20110101&#x27;</span>, <span class="string">&#x27;daily:hits&#x27;</span>, <span class="number">1</span></span><br><span class="line">get_counter <span class="string">&#x27;counters&#x27;</span>, <span class="string">&#x27;20110101&#x27;</span>, <span class="string">&#x27;daily:hits&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="1-5-9-修饰词"><a href="#1-5-9-修饰词" class="headerlink" title="1.5.9    修饰词"></a>1.5.9    修饰词</h4><h5 id="1、修饰词"><a href="#1、修饰词" class="headerlink" title="1、修饰词"></a>1、修饰词</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>, &#123;COLUMNS <span class="operator">=</span><span class="operator">&gt;</span> [ <span class="string">&#x27;列族名1:列名1&#x27;</span>, <span class="string">&#x27;列族名1:列名2&#x27;</span>, ...]&#125;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>, &#123;COLUMNS <span class="operator">=</span><span class="operator">&gt;</span> [ <span class="string">&#x27;info:id&#x27;</span>, <span class="string">&#x27;info:age&#x27;</span>]&#125;</span><br></pre></td></tr></table></figure><h5 id="2、TIMESTAMP-指定时间戳"><a href="#2、TIMESTAMP-指定时间戳" class="headerlink" title="2、TIMESTAMP 指定时间戳"></a>2、TIMESTAMP 指定时间戳</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>,&#123;TIMERANGE<span class="operator">=</span><span class="operator">&gt;</span>[timestamp1, timestamp2]&#125;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>,&#123;TIMERANGE<span class="operator">=</span><span class="operator">&gt;</span>[<span class="number">1551938004321</span>, <span class="number">1551938036450</span>]&#125;</span><br></pre></td></tr></table></figure><h5 id="3、VERSIONS"><a href="#3、VERSIONS" class="headerlink" title="3、VERSIONS"></a>3、VERSIONS</h5><blockquote><p>默认情况下一个列只能存储一个数据，后面如果修改数据就会将原来的覆盖掉，可以通过指定VERSIONS时HBase一列能存储多个值。</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="string">&#x27;tbl_test&#x27;</span>, <span class="string">&#x27;columnFamily1&#x27;</span></span><br><span class="line"><span class="keyword">describe</span> <span class="string">&#x27;tbl_test&#x27;</span></span><br><span class="line"></span><br><span class="line"># 修改列族版本号</span><br><span class="line"><span class="keyword">alter</span> <span class="string">&#x27;tbl_test&#x27;</span>, &#123; NAME<span class="operator">=</span><span class="operator">&gt;</span><span class="string">&#x27;columnFamily1&#x27;</span>, VERSIONS<span class="operator">=</span><span class="operator">&gt;</span><span class="number">3</span> &#125;</span><br><span class="line"></span><br><span class="line">put <span class="string">&#x27;tbl_test&#x27;</span>, <span class="string">&#x27;rowKey1&#x27;</span>, <span class="string">&#x27;columnFamily1:column1&#x27;</span>, <span class="string">&#x27;value1&#x27;</span></span><br><span class="line">put <span class="string">&#x27;tbl_test&#x27;</span>, <span class="string">&#x27;rowKey1&#x27;</span>, <span class="string">&#x27;columnFamily1:column1&#x27;</span>, <span class="string">&#x27;value2&#x27;</span></span><br><span class="line">put <span class="string">&#x27;tbl_test&#x27;</span>, <span class="string">&#x27;rowKey1&#x27;</span>, <span class="string">&#x27;columnFamily1:column1&#x27;</span>, <span class="string">&#x27;value3&#x27;</span></span><br><span class="line"></span><br><span class="line"># 默认返回最新的一条数据</span><br><span class="line"><span class="keyword">get</span> <span class="string">&#x27;tbl_test&#x27;</span>,<span class="string">&#x27;rowKey1&#x27;</span>,<span class="string">&#x27;columnFamily1:column1&#x27;</span></span><br><span class="line"></span><br><span class="line"># 返回<span class="number">3</span>个</span><br><span class="line"><span class="keyword">get</span> <span class="string">&#x27;tbl_test&#x27;</span>,<span class="string">&#x27;rowKey1&#x27;</span>,&#123;<span class="keyword">COLUMN</span><span class="operator">=</span><span class="operator">&gt;</span><span class="string">&#x27;columnFamily1:column1&#x27;</span>, VERSIONS<span class="operator">=</span><span class="operator">&gt;</span><span class="number">3</span>&#125;</span><br><span class="line"># 返回<span class="number">2</span>个</span><br><span class="line"><span class="keyword">get</span> <span class="string">&#x27;tbl_test&#x27;</span>,<span class="string">&#x27;rowKey1&#x27;</span>,&#123;<span class="keyword">COLUMN</span><span class="operator">=</span><span class="operator">&gt;</span><span class="string">&#x27;columnFamily1:column1&#x27;</span>, VERSIONS<span class="operator">=</span><span class="operator">&gt;</span><span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure><h5 id="4、STARTROW"><a href="#4、STARTROW" class="headerlink" title="4、STARTROW"></a>4、STARTROW</h5><blockquote><p>ROWKEY起始行。会先根据这个key定位到region，再向后扫描</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>, &#123; STARTROW <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;行键名&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>, &#123; STARTROW <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;vbirdbest&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><h5 id="5、STOPROW-：截止到STOPROW行，STOPROW行之前的数据，不包括STOPROW这行数据"><a href="#5、STOPROW-：截止到STOPROW行，STOPROW行之前的数据，不包括STOPROW这行数据" class="headerlink" title="5、STOPROW ：截止到STOPROW行，STOPROW行之前的数据，不包括STOPROW这行数据"></a>5、STOPROW ：截止到STOPROW行，STOPROW行之前的数据，不包括STOPROW这行数据</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>, &#123; STOPROW <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;行键名&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>, &#123; STOPROW <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;xiaoming&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><h5 id="6、LIMIT-返回的行数"><a href="#6、LIMIT-返回的行数" class="headerlink" title="6、LIMIT 返回的行数"></a>6、LIMIT 返回的行数</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>, &#123; LIMIT <span class="operator">=</span><span class="operator">&gt;</span> 行数&#125;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>, &#123; LIMIT <span class="operator">=</span><span class="operator">&gt;</span> <span class="number">2</span> &#125;</span><br></pre></td></tr></table></figure><h4 id="1-5-10-FILTER条件过滤器"><a href="#1-5-10-FILTER条件过滤器" class="headerlink" title="1.5.10    FILTER条件过滤器"></a>1.5.10    FILTER条件过滤器</h4><blockquote><p>过滤器之间可以使用AND、OR连接多个过滤器。</p></blockquote><h5 id="1、ValueFilter-值过滤器"><a href="#1、ValueFilter-值过滤器" class="headerlink" title="1、ValueFilter 值过滤器"></a>1、ValueFilter 值过滤器</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 语法：<span class="type">binary</span> 等于某个值</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>, <span class="keyword">FILTER</span><span class="operator">=</span><span class="operator">&gt;</span>&quot;ValueFilter(=,&#x27;binary:列值&#x27;)&quot;</span><br><span class="line"># 语法 substring:包含某个值</span><br><span class="line">scan <span class="string">&#x27;表名&#x27;</span>, <span class="keyword">FILTER</span><span class="operator">=</span><span class="operator">&gt;</span>&quot;ValueFilter(=,&#x27;substring:列值&#x27;)&quot;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>, <span class="keyword">FILTER</span><span class="operator">=</span><span class="operator">&gt;</span>&quot;ValueFilter(=, &#x27;binary:26&#x27;)&quot;</span><br><span class="line">scan <span class="string">&#x27;tbl_user&#x27;</span>, <span class="keyword">FILTER</span><span class="operator">=</span><span class="operator">&gt;</span>&quot;ValueFilter(=, &#x27;substring:6&#x27;)&quot;</span><br></pre></td></tr></table></figure><h5 id="2、ColumnPrefixFilter-列名前缀过滤器"><a href="#2、ColumnPrefixFilter-列名前缀过滤器" class="headerlink" title="2、ColumnPrefixFilter 列名前缀过滤器"></a>2、ColumnPrefixFilter 列名前缀过滤器</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 语法 substring:包含某个值</span><br><span class="line">scan &#x27;表名&#x27;, FILTER=&gt;&quot;ColumnPrefixFilter(&#x27;列名前缀&#x27;)&quot;</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">scan &#x27;tbl_user&#x27;, FILTER=&gt;&quot;ColumnPrefixFilter(&#x27;birth&#x27;)&quot;</span><br><span class="line"># 通过括号、AND和OR的条件组合多个过滤器</span><br><span class="line">scan &#x27;tbl_user&#x27;, FILTER=&gt;&quot;ColumnPrefixFilter(&#x27;birth&#x27;) AND ValueFilter(=,&#x27;substring:26&#x27;)&quot;</span><br></pre></td></tr></table></figure><h5 id="3、rowKey字典排序"><a href="#3、rowKey字典排序" class="headerlink" title="3、rowKey字典排序"></a>3、rowKey字典排序</h5><blockquote><p>Table中的所有行都是按照row key的字典排序的</p></blockquote><p><img src="https://s2.loli.net/2022/06/12/lHbNsQYjU8TCnMA.png" alt="image-20220612212431834"></p>]]></content>
    
    
    <summary type="html">对学习hbase shell的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hbase" scheme="http://example.com/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>Hbase概述及搭建</title>
    <link href="http://example.com/2022/06/08/Hbase%E6%A6%82%E8%BF%B0%E5%8F%8A%E6%90%AD%E5%BB%BA/"/>
    <id>http://example.com/2022/06/08/Hbase%E6%A6%82%E8%BF%B0%E5%8F%8A%E6%90%AD%E5%BB%BA/</id>
    <published>2022-06-07T16:00:00.000Z</published>
    <updated>2022-06-09T14:06:46.384Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、了解HBase"><a href="#一、了解HBase" class="headerlink" title="一、了解HBase"></a>一、了解HBase</h2><p>官方文档：<a href="https://hbase.apache.org/book.html">https://hbase.apache.org/book.html</a></p><h3 id="1-1-HBase概述"><a href="#1-1-HBase概述" class="headerlink" title="1.1    HBase概述"></a>1.1    HBase概述</h3><blockquote><p>HBase 是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，用于存储海量的结构化或者半结构化，非结构化的数据</p><p>HBase是Hadoop的生态系统之一，是建立在Hadoop文件系统（HDFS）之上的分布式、面向列的数据库，通过利用Hadoop的文件系统提供容错能力。如果需要进行实时读写或者随机访问大规模的数据集的时候，会考虑使用HBase。</p><p>HBase作为Google Bigtable的开源实现，Google Bigtable利用GFS作为其文件存储系统类似，则HBase利用Hadoop HDFS作为其文件存储系统；Google通过运行MapReduce来处理Bigtable中的海量数据，同样，HBase利用Hadoop MapReduce来处理HBase中的海量数据；Google Bigtable利用Chubby作为协同服务，HBase利用Zookeeper作为对应。在2010年5月，成为apache顶级项目</p></blockquote><p><img src="https://s2.loli.net/2022/06/09/IYKfrEsFbvx2Q4A.png" alt="image-20220608204054001"></p><h3 id="1-2-HBase处理数据"><a href="#1-2-HBase处理数据" class="headerlink" title="1.2    HBase处理数据"></a>1.2    HBase处理数据</h3><blockquote><p>虽然Hadoop是一个高容错、高延时的分布式文件系统和高并发的批处理系统，但是它不适用于提供实时计算；</p><p>HBase是可以提供实时计算的分布式数据库，数据被保存在HDFS分布式文件系统上，由HDFS保证期高容错性;</p><p>但是再生产环境中，HBase是如何基于hadoop提供实时性呢？ </p><p>HBase上的数据是以StoreFile(HFile)二进制流的形式存储在HDFS上block块儿中；</p><p>但是HDFS并不知道的HBase用于存储什么，它只把存储文件认为是二进制文件，也就是说，HBase的存储数据对于HDFS文件系统是透明的。</p></blockquote><h3 id="1-3-HBase与HDFS"><a href="#1-3-HBase与HDFS" class="headerlink" title="1.3    HBase与HDFS"></a>1.3    HBase与HDFS</h3><blockquote><p>在下面的表格中，我们对HDFS与HBase进行比较：    </p></blockquote><table><thead><tr><th>HDFS</th><th>HBase</th></tr></thead><tbody><tr><td>HDFS适于存储大容量文件的分布式文件系统。</td><td>HBase是建立在HDFS之上的数据库。</td></tr><tr><td>HDFS不支持快速单独记录查找。</td><td>HBase提供在较大的表快速查找</td></tr><tr><td>HDFS提供了高延迟批量处理;没有批处理概念。</td><td>HBase提供了数十亿条记录低延迟访问单个行记录（随机存取）。</td></tr><tr><td>HDFS提供的数据只能顺序访问。</td><td>HBase内部使用哈希表和提供随机接入，并且其存储索引，可将在HDFS文件中的数据进行快速查找。</td></tr></tbody></table><p>Hbase—&gt;HashMap</p><h2 id="二、HBase相关概念"><a href="#二、HBase相关概念" class="headerlink" title="二、HBase相关概念"></a>二、HBase相关概念</h2><h3 id="2-1-分布式数据库"><a href="#2-1-分布式数据库" class="headerlink" title="2.1    分布式数据库"></a>2.1    分布式数据库</h3><p><img src="https://s2.loli.net/2022/06/09/N9gnjo1GSKQ2UY4.png" alt="image-20220609215840812"></p><h3 id="2-2-列式存储"><a href="#2-2-列式存储" class="headerlink" title="2.2    列式存储"></a>2.2    列式存储</h3><p><img src="https://s2.loli.net/2022/06/09/PmgjIZuvkKYE72C.png" alt="image-20220609215924536"></p><h3 id="2-3-稀疏性"><a href="#2-3-稀疏性" class="headerlink" title="2.3    稀疏性"></a>2.3    稀疏性</h3><blockquote><p>理解稀疏（rowkey）</p><p>HBase中需要根据行键、列族、列限定符和时间戳来确定一个单元格，因此，可以视为一个“<strong>四维坐标</strong>”，即**[行键, 列族, 列限定符, 时间戳]**</p></blockquote><p><img src="https://s2.loli.net/2022/06/09/kC42Rz8LeFw67vy.png" alt="稀疏性"></p><h3 id="2-4-数据模型"><a href="#2-4-数据模型" class="headerlink" title="2.4    数据模型"></a>2.4    数据模型</h3><blockquote><p>HBase通过表格的模式存储数据，每个表格由列和行组成，其中，每个列又被划分为若干个列族（colnum family），请参考下面的图：</p></blockquote><p><img src="https://s2.loli.net/2022/06/09/eHQcAD7hfqXNRk4.png" alt="image-20220609220036643"></p><blockquote><p>          <strong>表：</strong>HBase的数据同样是用表来组织的，表由行和列组成，列分为若干个列族，行和列的坐标交叉决定了一个单元格。</p><p>       <strong>行：</strong>每个表由若干行组成，每个行有一个行键作为这一行的唯一标识。访问表中的行只有三种方式：通过单个行键进行查询、通过一个行键的区间来访问、全表扫描。</p><p>       <strong>列族：</strong>一个HBase表被分组成许多“列族”的集合，它是基本的访问控制单元。</p><p>       <strong>列修饰符（列限定符）：</strong>列族里的数据通过列限定符（或列）来定位</p><p>       <strong>单元格：</strong>在HBase表中，通过行、列族和列限定符确定一个“单元格”（cell），单元格中存储的数据没有数据类型，<strong>总被视为字节数组byte[]</strong></p><p>       <strong>时间戳：</strong>每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引</p></blockquote><h4 id="2-4-1-Hbase数据模型"><a href="#2-4-1-Hbase数据模型" class="headerlink" title="2.4.1    Hbase数据模型"></a>2.4.1    Hbase数据模型</h4><blockquote><p>HBase将数据存放在带有标签的<strong>表</strong>中，表由<strong>行和列</strong>组成，行和列交叉确定一个<strong>单元格</strong>，单元格有<strong>版本号</strong>，版本号自动分配，为数据插入该单元格时的<strong>时间戳</strong>。单元格的内容没有数据类型，<strong>所有数据都被视为未解释的字节数组</strong>。</p><p>  表格中每一行有一个<strong>行键</strong>（也是字节数组，任何形式的数据都可以表示成字符串，比如数据结构进行序列化之后），<strong>整个表根据行键的字节序来排序</strong>，所有对表的访问必须通过行键。</p><p>  表中的列又划分为多个<strong>列族</strong>（column family），同一个列族的所有成员具有相同的前缀，具体的列由列修饰符标识，因此，<strong>列族和列修饰符</strong>合起来才可以表示某一列，比如：info:format、cotents:image</p></blockquote><p><img src="https://s2.loli.net/2022/06/09/JkNlnw1zLYGjP2u.png" alt="image-20220609220054289"></p><blockquote><p>在创建一个表的时候，列族必须作为模式定义的一部分预先给出，而<strong>列族是支持动态扩展的</strong>，也就是列族成员可以随后按需加入。物理上，所有的列族成员一起存放在文件系统上，所以实际上说HBase是面向列的数据库，更准确的应该是<strong>面向列族</strong>，调优和存储都是在列族这个层次上进行的。一般情况下，同一个列族的成员最后具有相同的访问模式和大小特征。</p><p>  总结起来，HBase表和我们熟知的RDBMS的表很像，不同之处在于：<strong>行按行键排序，列划分为列族，单元格有版本号，没有数据类型。</strong></p></blockquote><h4 id="2-4-2-Hbase数据坐标"><a href="#2-4-2-Hbase数据坐标" class="headerlink" title="2.4.2    Hbase数据坐标"></a>2.4.2    Hbase数据坐标</h4><blockquote><p>HBase中需要根据行键、列族、列限定符和时间戳来确定一个<strong>单元格(cell)<strong>，cell中的数据是没有类型的，全部是</strong>字节码</strong>形式存贮。，因此，可以视为一个“<strong>四维坐标</strong>”，即**[行键, 列族, 列限定符, 时间戳]**。</p></blockquote><p><img src="https://s2.loli.net/2022/06/09/kz6J29nxcSCYZlV.png" alt="image-20220609220127385"></p><blockquote><p>对于上图这样一个HBase表，其数据坐标举例如下：</p></blockquote><table><thead><tr><th>键</th><th>值</th></tr></thead><tbody><tr><td>[“201505003”, “Info”, “email”, 1174184619081]</td><td>“<a href="mailto:&#120;&#105;&#101;&#x40;&#113;&#113;&#46;&#x63;&#x6f;&#109;">&#120;&#105;&#101;&#x40;&#113;&#113;&#46;&#x63;&#x6f;&#109;</a>”</td></tr><tr><td>[“201505003”, “Info”, “email”, 1174184620720]</td><td>“<a href="mailto:&#121;&#x6f;&#x75;&#x40;&#49;&#x36;&#x33;&#46;&#99;&#x6f;&#x6d;">&#121;&#x6f;&#x75;&#x40;&#49;&#x36;&#x33;&#46;&#99;&#x6f;&#x6d;</a>”</td></tr></tbody></table><h4 id="2-4-3-HBase区域"><a href="#2-4-3-HBase区域" class="headerlink" title="2.4.3    HBase区域"></a>2.4.3    HBase区域</h4><blockquote><p>HBase自动把表水平划分为<strong>区域</strong>（Region），每个区域都是有若干连续行构成的，一个区域由<strong>所属的表、起始行、终止行（不包括这行）</strong>三个要素来表示。</p><p>  一开始，一个表只有一个区域，但是随着数据的增加，区域逐渐变大，等到它超出设定的阈值大小，就会在某行的边界上进行拆分，分成两个大小<strong>基本相同</strong>的区域。然后随着数据的再增加，区域就不断的增加，如果超出了单台服务器的容量，就可以把一些区域放到其他节点上去，构成一个集群。也就是说：<strong>集群中的每个节点（Region Server）管理整个表的若干个区域</strong>。所以，我们说：<strong>区域是HBase集群上分布数据的最小单位</strong>。</p></blockquote><p><img src="https://s2.loli.net/2022/06/09/4lDPXtMkmfhdz2C.png" alt="image-20220609220146716"></p><h2 id="三、HBase系统架构"><a href="#三、HBase系统架构" class="headerlink" title="三、HBase系统架构"></a>三、HBase系统架构</h2><h3 id="3-1-架构图"><a href="#3-1-架构图" class="headerlink" title="3.1    架构图"></a>3.1    架构图</h3><p><img src="https://s2.loli.net/2022/06/09/Dhl7Z3jRKHa8F2p.png" alt="image-20220609220202453"></p><h3 id="3-2-组件介绍"><a href="#3-2-组件介绍" class="headerlink" title="3.2    组件介绍"></a>3.2    组件介绍</h3><p>HBase由三种类型的服务器以主从模式构成：</p><ul><li>Region Server：负责数据的读写服务，用户通过与Region server交互来实现对数据的访问。</li><li>HBase HMaster：负责Region的分配及数据库的创建和删除等操作。</li><li>ZooKeeper：负责维护集群的状态（某台服务器是否在线，服务器之间数据的同步操作及master的选举等）。</li></ul><p>HDFS的DataNode负责存储所有Region Server所管理的数据，即HBase中的所有数据都是以HDFS文件的形式存储的。出于使Region server所管理的数据更加本地化的考虑，Region server是根据DataNode分布的。HBase的数据在写入的时候都存储在本地。但当某一个region被移除或被重新分配的时候，就可能产生数据不在本地的情况。这种情况只有在所谓的compaction之后才能解决。</p><h4 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h4><blockquote><p>包含访问HBase的接口并维护cache来加快对HBase的访问</p></blockquote><h4 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h4><blockquote><p>保证任何时候，集群中只有一个master</p><p>存贮所有Region的寻址入口。</p><p>实时监控Region server的上线和下线信息。并实时通知Master</p><p>存储HBase的schema和table元数据</p></blockquote><h4 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h4><blockquote><p>为Region server分配region</p><p>负责Region server的负载均衡</p><p>发现失效的Region server并重新分配其上的region</p><p>管理用户对table的增删改操作</p></blockquote><h4 id="RegionServer"><a href="#RegionServer" class="headerlink" title="RegionServer"></a>RegionServer</h4><blockquote><p>Region server维护region，处理对这些region的IO请求</p><p>Region server负责切分在运行过程中变得过大的region　</p></blockquote><h4 id="HLog-WAL-log-："><a href="#HLog-WAL-log-：" class="headerlink" title="HLog(WAL log)："></a>HLog(WAL log)：</h4><blockquote><p>HLog文件就是一个普通的Hadoop Sequence File，Sequence File 的Key是 HLogKey对象，HLogKey中记录了写入数据的归属信息，除了table和 region名字外，同时还包括sequence number和timestamp，timestamp是” 写入时间”，sequence number的起始值为0，或者是最近一次存入文件系 统sequence number。</p><p>HLog SequeceFile的Value是HBase的KeyValue对象，即对应HFile中的 KeyValue</p></blockquote><h4 id="Region"><a href="#Region" class="headerlink" title="Region"></a>Region</h4><blockquote><p>HBase自动把表水平划分成多个区域(region)，每个region会保存一个表里面某段连续的数据；每个表一开始只有一个region，随着数据不断插 入表，region不断增大，当增大到一个阀值的时候，region就会等分会两个新的region（裂变）；</p><p>当table中的行不断增多，就会有越来越多的region。这样一张完整的表被保存在多个Regionserver上。</p></blockquote><h4 id="Memstore-与-storefile"><a href="#Memstore-与-storefile" class="headerlink" title="Memstore 与 storefile"></a>Memstore 与 storefile</h4><blockquote><ol><li><p>一个region由多个store组成，一个store对应一个CF（列簇）</p></li><li><p>store包括位于内存中的memstore和位于磁盘的storefile写操作先写入 memstore，当memstore中的数据达到某个阈值，hregionserver会启动 flashcache进程写入storefile，每次写入形成单独的一个storefile</p></li><li><p>当storefile文件的数量增长到一定阈值后，系统会进行合并（minor、 major compaction），在合并过程中会进行版本合并和删除工作 （majar），形成更大的storefile。</p></li><li><p>当一个region所有storefile的大小和超过一定阈值后，会把当前的region 分割为两个，并由hmaster分配到相应的regionserver服务器，实现负载均衡。</p></li><li><p>客户端检索数据，先在memstore找，找不到再找storefile</p></li><li><p>HRegion是HBase中分布式存储和负载均衡的最小单元。最小单元就表 示不同的HRegion可以分布在不同的HRegion server上。</p></li><li><p>HRegion由一个或者多个Store组成，每个store保存一个columns family。</p></li><li><p>每个Strore又由一个memStore和0至多个StoreFile组成。</p></li></ol><p>如图：StoreFile 以HFile格式保存在HDFS上。</p></blockquote><p><img src="https://s2.loli.net/2022/06/09/ckzY96POTFUQwtK.png" alt="image-20220609220217852"></p><p><img src="https://s2.loli.net/2022/06/09/B5yDYaUn7vM3lif.png" alt="image-20220609220227154"></p><h3 id="3-3-理解难点"><a href="#3-3-理解难点" class="headerlink" title="3.3    理解难点"></a>3.3    理解难点</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">　　1、flush刷新在HDFS上呈现究竟是怎么刷新的呢？？</span><br><span class="line">　　　　我们目前刚刚学习的时候，添加数据，都是一条一条的put进去，而我们在put的数据比较少（小于128M）的时候，我们put完去HDFS上并未查看到我们put的文件，这是因为数据还在内存中，也就是还在memStore中，所以要想在HDFS中查看到，我们必须手动刷新到磁盘中，这是将memStore的数据刷新到StoreFile中去，这样我们在HDFS中就可以查看到了。　　</span><br><span class="line"></span><br><span class="line">　　2、为什么Hbase不可以使用像Mysql那样进行查询？？</span><br><span class="line">　　　　首先，我们应该可以感受到，我们在插入的时候，每行数据，有多少列，列名叫什么完全是我们自己定义的，之所以不支持像MySql那样对列进行查询和操作，因为不确定列的个数和名称。</span><br><span class="line"></span><br><span class="line">　　3、数据最后存在HDFS上的，HDFS不支持删改，为什么Hbase就可以呢？？</span><br><span class="line">　　　　这里有个思想误区，的确，数据是以HFile形式存在HDFS上的，而且HDFS的确是不支持删改的，但是为什么Hbase就支持呢？首先，这里的删除并不是真正意义上的对数据进行删除，而是对数据进行打上标记，我们再去查的时，就不会查到这个打过标记的数据，这个数据Hmaster会每隔1小时清理。修改是put两次，Hbase会取最新的数据，过期数据也是这个方式被清理。</span><br></pre></td></tr></table></figure><h2 id="四、HBase1-7-1安装搭建"><a href="#四、HBase1-7-1安装搭建" class="headerlink" title="四、HBase1.7.1安装搭建"></a>四、HBase1.7.1安装搭建</h2><h3 id="4-1-hbase下载"><a href="#4-1-hbase下载" class="headerlink" title="4.1 hbase下载"></a>4.1 hbase下载</h3><p>官网下载地址：<a href="https://www.apache.org/dyn/closer.lua/hbase/1.7.1/hbase-1.7.1-bin.tar.gz">https://www.apache.org/dyn/closer.lua/hbase/1.7.1/hbase-1.7.1-bin.tar.gz</a></p><p><img src="https://s2.loli.net/2022/06/09/duV1iZ2pMrFfqCO.png" alt="image-20220609220320634"></p><h3 id="4-2-前期准备（Hadoop-zookeeper-jdk）"><a href="#4-2-前期准备（Hadoop-zookeeper-jdk）" class="headerlink" title="4.2 前期准备（Hadoop,zookeeper,jdk）"></a>4.2 前期准备（Hadoop,zookeeper,jdk）</h3><blockquote><p>启动hadoop</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure><blockquote><p>验证</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">http://master:50070</span><br></pre></td></tr></table></figure><blockquote><p>启动zookeeper（三台分别启动）</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><blockquote><p>检查状态</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">zkServer.sh status</span><br></pre></td></tr></table></figure><h3 id="4-3-搭建Hbase"><a href="#4-3-搭建Hbase" class="headerlink" title="4.3    搭建Hbase"></a>4.3    搭建Hbase</h3><h4 id="1、上传解压"><a href="#1、上传解压" class="headerlink" title="1、上传解压"></a>1、上传解压</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf hbase-1.4.6-bin.tar.gz</span><br></pre></td></tr></table></figure><h4 id="2、配置环境变量"><a href="#2、配置环境变量" class="headerlink" title="2、配置环境变量"></a>2、配置环境变量</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HBASE_HOME=/usr/local/soft/hbase-1.7.1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">HBASE_HOME/bin</span></span><br></pre></td></tr></table></figure><blockquote><p>source &#x2F;etc&#x2F;profile</p></blockquote><h4 id="3、修改hbase-env-sh文件"><a href="#3、修改hbase-env-sh文件" class="headerlink" title="3、修改hbase-env.sh文件"></a>3、修改hbase-env.sh文件</h4><blockquote><p>增加java配置</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/soft/jdk1.8.0_171</span><br></pre></td></tr></table></figure><blockquote><p>关闭默认zk配置（原本是注释的，放开修改false）</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure><h4 id="4、修改hbase-site-xml文件"><a href="#4、修改hbase-site-xml文件" class="headerlink" title="4、修改hbase-site.xml文件"></a>4、修改hbase-site.xml文件</h4><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1,node2,master<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br></pre></td></tr></table></figure><h4 id="5、修改regionservers文件"><a href="#5、修改regionservers文件" class="headerlink" title="5、修改regionservers文件"></a>5、修改regionservers文件</h4><blockquote><p>如果是伪分布式版本，增加master即可</p></blockquote><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">node1</span><br><span class="line">node2</span><br></pre></td></tr></table></figure><h4 id="6、同步到所有节点（如果是伪分布式不需要同步）"><a href="#6、同步到所有节点（如果是伪分布式不需要同步）" class="headerlink" title="6、同步到所有节点（如果是伪分布式不需要同步）"></a>6、同步到所有节点（如果是伪分布式不需要同步）</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scp -r hbase-1.7.1 node1:`pwd`</span><br><span class="line">scp -r hbase-1.7.1 node2:`pwd`</span><br></pre></td></tr></table></figure><h4 id="7、启动hbase集群-，-在master上执行"><a href="#7、启动hbase集群-，-在master上执行" class="headerlink" title="7、启动hbase集群 ， 在master上执行"></a>7、启动hbase集群 ， 在master上执行</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/09/6ybGojvBhIMR2VP.png" alt="image-20220609220348414"></p><h4 id="8、验证hbase"><a href="#8、验证hbase" class="headerlink" title="8、验证hbase"></a>8、验证hbase</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http://master:16010</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/09/GetFKMrWAZJDdqC.png" alt="image-20220609220410102"></p><blockquote><p>hbase日志文件所在的目录: &#x2F;usr&#x2F;local&#x2F;soft&#x2F;hbase-1.7.1&#x2F;logs</p></blockquote><h4 id="9、关闭集群的命令"><a href="#9、关闭集群的命令" class="headerlink" title="9、关闭集群的命令"></a>9、关闭集群的命令</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">stop-hbase.sh</span><br></pre></td></tr></table></figure><h3 id="4-4-启动顺序"><a href="#4-4-启动顺序" class="headerlink" title="4.4    启动顺序"></a>4.4    启动顺序</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">启动顺序</span><br><span class="line">Hadoop及hbase集群启动顺序 zookeepeer -&gt; hadoop -&gt; hbase</span><br><span class="line"></span><br><span class="line">停止顺序</span><br><span class="line">Hadoop及hbase集群关闭顺序 hbase -&gt; hadoop -&gt; zookeepeer</span><br></pre></td></tr></table></figure><h3 id="4-5-重置hbase"><a href="#4-5-重置hbase" class="headerlink" title="4.5    重置hbase"></a>4.5    重置hbase</h3><h5 id="1、关闭hbase集群"><a href="#1、关闭hbase集群" class="headerlink" title="1、关闭hbase集群"></a>1、关闭hbase集群</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1)杀死进程</span><br><span class="line"></span><br><span class="line">2)stop-hbase.sh</span><br></pre></td></tr></table></figure><h5 id="2、删除数据-hdfs"><a href="#2、删除数据-hdfs" class="headerlink" title="2、删除数据   hdfs"></a>2、删除数据   hdfs</h5> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs dfs -rmr /hbase</span><br></pre></td></tr></table></figure><h5 id="3、删除元数据-zk"><a href="#3、删除元数据-zk" class="headerlink" title="3、删除元数据 zk"></a>3、删除元数据 zk</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zkCli.sh</span><br><span class="line">rmr /hbase</span><br></pre></td></tr></table></figure><h5 id="4、重新启动hbase"><a href="#4、重新启动hbase" class="headerlink" title="4、重新启动hbase"></a>4、重新启动hbase</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure><h5 id="时间同步"><a href="#时间同步" class="headerlink" title="时间同步"></a>时间同步</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install ntp -y</span><br><span class="line"></span><br><span class="line">ntpdate -u time.windows.com</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">对学习Hbase的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hbase" scheme="http://example.com/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>Hive优化</title>
    <link href="http://example.com/2022/06/07/Hive%E4%BC%98%E5%8C%96/"/>
    <id>http://example.com/2022/06/07/Hive%E4%BC%98%E5%8C%96/</id>
    <published>2022-06-06T16:00:00.000Z</published>
    <updated>2022-06-08T07:13:17.011Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Hive优化"><a href="#Hive优化" class="headerlink" title="Hive优化"></a>Hive优化</h2><h2 id="1-1-hive的随机抓取策略"><a href="#1-1-hive的随机抓取策略" class="headerlink" title="1.1    hive的随机抓取策略"></a>1.1    <strong>hive的随机抓取策略</strong></h2><blockquote><p>理论上来说，Hive中的所有sql都需要进行mapreduce，但是hive的抓取策略帮我们<br>省略掉了这个过程，把切片split的过程提前帮我们做了。<br>set hive.fetch.task.conversion&#x3D;none;<br>(一旦进行这么设置，select字段名也是需要进行mapreduce的过程，默认是more)</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Fetch</span>抓取的模式</span><br><span class="line">可以通过 <span class="keyword">set</span> hive.fetch.task.conversion查看，有以下<span class="number">3</span>种模式：</span><br><span class="line"></span><br><span class="line"><span class="keyword">none</span>：所有涉及hdfs的读取查询都走mapreduce任务；</span><br><span class="line">mininal：在进行简单的<span class="keyword">select</span> <span class="operator">*</span>，简单的过滤或涉及分区字段的过滤时走mr；</span><br><span class="line">more:在mininal模式的基础上，增加了针对查询语句字段进行一些别名的计算操作。</span><br><span class="line">以下HQL，mininal模式与more模式下都不会走mr任务:</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">sale_ord_id,</span><br><span class="line">store_id</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">test_table</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">dt <span class="operator">=</span> <span class="string">&#x27;2021-01-01&#x27;</span></span><br><span class="line"> limit <span class="number">10</span>;</span><br><span class="line"> </span><br><span class="line">以下HQL,mininal模式会走mr任务，more模式不会：</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">sale_ord_id,</span><br><span class="line">store_id,</span><br><span class="line">if(store_id <span class="operator">&gt;</span> <span class="number">20</span>,<span class="number">1</span>,<span class="number">0</span>) <span class="keyword">as</span> store_id_new</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">test_table</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">dt <span class="operator">=</span> <span class="string">&#x27;2021-01-01&#x27;</span></span><br><span class="line"> limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure><blockquote><p>查看怎么将一个sql转化成一个MR任务的<br>explain sql语句<br>例如：<br>explain select count(*) from stu_dy1_1;<br>更加详细的查看，例如：<br><strong>explain extended select count(*) from stu_dy1_1;</strong><br>当你输入一个sql语句的时候，hive会将对其关键字进行截串，截完串之后，变成<br>都是一些TOK开头的一些东西，然后经过这样的抽象语法树，再转成具体的查询块，<br>最后变成逻辑查询计划</p></blockquote><h2 id="1-2-本地运行模式"><a href="#1-2-本地运行模式" class="headerlink" title="1.2    本地运行模式"></a>1.2    本地运行模式</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">大多数的 Hadoop Job 是需要 Hadoop 提供的完整的可扩展性来处理大数据集的。不过，</span><br><span class="line">有时 Hive 的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能</span><br><span class="line">会比实际 job 的执行时间要多的多。对于大多数这种情况， Hive 可以通过本地模式在单台机</span><br><span class="line">器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</span><br><span class="line">用户可以通过设置 hive.exec.mode.local.auto 的值为 true ，来让 Hive 在适当的时候自动</span><br><span class="line">启动这个优化。</span><br><span class="line"></span><br><span class="line">本地模式运行比集群模式块很多，33秒的任务降到2秒</span><br><span class="line">更改为本地模式：</span><br><span class="line">hive&gt; set hive.exec.mode.local.auto=true</span><br><span class="line">注意：</span><br><span class="line">hive&gt; set hive.exec.mode.local.auto.inputbytes.max=134217728     ---&gt; 128M</span><br><span class="line">（默认值就是128）</span><br><span class="line">表示加载文件的最大值，若大于该配置仍然会以集群的方式去运行。</span><br><span class="line">97万行数据，50MB</span><br><span class="line">当我们开发或者测试阶段，可以去使用本地模式进行运行，默认是集群模式</span><br><span class="line">但是，这里有个问题，当我们去更改为本地模式的时候，在8088的页面上就看不到</span><br><span class="line">任务的执行情况了。</span><br><span class="line"></span><br><span class="line">测试：select count(*) from emp group by deptno;</span><br></pre></td></tr></table></figure><h2 id="1-3-并行计算"><a href="#1-3-并行计算" class="headerlink" title="1.3    并行计算"></a>1.3    <strong>并行计算</strong></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">通过设置以下参数开启并行模式（默认是false）</span><br><span class="line">set hive.exec.parallel=true;</span><br><span class="line"></span><br><span class="line">注意：hive.exec.parallel.thread.number</span><br><span class="line">(一次SQl计算中允许并行执行的job个数最大值，默认是8个)</span><br><span class="line"></span><br><span class="line">举例：</span><br><span class="line">select t1.n1,t2.n2 from (select count(ename) as n1 from emp) t1,(select count(dname) as n2 from dept) t2;</span><br><span class="line">注意，有时候开启并行计算运行时间并没有不开启的快，那是因为，资源的问题。</span><br><span class="line">需要两套资源，资源申请会浪费点时间，最多可以并行8个，默认是8个。</span><br><span class="line">所以，并行的越多，不一定是越快，因为它涉及到一个资源申请的策略。</span><br></pre></td></tr></table></figure><h2 id="1-4-严格模式-理解为增加一些限制"><a href="#1-4-严格模式-理解为增加一些限制" class="headerlink" title="1.4    严格模式(理解为增加一些限制)"></a>1.4    <strong>严格模式(理解为增加一些限制)</strong></h2><p>​    <strong>1.什么是Hive的严格模式</strong><br>​        hive中的一种模式,在该模式下禁止一些不好SQL的执行。</p><p>​    <strong>2.Hive的严格模式不允许哪些SQL执行</strong><br>​        <strong>2.1 禁止分区表全表扫描</strong><br>               分区表往往数据量大,如果不加分区查询会带来巨大的资源消耗 。例如以下分区表<br>               SELECT DISTINCT(planner_id) FROM fracture_ins WHERE planner_id&#x3D;5;</p><p>​                报错如下:<br>​               FAILED: Error in semantic analysis: No Partition Predicate Found for Alias “fracture_ins” Table “fracture_ins</p><p>​               解决如下:<br>​              SELECT DISTINCT(planner_id) FROM fracture_ins WHERE planner_id&#x3D;5 AND hit_date&#x3D;20120101;</p><p>​      <strong>2.2 禁止排序不加limit</strong><br>​        排序最终是要都进到一个Reduce中操作,防止reducer额外执行很长一段时间<br>​        SELECT * FROM fracture_ins WHERE hit_date&gt;2012 ORDER BY planner_id;<br>​        出现如下错误<br>​               FAILED: Error in semantic analysis: line 1:56 In strict mode,limit must be specified if ORDER BY is present planner_id<br>​        解决方案就是增加一个limit关键字：<br>​               hive&gt; SELECT * FROM fracture_ins WHERE hit_date&gt;2012 ORDER BY planner_id LIMIT 100000;</p><p>​      <strong>2.3 禁止笛卡尔积</strong><br>​          笛卡尔积是什么: A&#x3D;{a,b}, B&#x3D;{0,1,2}，则 A×B&#x3D;{(a, 0), (a, 1), (a, 2), (b, 0), (b, 1), (b, 2)}</p><p>​          SELECT * FROM fracture_act JOIN fracture_ads;<br>​        解决方法<br>​        SELECT * FROM fracture_act JOIN fracture_ads WHERE fracture_act.planner_id &#x3D; fracture_ads.planner_id;</p><p><strong>3.Hive的严格模式怎样开启</strong></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> 查看当前严格模式的状态</span><br><span class="line"><span class="keyword">set</span> hive.mapred.mode;</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 设置为严格模式</span><br><span class="line"><span class="keyword">set</span> hive.mapred.mode<span class="operator">=</span>strict;</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 设置为非严格模式</span><br><span class="line"><span class="keyword">set</span> hive.mapred.mode<span class="operator">=</span>nostrict;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">注意，这里的严格模式和动态分区的那个严格模式半毛钱关系没有）</span><br><span class="line">通过设置以下参数开启严格模式：</span><br><span class="line">set hive.mapred.mode=strict;</span><br><span class="line">(默认为：nonstrict非严格模式)</span><br><span class="line"></span><br><span class="line">查询限制：</span><br><span class="line">1、对于分区表，必须添加where对于分区字段的条件过滤</span><br><span class="line">2、order by 语句必须包含limit输出限制</span><br><span class="line">3、限制执行笛卡尔积的查询</span><br><span class="line">这些限制是帮助我们提高查询效率的。</span><br></pre></td></tr></table></figure><h2 id="1-5-Hive排序"><a href="#1-5-Hive排序" class="headerlink" title="1.5    Hive排序"></a>1.5    <strong>Hive排序</strong></h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> 对于查询结果做全排序，只允许有一个reduce处理</span><br><span class="line">（注意：它会把我们所有的字段或者查询结果全部放在一个reduce里进行处理</span><br><span class="line">当数据量较大时候，有可能reduce执行不完，所以，我们以后把这个给弃用掉）</span><br><span class="line"></span><br><span class="line"><span class="operator">*</span><span class="operator">*</span>   sort <span class="keyword">by</span> 对于单个reduce进行排序 但是我们将每个reduce里面进行排序，没有考虑到</span><br><span class="line">每个reduce之间的排序。所以我们引出下一个</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span>   distribute <span class="keyword">by</span> 分区排序，通常结合sort <span class="keyword">by</span>一起使用</span><br><span class="line">（distribute <span class="keyword">by</span> <span class="keyword">column</span> sort <span class="keyword">by</span> <span class="keyword">column</span> <span class="keyword">asc</span><span class="operator">|</span><span class="keyword">desc</span>）</span><br><span class="line"></span><br><span class="line">cluster <span class="keyword">by</span> 相当于distribute <span class="keyword">by</span> <span class="operator">+</span> sort <span class="keyword">by</span>  (注意，虽然是两个结合，但是我们也不去用它</span><br><span class="line">原因很简单，cluster <span class="keyword">by</span>不能通过<span class="keyword">asc</span> <span class="keyword">desc</span>的方式指定排序方式规则)</span><br></pre></td></tr></table></figure><h2 id="1-6-Hive-join数据倾斜"><a href="#1-6-Hive-join数据倾斜" class="headerlink" title="1.6    Hive join数据倾斜"></a>1.6    Hive join数据倾斜</h2><p>1、小表join小表 不管他</p><p>2、小表join大表   map-join</p><p>3、大表join大表  map-side</p><p>考虑会不会发生reduce,并且考虑reduce压力是否大（是否会出现某个reduce数据量庞大的情况）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">join</span>计算的时候，将小表（驱动表）放在<span class="keyword">join</span>的左边</span><br><span class="line">Map <span class="keyword">join</span>：在Map端完成<span class="keyword">join</span></span><br><span class="line">两种实现方式：</span><br><span class="line"><span class="number">1</span>、<span class="keyword">sql</span>方式，在<span class="keyword">sql</span>语句中添加Mapjoin标记（mapjoin hint）</span><br><span class="line"><span class="operator">&gt;&gt;</span>语法：</span><br><span class="line"><span class="keyword">select</span> <span class="comment">/*+MAPJOIN(smallTable)*/</span> smallTable.key bigTable.value <span class="keyword">from</span> smallTable <span class="keyword">join</span> bigTable <span class="keyword">on</span> smallTable.key<span class="operator">=</span>bigTable.key;</span><br><span class="line"><span class="number">2</span>、自动开启mapjoin</span><br><span class="line">通过修改以下配置启用自动的mapjoin：</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line">(注意：该参数为<span class="literal">true</span>的时候，Hive自动对左边的表统计量，如果</span><br><span class="line">是小表，就加入到内存，即对小表使用Mapjoin)</span><br><span class="line"></span><br><span class="line">相关配置参数</span><br><span class="line">　　hive.mapjoin.smalltable.filesize;(默认<span class="number">25</span>M,大表小表判断的阈值，如果表的大小小于该值则会被加载到内存中运行。)</span><br><span class="line">　　hive.ignore,mapjoin.hint;(默认值：<span class="literal">true</span>;是否忽略mapjoin hint的标记)</span><br><span class="line">　　hive.auto.convert.join.noconditionaltask;(默认值：<span class="literal">true</span>；将普通的<span class="keyword">join</span>转换为mapjoin时，是否将多个mapjoin转化为一个mapjoin)</span><br><span class="line">　　hive.auto.convert.join.noconditionaltask.size;(将多个mapjoin转化为一个mapjoin时，这个表的最大值)</span><br><span class="line"><span class="number">3</span>、尽可能使用相同的连接键，如果不同，多一个<span class="keyword">join</span>就会多开启一个mapreduce，执行速度变得慢。</span><br><span class="line"><span class="number">4</span>、大表<span class="keyword">join</span>大表（当两个都是大表的时候，只能发生reduce了，但是这里有两个优化策略）（面试的时候说，加分）</span><br><span class="line">　　a: 空key过滤:</span><br><span class="line">　　　　有时<span class="keyword">join</span>超时是因为某些key对应的数据太多,而相同key对应的数据都会发送到相同的 reducer上,从而导致内存不够。</span><br><span class="line">　　　　此时我们应该仔细分析这些异常的key,很多情况下,这些key对应的数据是异常数据,我们需要在<span class="keyword">SQL</span>语句中进行过滤。</span><br><span class="line">　　　　但是这个的前提条件是异常数据，但是我们一般拿到的数据都是经过ETL数据清洗过后的，一般影响不大，面试的时候可以说。</span><br><span class="line">　　b: 空key转换:</span><br><span class="line">　　　　有时虽然某个key为空对应的数据很多,但是相应的数据不是异常数据,必须要包含在<span class="keyword">join</span>的结果中,</span><br><span class="line">　　　　此时我们可以表a中key为空的字段赋随机的值,使得数据随机均匀地分不到不同的 reducer上。</span><br><span class="line">　　　　但是我们一般拿到的数据都是经过ETL数据清洗过后的，规则数据，一般影响不大，面试的时候可以说。</span><br><span class="line"><span class="number">5</span>、Map<span class="operator">-</span>Side聚合</span><br><span class="line">通过设置以下参数开启在Map端的聚合</span><br><span class="line"><span class="keyword">set</span> hive.map.aggr<span class="operator">=</span><span class="literal">true</span>;（一定要进行开启，虽然进行了两个mapreduce，但是当数据倾斜发生的时候，很多时候会根本跑不出结果，卡死在<span class="number">99</span><span class="operator">%</span>或者<span class="number">100</span><span class="operator">%</span>，慢总比出不来结果要好）！！！！！！！</span><br><span class="line">相关配置参数</span><br><span class="line">　　hive. groupby mapaggr. checkinterval;</span><br><span class="line">　　map端 igroup <span class="keyword">by</span>执行聚合时处理的多少行数据(默认:<span class="number">10000</span></span><br><span class="line">　　hive.map.aggr.hash.min.reduction;比例(若聚合之后的数据<span class="number">100</span>大该<span class="number">0.5</span>,map端聚合使用的内存的最大值</span><br><span class="line">　　hive.mapaggr.hashforce.flush.memory.threshold;map端做聚合操作是has表的最大可用内容,大于该值则会触发fush</span><br><span class="line">　　hive.groupby.skewindata<span class="operator">-</span>是否对 GroupBy产生的数据倾斜做优化,默认为<span class="literal">false</span>(十分重要！！！)</span><br><span class="line"><span class="number">6</span>、数据倾斜，尽可能地让我们的数据散列到不同的reduce里面去,负载均衡</span><br></pre></td></tr></table></figure><h2 id="1-7-合并小文件"><a href="#1-7-合并小文件" class="headerlink" title="1.7    合并小文件"></a>1.7    <strong>合并小文件</strong></h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">Hive优化</span><br><span class="line">合并小文件</span><br><span class="line">文件数目小,容易在文件存储端造成压力,给hdfs造成压力,影响效率</span><br><span class="line">设置合并属性</span><br><span class="line">　　是否合并map输出文件: hive.merge.mapfiles<span class="operator">=</span><span class="literal">true</span></span><br><span class="line">　　是否合并reduce输出文件: hive.merge.mapredfiles<span class="operator">=</span><span class="literal">true</span></span><br><span class="line">　　合并文件的大小: hive.merge.size.per.task<span class="operator">=</span><span class="number">256</span><span class="operator">*</span><span class="number">1000</span><span class="operator">*</span><span class="number">1000</span></span><br><span class="line">去重统计</span><br><span class="line">数据量小的时候无所谓,数据量大的情况下,由于 COUNT <span class="keyword">DISTINCT</span>操作需要用一个 Reduce Task来完成,</span><br><span class="line">这一个 Reduce需要处理的数据量太大,就会导致整个JOb很难完成,一般 COUNT <span class="keyword">DISTINCT</span>使用先 <span class="keyword">GROUP</span> <span class="keyword">BY</span>再COUNT的方式替换</span><br></pre></td></tr></table></figure><h2 id="1-8-控制map和reduce的数量-一般情况下我们不去动它"><a href="#1-8-控制map和reduce的数量-一般情况下我们不去动它" class="headerlink" title="1.8    控制map和reduce的数量(一般情况下我们不去动它)"></a>1.8    <strong>控制map和reduce的数量(一般情况下我们不去动它)</strong></h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">控制Hive中Map以及 Reduce的数量</span><br><span class="line">Map数量相关的参数</span><br><span class="line">mapred.max.split.size;一个split的最大值,即每个map处理文件的最大值</span><br><span class="line">mapred.min.split.size.per.node个节点上split的最小值</span><br><span class="line">mapred.min.split.size.per.rack一个机架上spit的最小值</span><br><span class="line">Reduce数量相关的参数</span><br><span class="line">mapred.reduce.tasks;强制指定reduce任务的数量</span><br><span class="line">hive.exec.reducers.bytes.per.reducer每个reduce任务处理的数据量</span><br><span class="line">hive.exec.reducers.max每个任务最大的reduce数</span><br></pre></td></tr></table></figure><h2 id="1-9-JVM重用"><a href="#1-9-JVM重用" class="headerlink" title="1.9    JVM重用"></a>1.9    <strong>JVM重用</strong></h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">当我们的小文件个数过多，task个数过多，需要申请的资源过多的时候，我们可以先申请一部分资源，全部执行完毕后再释放，</span><br><span class="line">比我们申请一个释放一个要快。</span><br><span class="line">通过 <span class="keyword">set</span> mapred.job.reuse.jvm.num.tasks<span class="operator">=</span>n;来设置</span><br><span class="line">（n为task插槽个数）</span><br><span class="line">缺点：</span><br><span class="line">设置开启后，task插槽会一直占用资源，无论是否有task进行，直到所有的task,</span><br><span class="line">即整个job全部执行完毕后，才会释放所有的task插槽，所以我们要合理地设置这个n</span><br><span class="line">(比如，我们设置申请了<span class="number">10</span>个，但是现在来了<span class="number">6</span>个，剩下<span class="number">4</span>个插槽会在job全部执行完毕之前一直占用资源)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">对学习Hive的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hive" scheme="http://example.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>连续登陆问题</title>
    <link href="http://example.com/2022/06/07/%E8%BF%9E%E7%BB%AD%E7%99%BB%E5%BD%95%E9%97%AE%E9%A2%98/"/>
    <id>http://example.com/2022/06/07/%E8%BF%9E%E7%BB%AD%E7%99%BB%E5%BD%95%E9%97%AE%E9%A2%98/</id>
    <published>2022-06-06T16:00:00.000Z</published>
    <updated>2022-06-08T06:55:31.062Z</updated>
    
    <content type="html"><![CDATA[<h4 id="连续登陆问题"><a href="#连续登陆问题" class="headerlink" title="连续登陆问题"></a>连续登陆问题</h4><blockquote><p>在电商、物流和银行可能经常会遇到这样的需求：统计用户连续交易的总额、连续登陆天数、连续登陆开始和结束时间、间隔天数等</p></blockquote><h5 id="数据："><a href="#数据：" class="headerlink" title="数据："></a>数据：</h5><blockquote><p>注意：每个用户每天可能会有多条记录</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iddatestr  amount</span><br><span class="line">1,2019-02-08,6214.23 </span><br><span class="line">1,2019-02-08,6247.32 </span><br><span class="line">1,2019-02-09,85.63 </span><br><span class="line">1,2019-02-09,967.36 </span><br><span class="line">1,2019-02-10,85.69 </span><br><span class="line">1,2019-02-12,769.85 </span><br><span class="line">1,2019-02-13,943.86 </span><br><span class="line">1,2019-02-14,538.42</span><br><span class="line">1,2019-02-15,369.76</span><br><span class="line">1,2019-02-16,369.76</span><br><span class="line">1,2019-02-18,795.15</span><br><span class="line">1,2019-02-19,715.65</span><br><span class="line">1,2019-02-21,537.71</span><br><span class="line">2,2019-02-08,6214.23 </span><br><span class="line">2,2019-02-08,6247.32 </span><br><span class="line">2,2019-02-09,85.63 </span><br><span class="line">2,2019-02-09,967.36 </span><br><span class="line">2,2019-02-10,85.69 </span><br><span class="line">2,2019-02-12,769.85 </span><br><span class="line">2,2019-02-13,943.86 </span><br><span class="line">2,2019-02-14,943.18</span><br><span class="line">2,2019-02-15,369.76</span><br><span class="line">2,2019-02-18,795.15</span><br><span class="line">2,2019-02-19,715.65</span><br><span class="line">2,2019-02-21,537.71</span><br><span class="line">3,2019-02-08,6214.23 </span><br><span class="line">3,2019-02-08,6247.32 </span><br><span class="line">3,2019-02-09,85.63 </span><br><span class="line">3,2019-02-09,967.36 </span><br><span class="line">3,2019-02-10,85.69 </span><br><span class="line">3,2019-02-12,769.85 </span><br><span class="line">3,2019-02-13,943.86 </span><br><span class="line">3,2019-02-14,276.81</span><br><span class="line">3,2019-02-15,369.76</span><br><span class="line">3,2019-02-16,369.76</span><br><span class="line">3,2019-02-18,795.15</span><br><span class="line">3,2019-02-19,715.65</span><br><span class="line">3,2019-02-21,537.71</span><br></pre></td></tr></table></figure><h5 id="建表语句"><a href="#建表语句" class="headerlink" title="建表语句"></a>建表语句</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> deal_tb(</span><br><span class="line">    id string</span><br><span class="line">    ,datestr string</span><br><span class="line">    ,amount string</span><br><span class="line">)<span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br></pre></td></tr></table></figure><h5 id="计算逻辑"><a href="#计算逻辑" class="headerlink" title="计算逻辑"></a>计算逻辑</h5><ul><li>先按用户和日期分组求和，使每个用户每天只有一条数据</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> id,datestr,<span class="built_in">sum</span>(amount) <span class="keyword">as</span> sum_amount <span class="keyword">from</span> deal_tb <span class="keyword">group</span> <span class="keyword">by</span> id,datestr;</span><br></pre></td></tr></table></figure><ul><li>根据用户ID分组按日期排序，将日期和分组序号相减得到连续登陆的开始日期，如果开始日期相同说明连续登陆</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> tt1.id,tt1.datestr,tt1.sum_amount,date_sub(tt1.datestr,tt1.rn) <span class="keyword">as</span> grp <span class="keyword">from</span> (<span class="keyword">select</span> t1.id <span class="keyword">as</span> id,t1.datestr <span class="keyword">as</span> datestr,t1.sum_amount <span class="keyword">as</span> sum_amount,<span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> t1.id <span class="keyword">order</span> <span class="keyword">by</span> t1.datestr) <span class="keyword">as</span> rn <span class="keyword">from</span> (<span class="keyword">select</span> id,datestr,<span class="built_in">sum</span>(amount) <span class="keyword">as</span> sum_amount <span class="keyword">from</span> deal_tb <span class="keyword">group</span> <span class="keyword">by</span> id,datestr) t1) tt1;</span><br></pre></td></tr></table></figure><ul><li><em>datediff(string end_date,string start_date);</em> 等于0说明连续登录</li><li>统计用户连续交易的总额、连续登陆天数、连续登陆开始和结束时间、间隔天数</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ttt1.id,ttt1.grp,round(<span class="built_in">sum</span>(ttt1.sum_amount),<span class="number">2</span>) <span class="keyword">as</span> user_sum_amount,<span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">as</span> user_days,<span class="built_in">min</span>(ttt1.datestr) <span class="keyword">as</span> user_start_date,<span class="built_in">max</span>(ttt1.datestr) <span class="keyword">as</span> user_end_date,datediff(ttt1.grp,<span class="built_in">lag</span>(ttt1.grp,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> ttt1.id <span class="keyword">order</span> <span class="keyword">by</span> ttt1.grp)) <span class="keyword">as</span> interval_days <span class="keyword">from</span> (<span class="keyword">select</span> tt1.id <span class="keyword">as</span> id,tt1.datestr <span class="keyword">as</span> datestr,tt1.sum_amount <span class="keyword">as</span> sum_amount,date_sub(tt1.datestr,tt1.rn) <span class="keyword">as</span> grp <span class="keyword">from</span> (<span class="keyword">select</span> t1.id <span class="keyword">as</span> id,t1.datestr <span class="keyword">as</span> datestr,t1.sum_amount <span class="keyword">as</span> sum_amount,<span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> t1.id <span class="keyword">order</span> <span class="keyword">by</span> t1.datestr) <span class="keyword">as</span> rn <span class="keyword">from</span> (<span class="keyword">select</span> id,datestr,<span class="built_in">sum</span>(amount) <span class="keyword">as</span> sum_amount <span class="keyword">from</span> deal_tb <span class="keyword">group</span> <span class="keyword">by</span> id,datestr) t1) tt1) ttt1 <span class="keyword">group</span> <span class="keyword">by</span> ttt1.id,ttt1.grp;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> ttt1.id, ttt1.grp</span><br><span class="line">, round(<span class="built_in">sum</span>(ttt1.sum_amount), <span class="number">2</span>) <span class="keyword">AS</span> user_sum_amount</span><br><span class="line">, <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">AS</span> user_days, <span class="built_in">min</span>(ttt1.datestr) <span class="keyword">AS</span> user_start_date</span><br><span class="line">, <span class="built_in">max</span>(ttt1.datestr) <span class="keyword">AS</span> user_end_date</span><br><span class="line">, datediff(ttt1.grp, <span class="built_in">lag</span>(ttt1.grp, <span class="number">1</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> ttt1.id <span class="keyword">ORDER</span> <span class="keyword">BY</span> ttt1.grp)) <span class="keyword">AS</span> interval_days</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> tt1.id <span class="keyword">AS</span> id, tt1.datestr <span class="keyword">AS</span> datestr, tt1.sum_amount <span class="keyword">AS</span> sum_amount</span><br><span class="line">, date_sub(tt1.datestr, tt1.rn) <span class="keyword">AS</span> grp</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> t1.id <span class="keyword">AS</span> id, t1.datestr <span class="keyword">AS</span> datestr, t1.sum_amount <span class="keyword">AS</span> sum_amount, <span class="built_in">row_number</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> t1.id <span class="keyword">ORDER</span> <span class="keyword">BY</span> t1.datestr) <span class="keyword">AS</span> rn</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> id, datestr, <span class="built_in">sum</span>(amount) <span class="keyword">AS</span> sum_amount</span><br><span class="line"><span class="keyword">FROM</span> deal_tb</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> id, datestr</span><br><span class="line">) t1</span><br><span class="line">) tt1</span><br><span class="line">) ttt1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> ttt1.id, ttt1.grp;</span><br></pre></td></tr></table></figure><ul><li>结果</li></ul><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">12019-02-0713600.2332019-02-082019-02-10 NULL</span><br><span class="line">12019-02-082991.65052019-02-122019-02-161</span><br><span class="line">12019-02-091510.822019-02-182019-02-191</span><br><span class="line">12019-02-10537.7112019-02-212019-02-211</span><br><span class="line">22019-02-0713600.2332019-02-082019-02-10 NULL</span><br><span class="line">22019-02-083026.64942019-02-122019-02-151</span><br><span class="line">22019-02-101510.822019-02-182019-02-192</span><br><span class="line">22019-02-11537.7112019-02-212019-02-211</span><br><span class="line">32019-02-0713600.2332019-02-082019-02-10 NULL</span><br><span class="line">32019-02-082730.0452019-02-122019-02-161</span><br><span class="line">32019-02-091510.822019-02-182019-02-191</span><br><span class="line">32019-02-10537.7112019-02-212019-02-211</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">对学习Hive的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hive" scheme="http://example.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive的基本操作-2</title>
    <link href="http://example.com/2022/06/02/Hive%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-2/"/>
    <id>http://example.com/2022/06/02/Hive%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-2/</id>
    <published>2022-06-01T16:00:00.000Z</published>
    <updated>2022-06-08T07:02:58.058Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、Hive分区"><a href="#1、Hive分区" class="headerlink" title="1、Hive分区"></a>1、Hive分区</h2><blockquote><p>在大数据中，最常见的一种思想就是<strong>分治</strong>，我们可以<strong>把大的文件切割划分成一个个的小的文件</strong>，这样每次操作一个个小的文件就会很容易了，同样的道理，在hive当中也是支持这种思想的，就是我们可以把大的数据，按照每天或者每小时切分成一个个小的文件，这样去操作小的文件就会容易很多了。</p><p>假如现在我们公司一天产生3亿的数据量，那么为了方便管理和查询，就做以下的事情。</p><p>​        1）建立分区（可按照日期，部门等等具体业务分区）</p><p>​        2）分门别类的管理</p></blockquote><p><img src="https://s2.loli.net/2022/06/03/QdRsgvWS7yYnN13.png" alt="hive分区的种类.png"></p><h3 id="1-2-静态分区（SP）"><a href="#1-2-静态分区（SP）" class="headerlink" title="1.2    静态分区（SP）"></a>1.2    静态分区（SP）</h3><blockquote><p>静态分区（SP）static partition–partition by (字段 类型)</p><p>​        <strong>借助于物理的文件夹分区，实现快速检索的目的。</strong></p><p>​        <strong>一般对于查询比较频繁的列设置为分区列。</strong></p><p>​        <strong>分区查询的时候直接把对应分区中所有数据放到对应的文件夹中</strong>。</p></blockquote><blockquote><p>创建单分区表语法：</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> t_student (</span><br><span class="line">sno <span class="type">int</span>,</span><br><span class="line">sname string</span><br><span class="line">) partitioned <span class="keyword">by</span>(grade <span class="type">int</span>)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line"><span class="comment">--  分区的字段不要和表的字段相同。相同会报错error10035</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">1</span>,xiaohu01,<span class="number">1</span></span><br><span class="line"><span class="number">2</span>,xiaohu02,<span class="number">1</span></span><br><span class="line"><span class="number">3</span>,xiaohu03,<span class="number">1</span></span><br><span class="line"><span class="number">4</span>,xiaohu04,<span class="number">1</span></span><br><span class="line"><span class="number">5</span>,xiaohu05,<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">6</span>,xiaohu06,<span class="number">2</span></span><br><span class="line"><span class="number">7</span>,xiaohu07,<span class="number">2</span></span><br><span class="line"><span class="number">8</span>,xiaohu08,<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">9</span>,xiaohu09,<span class="number">3</span></span><br><span class="line"><span class="number">10</span>,xiaohu10,<span class="number">3</span></span><br><span class="line"><span class="number">11</span>,xiaohu11,<span class="number">3</span></span><br><span class="line"><span class="number">12</span>,xiaohu12,<span class="number">3</span></span><br><span class="line"><span class="number">13</span>,xiaohu13,<span class="number">3</span></span><br><span class="line"><span class="number">14</span>,xiaohu14,<span class="number">3</span></span><br><span class="line"><span class="number">15</span>,xiaohu15,<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">16</span>,xiaohu16,<span class="number">4</span></span><br><span class="line"><span class="number">17</span>,xiaohu17,<span class="number">4</span></span><br><span class="line"><span class="number">18</span>,xiaohu18,<span class="number">4</span></span><br><span class="line"><span class="number">19</span>,xiaohu19,<span class="number">4</span></span><br><span class="line"><span class="number">20</span>,xiaohu20,<span class="number">4</span></span><br><span class="line"><span class="number">21</span>,xiaohu21,<span class="number">4</span></span><br><span class="line"><span class="comment">-- 载入数据</span></span><br><span class="line"><span class="comment">-- 将相应年级一次导入</span></span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/usr/local/soft/shujia/student1.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> t_student <span class="keyword">partition</span>(grade<span class="operator">=</span><span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 演示多拷贝一行上传，分区的列的值是分区的值，不是原来的值</span></span><br></pre></td></tr></table></figure><blockquote><p>静态多分区表语法：</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> t_teacher (</span><br><span class="line">tno <span class="type">int</span>,</span><br><span class="line">tname string</span><br><span class="line">) partitioned <span class="keyword">by</span>(grade <span class="type">int</span>,clazz <span class="type">int</span>)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--注意：前后两个分区的关系为父子关系，也就是grade文件夹下面有多个clazz子文件夹。</span></span><br><span class="line"><span class="number">1</span>,xiaoge01,<span class="number">1</span>,<span class="number">1</span></span><br><span class="line"><span class="number">2</span>,xiaoge02,<span class="number">1</span>,<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">3</span>,xiaoge03,<span class="number">1</span>,<span class="number">2</span></span><br><span class="line"><span class="number">4</span>,xiaoge04,<span class="number">1</span>,<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">5</span>,xiaoge05,<span class="number">1</span>,<span class="number">3</span></span><br><span class="line"><span class="number">6</span>,xiaoge06,<span class="number">1</span>,<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">7</span>,xiaoge07,<span class="number">2</span>,<span class="number">1</span></span><br><span class="line"><span class="number">8</span>,xiaoge08,<span class="number">2</span>,<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">9</span>,xiaoge09,<span class="number">2</span>,<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--载入数据</span></span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/usr/local/soft/shujia/teacher11.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> t_teacher <span class="keyword">partition</span>(grade<span class="operator">=</span><span class="number">1</span>,clazz<span class="operator">=</span><span class="number">1</span>);</span><br></pre></td></tr></table></figure><blockquote><p>分区表查询</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_student <span class="keyword">where</span> grade <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 全表扫描，不推荐，效率低</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> students_pt1;</span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 使用<span class="keyword">where</span>条件进行分区裁剪，避免了全表扫描，效率高</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> students_pt1 <span class="keyword">where</span> grade <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 也可以在<span class="keyword">where</span>条件中使用非等值判断</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> students_pt1 <span class="keyword">where</span> grade<span class="operator">&lt;</span><span class="number">3</span> <span class="number">1</span> <span class="keyword">and</span> grade<span class="operator">&gt;=</span><span class="number">1</span>;</span><br></pre></td></tr></table></figure><blockquote><p>查看分区</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> partitions t_student;</span><br></pre></td></tr></table></figure><blockquote><p>添加分区</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t_student <span class="keyword">add</span> <span class="keyword">partition</span> (grade<span class="operator">=</span><span class="number">5</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t_student <span class="keyword">add</span> <span class="keyword">partition</span> (grade<span class="operator">=</span><span class="number">5</span>) location <span class="string">&#x27;指定数据文件的路径&#x27;</span>;</span><br></pre></td></tr></table></figure><blockquote><p>删除分区</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t_student <span class="keyword">drop</span> <span class="keyword">partition</span> (grade<span class="operator">=</span><span class="number">5</span>);</span><br></pre></td></tr></table></figure><h3 id="1-3-动态分区（DP）"><a href="#1-3-动态分区（DP）" class="headerlink" title="1.3    动态分区（DP）"></a>1.3    动态分区（DP）</h3><ul><li>动态分区（DP）dynamic partition</li><li>静态分区与动态分区的<strong>主要区别在于静态分区是手动指定，而动态分区是通过数据来进行判断。</strong></li><li>详细来说，静态分区的列是在编译时期通过用户传递来决定的；<strong>动态分区只有在SQL执行时才能决定</strong>。</li></ul><blockquote><p>开启动态分区首先要在hive会话中设置如下的参数</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 表示开启动态分区</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">set</span> hive.exec.dynamic.partition<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"># 表示动态分区模式：strict（需要配合静态分区一起使用）、nostrict</span><br><span class="line"># strict： <span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> students_pt <span class="keyword">partition</span>(dt<span class="operator">=</span><span class="string">&#x27;anhui&#x27;</span>,pt) <span class="keyword">select</span> ......,pt <span class="keyword">from</span> students;</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">set</span> hive.exec.dynamic.partition.mode<span class="operator">=</span>nonstrict;</span><br><span class="line"></span><br><span class="line"># 表示支持的最大的分区数量为<span class="number">1000</span>，可以根据业务自己调整</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">set</span> hive.exec.max.dynamic.partitions.pernode<span class="operator">=</span><span class="number">1000</span>;</span><br></pre></td></tr></table></figure><blockquote><p>其余的参数详细配置如下</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">设置为<span class="literal">true</span>表示开启动态分区的功能（默认为<span class="literal">false</span>）</span><br><span class="line"><span class="comment">--hive.exec.dynamic.partition=true;</span></span><br><span class="line"></span><br><span class="line">设置为nonstrict，表示允许所有分区都是动态的（默认为strict）</span><br><span class="line"><span class="comment">-- hive.exec.dynamic.partition.mode=nonstrict; </span></span><br><span class="line"></span><br><span class="line">每个mapper或reducer可以创建的最大动态分区个数(默认为<span class="number">100</span>) </span><br><span class="line">比如：源数据中包含了一年的数据，即<span class="keyword">day</span>字段有<span class="number">365</span>个值，那么该参数就需要设置成大于<span class="number">365</span>，如果使用默认值<span class="number">100</span>，则会报错</span><br><span class="line"><span class="comment">--hive.exec.max.dynamic.partition.pernode=100; </span></span><br><span class="line"></span><br><span class="line">一个动态分区创建可以创建的最大动态分区个数（默认值<span class="number">1000</span>）</span><br><span class="line"><span class="comment">--hive.exec.max.dynamic.partitions=1000;</span></span><br><span class="line"></span><br><span class="line">全局可以创建的最大文件个数（默认值<span class="number">100000</span>）</span><br><span class="line"><span class="comment">--hive.exec.max.created.files=100000; </span></span><br><span class="line"></span><br><span class="line">当有空分区产生时，是否抛出异常（默认<span class="literal">false</span>） </span><br><span class="line"><span class="comment">-- hive.error.on.empty.partition=false;  </span></span><br></pre></td></tr></table></figure><ul><li>案例1： 动态插入学生年级班级信息</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--创建分区表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> t_student_d (</span><br><span class="line">sno <span class="type">int</span>,</span><br><span class="line">sname string</span><br><span class="line">) partitioned <span class="keyword">by</span> (grade <span class="type">int</span>,clazz <span class="type">int</span>)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--创建外部表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> t_student_e (</span><br><span class="line">sno <span class="type">int</span>,</span><br><span class="line">sname string,</span><br><span class="line">grade <span class="type">int</span>,</span><br><span class="line">clazz <span class="type">int</span></span><br><span class="line">) </span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">location &quot;/shujia/student&quot;;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">数据：</span><br><span class="line"></span><br><span class="line">1,xiaohu01,1,1</span><br><span class="line">2,xiaohu02,1,1</span><br><span class="line">3,xiaohu03,1,1</span><br><span class="line">4,xiaohu04,1,2</span><br><span class="line">5,xiaohu05,1,2</span><br><span class="line">6,xiaohu06,2,3</span><br><span class="line">7,xiaohu07,2,3</span><br><span class="line">8,xiaohu08,2,3</span><br><span class="line">9,xiaohu09,3,3</span><br><span class="line">10,xiaohu10,3,3</span><br><span class="line">11,xiaohu11,3,3</span><br><span class="line">12,xiaohu12,3,4</span><br><span class="line">13,xiaohu13,3,4</span><br><span class="line">14,xiaohu14,3,4</span><br><span class="line">15,xiaohu15,3,4</span><br><span class="line">16,xiaohu16,4,4</span><br><span class="line">17,xiaohu17,4,4</span><br><span class="line">18,xiaohu18,4,5</span><br><span class="line">19,xiaohu19,4,5</span><br><span class="line">20,xiaohu20,4,5</span><br><span class="line">21,xiaohu21,4,5</span><br></pre></td></tr></table></figure><blockquote><p>如果静态分区的话，我们插入数据必须指定分区的值。</p><p>如果想要插入多个班级的数据，我要写很多SQL并且执行24次很麻烦。</p><p>而且静态分区有可能会产生数据错误问题</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 会报错 </span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t_student_d <span class="keyword">partition</span> (grade<span class="operator">=</span><span class="number">1</span>) <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_student_e <span class="keyword">where</span> grade<span class="operator">=</span><span class="number">1</span>;</span><br></pre></td></tr></table></figure><blockquote><p>如果使用动态分区，动态分区会根据select的结果自动判断数据应该load到哪儿分区去。</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t_student_d <span class="keyword">partition</span> (grade,clazz) <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_student_e;</span><br></pre></td></tr></table></figure><blockquote><p>优点：不用手动指定了，自动会对数据进行分区</p><p>缺点：可能会出现数据倾斜</p></blockquote><h2 id="2、Hive分桶"><a href="#2、Hive分桶" class="headerlink" title="2、Hive分桶"></a>2、Hive分桶</h2><h3 id="2-1-业务场景"><a href="#2-1-业务场景" class="headerlink" title="2.1    业务场景"></a>2.1    业务场景</h3><blockquote><p>数据分桶的适用场景：<br>        分区提供了一个隔离数据和优化查询的便利方式，不过并非所有的数据都可形成合理的分区，尤其是需要确定合适大小的分区划分方式<br>        不合理的数据分区划分方式可能导致有的分区数据过多，而某些分区没有什么数据的尴尬情况<br>        分桶是将数据集分解为更容易管理的若干部分的另一种技术。<br>        分桶就是将数据按照字段进行划分，可以将数据按照字段划分到多个文件当中去。</p></blockquote><h3 id="2-2-数据分桶原理"><a href="#2-2-数据分桶原理" class="headerlink" title="2.2    数据分桶原理"></a>2.2    数据分桶原理</h3><ul><li>Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。<ul><li>bucket num &#x3D; hash_function(bucketing_column) mod num_buckets</li><li>列的值做哈希取余 决定数据应该存储到哪个桶</li></ul></li></ul><h3 id="2-3-数据分桶优势"><a href="#2-3-数据分桶优势" class="headerlink" title="2.3    数据分桶优势"></a>2.3    数据分桶优势</h3><blockquote><p><strong>方便抽样</strong></p><p>​        使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便</p><p><strong>提高join查询效率</strong></p><p>​        获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。</p></blockquote><h3 id="2-4-分桶实战"><a href="#2-4-分桶实战" class="headerlink" title="2.4    分桶实战"></a>2.4    分桶实战</h3><blockquote><p>​    首先，分区和分桶是两个不同的概念，很多资料上说需要先分区在分桶，其实不然，分区是对数据进行划分，而分桶是对文件进行划分。</p><p>​    当我们的分区之后，最后的文件还是很大怎么办，就引入了分桶的概念。</p><p>将这个比较大的文件再分成若干个小文件进行存储，我们再去查询的时候，在这个小范围的文件中查询就会快很多。</p><p>​        对于hive中的每一张表、分区都可以进一步的进行分桶。</p><p>​        当然，分桶不是说将文件随机进行切分存储，而是有规律的进行存储。在看完下面的例子后进行解释，现在干巴巴的解释也不太好理解。它是由列的哈希值除以桶的个数来决定每条数据划分在哪个桶中。</p><p>创建顺序和分区一样，创建的方式不一样。</p></blockquote><blockquote><p><strong>首先我们需要开启分桶的支持</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">（依然十分重要，不然无法进行分桶操作！！！！）</span><br><span class="line"><span class="keyword">set</span> hive.enforce.bucketing<span class="operator">=</span><span class="literal">true</span>; </span><br></pre></td></tr></table></figure><blockquote><p><strong>数据准备（id,name,age）</strong></p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1,tom,11</span><br><span class="line">2,cat,22</span><br><span class="line">3,dog,33</span><br><span class="line">4,hive,44</span><br><span class="line">5,hbase,55</span><br><span class="line">6,mr,66</span><br><span class="line">7,alice,77</span><br><span class="line">8,scala,88</span><br></pre></td></tr></table></figure><blockquote><p><strong>创建一个普通的表</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> psn31</span><br><span class="line">(</span><br><span class="line">id <span class="type">int</span>,</span><br><span class="line">name string,</span><br><span class="line">age <span class="type">int</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br></pre></td></tr></table></figure><blockquote><p><strong>将数据load到这张表中</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;文件在Linux上的绝对路径&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> psn31;</span><br></pre></td></tr></table></figure><blockquote><p><strong>创建分桶表</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> psn_bucket</span><br><span class="line">(</span><br><span class="line">id <span class="type">int</span>,</span><br><span class="line">name string,</span><br><span class="line">age <span class="type">int</span></span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span>(age) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br></pre></td></tr></table></figure><blockquote><p><strong>将数据insert到表psn_bucket中</strong></p><p><strong>(注意：这里和分区表插入数据有所区别，分区表需要select 和指定分区，而分桶则不需要)</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> psn_bucket <span class="keyword">select</span> id,name,age <span class="keyword">from</span> psn31;</span><br></pre></td></tr></table></figure><blockquote><p><strong>在HDFS上查看数据</strong></p></blockquote><p><img src="https://s2.loli.net/2022/06/08/xOIdzYoEqhRLe4g.png" alt="image-20220601223434297.png"></p><blockquote><p><strong>查询数据</strong></p><p><strong>我们在linux中使用Hadoop的命令查看一下（与我们猜想的顺序一致）</strong></p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop fs -cat /user/hive/warehouse/bigdata17.db/psn_bucket/*</span><br></pre></td></tr></table></figure><blockquote><p>这里设置的桶的个数是4 数据按照 年龄%4 进行放桶(文件)<br>11%4 &#x3D;&#x3D; 3 —–&gt; 000003_0<br>22%4 &#x3D;&#x3D; 2 —–&gt; 000002_0<br>33%4 &#x3D;&#x3D; 1 —–&gt; 000001_0<br>44%4 &#x3D;&#x3D; 0 —–&gt; 000000_0<br>…以此类推</p></blockquote><p><img src="https://s2.loli.net/2022/06/08/smJGetELUTlaIuo.png" alt="分桶逻辑发逻辑.png"></p><blockquote><p>在Hive进行查询</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y)</span></span><br><span class="line"><span class="comment">-- 分桶语句中的分母表示的是数据将会被散列的桶的个数，分子表示将会选择的桶的个数。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- x表示从哪个bucket开始抽取。</span></span><br><span class="line"><span class="comment">-- 例如，table总bucket数为32，tablesample(bucket 2 out of 2)</span></span><br><span class="line"><span class="comment">-- 表示总共抽取（2/2=）1个bucket的数据，分别为第2个bucket和第（2+2=）4个bucket的数据</span></span><br><span class="line"><span class="comment">-- y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。</span></span><br><span class="line"><span class="comment">-- 例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> psn_bucket <span class="keyword">tablesample</span>(bucket <span class="number">2</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">2</span>);</span><br><span class="line">随机取值（设置因子，桶的个数<span class="operator">/</span>因子）</span><br><span class="line">这里就是取<span class="number">2</span>号桶和<span class="number">4</span>号桶，取<span class="number">2</span>个</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> psn_bucket <span class="keyword">tablesample</span>(bucket <span class="number">2</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span>);</span><br><span class="line">随机取值（设置因子，桶的个数<span class="operator">/</span>因子）</span><br><span class="line">这里就是取<span class="number">2</span>号桶，取一个</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> psn_bucket <span class="keyword">tablesample</span>(bucket <span class="number">2</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">8</span>);</span><br><span class="line">随机取值（设置倍数，倍数<span class="operator">/</span>桶的个数）</span><br><span class="line">这里就是取<span class="number">2</span>号桶 <span class="number">1</span><span class="operator">/</span><span class="number">2</span>个数据</span><br><span class="line">取出来是一条数据</span><br></pre></td></tr></table></figure><h2 id="3、Hive-JDBC"><a href="#3、Hive-JDBC" class="headerlink" title="3、Hive JDBC"></a>3、Hive JDBC</h2><h5 id="启动hiveserver2"><a href="#启动hiveserver2" class="headerlink" title="启动hiveserver2"></a>启动hiveserver2</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive --service hiveserver2 &amp;</span><br><span class="line">或者</span><br><span class="line">hiveserver2 &amp;</span><br></pre></td></tr></table></figure><h5 id="新建maven项目并添加两个依赖"><a href="#新建maven项目并添加两个依赖" class="headerlink" title="新建maven项目并添加两个依赖"></a>新建maven项目并添加两个依赖</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.7.6&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-jdbc --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.2.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h5 id="编写JDBC代码"><a href="#编写JDBC代码" class="headerlink" title="编写JDBC代码"></a>编写JDBC代码</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HiveJDBC</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ClassNotFoundException, SQLException &#123;</span><br><span class="line">        Class.forName(<span class="string">&quot;org.apache.hive.jdbc.HiveDriver&quot;</span>);</span><br><span class="line">        <span class="type">Connection</span> <span class="variable">conn</span> <span class="operator">=</span> DriverManager.getConnection(<span class="string">&quot;jdbc:hive2://master:10000/bigdata17&quot;</span>);</span><br><span class="line">        <span class="type">Statement</span> <span class="variable">stat</span> <span class="operator">=</span> conn.createStatement();</span><br><span class="line">        <span class="type">ResultSet</span> <span class="variable">rs</span> <span class="operator">=</span> stat.executeQuery(<span class="string">&quot;select * from students limit 10&quot;</span>);</span><br><span class="line">        <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">id</span> <span class="operator">=</span> rs.getInt(<span class="number">1</span>);</span><br><span class="line">            <span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span> rs.getString(<span class="number">2</span>);</span><br><span class="line">            <span class="type">int</span> <span class="variable">age</span> <span class="operator">=</span> rs.getInt(<span class="number">3</span>);</span><br><span class="line">            <span class="type">String</span> <span class="variable">gender</span> <span class="operator">=</span> rs.getString(<span class="number">4</span>);</span><br><span class="line">            <span class="type">String</span> <span class="variable">clazz</span> <span class="operator">=</span> rs.getString(<span class="number">5</span>);</span><br><span class="line">            System.out.println(id + <span class="string">&quot;,&quot;</span> + name + <span class="string">&quot;,&quot;</span> + age + <span class="string">&quot;,&quot;</span> + gender + <span class="string">&quot;,&quot;</span> + clazz);</span><br><span class="line">        &#125;</span><br><span class="line">        rs.close();</span><br><span class="line">        stat.close();</span><br><span class="line">        conn.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、Hive查询语法-DQL"><a href="#4、Hive查询语法-DQL" class="headerlink" title="4、Hive查询语法(DQL)"></a>4、Hive查询语法(DQL)</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> <span class="operator">|</span> <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line"><span class="keyword">FROM</span> table_reference</span><br><span class="line">[<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">[<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">[<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">[CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line"><span class="operator">|</span> [DISTRIBUTE <span class="keyword">BY</span> col_list] [SORT <span class="keyword">BY</span> col_list]</span><br><span class="line">]</span><br><span class="line">[LIMIT [<span class="keyword">offset</span>,] <span class="keyword">rows</span>]</span><br></pre></td></tr></table></figure><h3 id="4-1-全局排序"><a href="#4-1-全局排序" class="headerlink" title="4.1    全局排序"></a>4.1    全局排序</h3><ul><li><strong>order by 会对输入做全局排序，因此只有一个reducer</strong>，会导致当输入规模较大时，需要较长的计算时间</li><li>使用 order by子句排序 :ASC（ascend）升序（默认）| DESC（descend）降序</li><li><strong>order by放在select语句的结尾</strong></li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> 表名 <span class="keyword">order</span> <span class="keyword">by</span> 字段名<span class="number">1</span>[，别名<span class="number">2.</span>..];</span><br></pre></td></tr></table></figure><h3 id="4-2-局部排序"><a href="#4-2-局部排序" class="headerlink" title="4.2    局部排序"></a>4.2    局部排序</h3><ul><li><strong>sort by 不是全局排序,其在数据进入reducer前完成排序</strong>。</li><li>如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1,则sort by 只保证每个reducer的输出有序，<strong>不保证全局有序</strong>。asc,desc</li><li>设置reduce个数</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduce<span class="operator">=</span><span class="number">3</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.reduce.tasks<span class="operator">=</span><span class="number">3</span>;</span><br></pre></td></tr></table></figure><ul><li>查看reduce个数</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduce;</span><br></pre></td></tr></table></figure><ul><li>排序</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> 表名 sort <span class="keyword">by</span> 字段名[,字段名...];</span><br></pre></td></tr></table></figure><h3 id="4-3-分区排序"><a href="#4-3-分区排序" class="headerlink" title="4.3    分区排序"></a>4.3    分区排序</h3><blockquote><p><strong>distribute by（字段）根据指定的字段将数据</strong>分到不同的reducer，且分发算法是hash散列。</p><p><strong>类似MR中partition,进行分区，结合sort by使用。</strong>（注意：distribute by 要在sort by之前）</p><p>对于distrbute by 进行测试，一定要多分配reduce进行处理，否则无法看到distribute by的效果。</p><p>设置reduce个数</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduce<span class="operator">=</span><span class="number">7</span>;</span><br></pre></td></tr></table></figure><ul><li>排序</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> 表名 distribute <span class="keyword">by</span> 字段名[,字段名...];</span><br></pre></td></tr></table></figure><h3 id="4-3-分区并排序"><a href="#4-3-分区并排序" class="headerlink" title="4.3    分区并排序"></a>4.3    分区并排序</h3><ul><li>cluster by（字段）除了具有Distribute by的功能外，还会对该字段进行排序</li><li>cluster by &#x3D; distribute by + sort by 只能默认升序，不能使用倒序</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> 表名 sort cluster <span class="keyword">by</span> 字段名[,字段名...];</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> 表名 distribute <span class="keyword">by</span> 字段名[,字段名...] sort <span class="keyword">by</span> 字段名[,字段名...];</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/08/ZVozuJNtE9wXm2a.png" alt="hive几种分区的区别.png"></p><h2 id="5、Hive内置函数"><a href="#5、Hive内置函数" class="headerlink" title="5、Hive内置函数"></a>5、Hive内置函数</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">https:<span class="operator">/</span><span class="operator">/</span>cwiki.apache.org<span class="operator">/</span>confluence<span class="operator">/</span>display<span class="operator">/</span>Hive<span class="operator">/</span>LanguageManual<span class="operator">+</span>UDF</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 1.查看系统自带函数</span></span><br><span class="line"><span class="keyword">show</span> functions;</span><br><span class="line"><span class="comment">-- 2.显示自带的函数的用法</span></span><br><span class="line"><span class="keyword">desc</span> <span class="keyword">function</span> upper;</span><br><span class="line"><span class="comment">-- 3.详细显示自带的函数的用法</span></span><br><span class="line"><span class="keyword">desc</span> <span class="keyword">function</span> extended upper;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/08/gWa3pOnDHT89EtI.png" alt="内置函数的分类.png"></p><h3 id="5-1-内置函数分类"><a href="#5-1-内置函数分类" class="headerlink" title="5.1    内置函数分类"></a>5.1    内置函数分类</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">关系操作符：包括 <span class="operator">=</span> 、 <span class="operator">&lt;&gt;</span> 、 <span class="operator">&lt;=</span> 、<span class="operator">&gt;=</span>等</span><br><span class="line"></span><br><span class="line">算数操作符：包括 <span class="operator">+</span> 、 <span class="operator">-</span> 、 <span class="operator">*</span>、／等</span><br><span class="line"></span><br><span class="line">逻辑操作符：包括<span class="keyword">AND</span> 、 <span class="operator">&amp;&amp;</span> 、 <span class="keyword">OR</span> 、 <span class="operator">||</span> 等</span><br><span class="line"></span><br><span class="line">复杂类型构造函数：包括map、struct、create_union等</span><br><span class="line"></span><br><span class="line">复杂类型操作符：包括A[n]、Map[key]、S.x</span><br><span class="line"></span><br><span class="line">数学操作符：包括<span class="built_in">ln</span>(<span class="keyword">double</span> a)、<span class="built_in">sqrt</span>(<span class="keyword">double</span> a)等</span><br><span class="line"></span><br><span class="line">集合操作符：包括size(<span class="keyword">Array</span>)、sort_array(<span class="keyword">Array</span>)等</span><br><span class="line"></span><br><span class="line">类型转换函数： <span class="type">binary</span>(string<span class="operator">|</span><span class="type">binary</span>)、<span class="built_in">cast</span>(expr <span class="keyword">as</span> )</span><br><span class="line"></span><br><span class="line">日期函数：包括from_unixtime(<span class="type">bigint</span> unixtime[, string format])、unix_timestamp()等</span><br><span class="line"></span><br><span class="line">条件函数：包括if(<span class="type">boolean</span> testCondition, T valueTrue, T valueFalseOrNull)等</span><br><span class="line"></span><br><span class="line">字符串函数：包括acat(string<span class="operator">|</span><span class="type">binary</span> A, string<span class="operator">|</span><span class="type">binary</span> B…)等</span><br><span class="line"></span><br><span class="line">其他：xpath、get_json_objectscii(string str)、con</span><br></pre></td></tr></table></figure><h3 id="5-2-UDTF-hive中特殊的一个功能（进一出多）"><a href="#5-2-UDTF-hive中特殊的一个功能（进一出多）" class="headerlink" title="5.2    UDTF hive中特殊的一个功能（进一出多）"></a>5.2    UDTF hive中特殊的一个功能（进一出多）</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- UDF 进一出一</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- UDAF 进多出一</span></span><br><span class="line"><span class="comment">-- collect_set()和collect_list()都是对多列转成一行，区别就是list里面可重复而set里面是去重的</span></span><br><span class="line"><span class="comment">-- concat_ws(&#x27;:&#x27;,collect_set(type))   &#x27;:&#x27; 表示你合并后用什么分隔，collect_set(stage)表示要合并表中的那一列数据</span></span><br><span class="line"><span class="keyword">select</span> 字段名,concat_ws(<span class="string">&#x27;:&#x27;</span>,collect_set(列名)) <span class="keyword">as</span> 别名 <span class="keyword">from</span> 表名 <span class="keyword">group</span> <span class="keyword">by</span> id;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- UDTF 进一出多</span></span><br><span class="line"><span class="comment">-- explode  可以将一组数组的数据变成一列表</span></span><br><span class="line"><span class="keyword">select</span>  explode(split(列名,&quot;数据的分隔符&quot;)) <span class="keyword">from</span> 表名;</span><br><span class="line"><span class="comment">-- lateral view 表生成函数，可以将explode的数据生成一个列表</span></span><br><span class="line"><span class="keyword">select</span> id,name,列名 <span class="keyword">from</span> 表<span class="number">1</span>,<span class="keyword">lateral</span> <span class="keyword">view</span> explode(split(表<span class="number">1.</span>列名,&quot;数据的分隔符&quot;))新列名 <span class="keyword">as</span> 别列名;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建数据库表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_movie1(</span><br><span class="line">id <span class="type">int</span>,</span><br><span class="line">name string,</span><br><span class="line">types string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 电影数据  movie1.txt</span></span><br><span class="line"><span class="comment">-- 加载数据到数据库 load data inpath &#x27;/shujia/movie1.txt&#x27; into table t_movie1;</span></span><br><span class="line"><span class="number">1</span>,这个杀手不太冷,剧情<span class="operator">-</span>动作<span class="operator">-</span>犯罪</span><br><span class="line"><span class="number">2</span>,七武士,动作<span class="operator">-</span>冒险<span class="operator">-</span>剧情</span><br><span class="line"><span class="number">3</span>,勇敢的心,动作<span class="operator">-</span>传记<span class="operator">-</span>剧情<span class="operator">-</span>历史<span class="operator">-</span>战争</span><br><span class="line"><span class="number">4</span>,东邪西毒,剧情<span class="operator">-</span>动作<span class="operator">-</span>爱情<span class="operator">-</span>武侠<span class="operator">-</span>古装</span><br><span class="line"><span class="number">5</span>,霍比特人,动作<span class="operator">-</span>奇幻<span class="operator">-</span>冒险</span><br><span class="line"></span><br><span class="line"><span class="comment">-- explode  可以将一组数组的数据变成一列表</span></span><br><span class="line"><span class="keyword">select</span>  explode(split(types,&quot;-&quot;)) <span class="keyword">from</span> t_movie1;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- lateral view 表生成函数，可以将explode的数据生成一个列表</span></span><br><span class="line"><span class="keyword">select</span> id,name,type <span class="keyword">from</span> t_movie1 <span class="keyword">lateral</span> <span class="keyword">view</span> explode(split(types,&quot;-&quot;)) typetable <span class="keyword">as</span> type;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建数据库表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_movie2(</span><br><span class="line">id <span class="type">int</span>,</span><br><span class="line">name string,</span><br><span class="line">type string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 电影数据 movie2.txt</span></span><br><span class="line"><span class="comment">-- 加载数据到数据库 load data inpath &#x27;/shujia/movie2.txt&#x27; into table t_movie2;</span></span><br><span class="line"><span class="number">1</span>,这个杀手不太冷,剧情</span><br><span class="line"><span class="number">1</span>,这个杀手不太冷,动作</span><br><span class="line"><span class="number">1</span>,这个杀手不太冷,犯罪</span><br><span class="line"><span class="number">2</span>,七武士,动作</span><br><span class="line"><span class="number">2</span>,七武士,冒险</span><br><span class="line"><span class="number">2</span>,七武士,剧情</span><br><span class="line"><span class="number">3</span>,勇敢的心,动作</span><br><span class="line"><span class="number">3</span>,勇敢的心,传记</span><br><span class="line"><span class="number">3</span>,勇敢的心,剧情</span><br><span class="line"><span class="number">3</span>,勇敢的心,历史</span><br><span class="line"><span class="number">3</span>,勇敢的心,战争</span><br><span class="line"><span class="number">4</span>,东邪西毒,剧情</span><br><span class="line"><span class="number">4</span>,东邪西毒,动作</span><br><span class="line"><span class="number">4</span>,东邪西毒,爱情</span><br><span class="line"><span class="number">4</span>,东邪西毒,武侠</span><br><span class="line"><span class="number">4</span>,东邪西毒,古装</span><br><span class="line"><span class="number">5</span>,霍比特人,动作</span><br><span class="line"><span class="number">5</span>,霍比特人,奇幻</span><br><span class="line"><span class="number">5</span>,霍比特人,冒险</span><br><span class="line"></span><br><span class="line"><span class="comment">-- collect_set()和collect_list()都是对列转成行，区别就是list里面可重复而set里面是去重的</span></span><br><span class="line"><span class="comment">-- concat_ws(&#x27;:&#x27;,collect_set(type))   &#x27;:&#x27; 表示你合并后用什么分隔，collect_set(stage)表示要合并表中的那一列数据</span></span><br><span class="line"><span class="keyword">select</span> id,concat_ws(<span class="string">&#x27;:&#x27;</span>,collect_set(type)) <span class="keyword">as</span> types <span class="keyword">from</span> t_movie2 <span class="keyword">group</span> <span class="keyword">by</span> id;</span><br></pre></td></tr></table></figure><h3 id="5-3-WordCount案例"><a href="#5-3-WordCount案例" class="headerlink" title="5.3    WordCount案例"></a>5.3    WordCount案例</h3><blockquote><p>数据准备</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hello,world</span><br><span class="line">hello,bigdata</span><br><span class="line">like,life</span><br><span class="line">bigdata,good</span><br></pre></td></tr></table></figure><blockquote><p>建表</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> wc</span><br><span class="line">(</span><br><span class="line">line string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>导入数据</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/usr/local/soft/data/wc1.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> wc;</span><br></pre></td></tr></table></figure><blockquote><p><strong>步骤1：先对一行数据进行切分</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> split(line,<span class="string">&#x27;,&#x27;</span>) <span class="keyword">from</span> wc;</span><br></pre></td></tr></table></figure><blockquote><p><strong>步骤2：将行转列</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> explode(split(line,<span class="string">&#x27;,&#x27;</span>)) <span class="keyword">from</span> wc; </span><br></pre></td></tr></table></figure><blockquote><p><strong>步骤3：将相同的进行分组统计</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> w.word,<span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> (<span class="keyword">select</span> explode(split(line,<span class="string">&#x27;,&#x27;</span>)) <span class="keyword">as</span> word <span class="keyword">from</span> wc) w <span class="keyword">group</span> <span class="keyword">by</span> w.word;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">对学习Hive的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hive" scheme="http://example.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive概述及安装</title>
    <link href="http://example.com/2022/05/31/Hive%E6%A6%82%E8%BF%B0%E5%8F%8A%E5%AE%89%E8%A3%85/"/>
    <id>http://example.com/2022/05/31/Hive%E6%A6%82%E8%BF%B0%E5%8F%8A%E5%AE%89%E8%A3%85/</id>
    <published>2022-05-30T16:00:00.000Z</published>
    <updated>2022-06-01T13:50:38.275Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Hive基本概念"><a href="#一、Hive基本概念" class="headerlink" title="一、Hive基本概念"></a>一、Hive基本概念</h2><h3 id="1-1-Hive简介"><a href="#1-1-Hive简介" class="headerlink" title="1.1    Hive简介"></a>1.1    Hive简介</h3><p>Hive本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据存储，说白了hive可以理解为一个将SQL转换为MapReduce的任务的工具，甚至更近一步说hive就是一个MapReduce客户端。</p><p><img src="https://s2.loli.net/2022/06/01/EXbKAUlr4ao5e83.png" alt="image-20220531221835408"></p><h4 id="1-1-1-为什么使用Hive"><a href="#1-1-1-为什么使用Hive" class="headerlink" title="1.1.1    为什么使用Hive?"></a>1.1.1    为什么使用Hive?</h4><blockquote><p>如果直接使用hadoop的话，人员学习成本太高，项目要求周期太短，MapReduce实现复杂查询逻辑开发难度太大。如果使用hive的话，可以操作接口采用类SQL语法，提高开发能力，免去了写MapReduce，减少开发人员学习成本，功能扩展很方便（比如：开窗函数）。</p></blockquote><h4 id="1-1-2-Hive的特点："><a href="#1-1-2-Hive的特点：" class="headerlink" title="1.1.2    Hive的特点："></a>1.1.2    Hive的特点：</h4><blockquote><p>1、可扩展性</p><p>​    Hive可以自由的扩展集群的规模，一般情况下不需要重启服务</p><p>2、延申性</p><p>​    Hive支持自定义函数，用户可以根据自己的需求来实现自己的函数</p><p>3、容错</p><p>​    即使节点出现错误，SQL仍然可以完成执行</p></blockquote><h4 id="1-1-3-Hive的优缺点："><a href="#1-1-3-Hive的优缺点：" class="headerlink" title="1.1.3    Hive的优缺点："></a>1.1.3    Hive的优缺点：</h4><blockquote><p><strong>优点：</strong></p><p>​    1、操作接口采用类sql语法，提供快速开发的能力（简单、容易上手）</p><p>​    2、避免了去写MapReduce,减少开发人员的学习成本</p><p>​    3、Hive的延迟性比较高，因此Hive常用于数据分析，适用于对实时性要求不高的场合</p><p>​    4、Hive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。（不断地开关JVM虚拟机）</p><p>​    5、Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</p><p>​    6、集群可自由扩展并且具有良好的容错性，节点出现问题SQL仍可以完成执行</p><p><strong>缺点：</strong></p><p>​    1、Hive的HQL表达能力有限</p><p>​            （1）迭代式算法无法表达    （反复调用，mr之间独立，只有一个map一个reduce，反复开关）                </p><p>​            （2）数据挖掘方面不擅长</p><p>​    2、Hive 的效率比较低</p><p>​                （1）Hive 自动生成的 MapReduce 作业，通常情况下不够智能化</p><p>​                （2）Hive 调优比较困难，粒度较粗   （hql根据模板转成mapreduce，不能像自己编写mapreduce一样精细，无法控制在map处理数据还是在reduce处理数据）</p></blockquote><h4 id="1-1-4-Hive和传统数据库对比"><a href="#1-1-4-Hive和传统数据库对比" class="headerlink" title="1.1.4    Hive和传统数据库对比"></a>1.1.4    Hive和传统数据库对比</h4><p><img src="https://s2.loli.net/2022/06/01/JlRsONUB1oQMrxy.png" alt="image-20220531213145918"></p><h4 id="1-1-5-Hive应用场景"><a href="#1-1-5-Hive应用场景" class="headerlink" title="1.1.5    Hive应用场景"></a>1.1.5    Hive应用场景</h4><blockquote><p>日志分析：大部分互联网公司使用hive进行日志分析，如百度、淘宝等。</p><p>​    统计一个网站一个时间段内的<strong>pv,uv，SKU,SPU</strong></p><p>​    多维度数据分析</p><p>海量结构化数据离线分析</p><p><strong>构建数据仓库</strong></p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PV（Page View）访问量, 即页面浏览量或点击量，衡量网站用户访问的网页数量；在一定统计周期内用户每打开或刷新一个页面就记录1次，多次打开或刷新同一页面则浏览量累计。</span><br><span class="line"></span><br><span class="line">UV（Unique Visitor）独立访客，统计1天内访问某站点的用户数(以cookie为依据);访问网站的一台电脑客户端为一个访客。可以理解成访问某网站的电脑的数量。网站判断来访电脑的身份是通过来访电脑的cookies实现的。如果更换了IP后但不清除cookies，再访问相同网站，该网站的统计中UV数是不变的。如果用户不保存cookies访问、清除了cookies或者更换设备访问，计数会加1。00:00-24:00内相同的客户端多次访问只计为1个访客。</span><br></pre></td></tr></table></figure><h3 id="1-2-Hive架构"><a href="#1-2-Hive架构" class="headerlink" title="1.2    Hive架构"></a>1.2    Hive架构</h3><p><img src="https://s2.loli.net/2022/06/01/vRqfEbaU8TzhVQs.png" alt="image-20220531214038409"></p><h4 id="1-2-1-Client"><a href="#1-2-1-Client" class="headerlink" title="1.2.1    Client"></a>1.2.1    Client</h4><blockquote><p>Hive允许client连接的方式有三个CLI（hive shell）、JDBC&#x2F;ODBC(java访问hive)、WEBUI（浏览器访问 hive）。JDBC访问时中间件Thrift软件框架，跨语言服务开发。DDL DQL DML,整体仿写一套SQL语句。</p><p>​        1）client–需要下载安装包</p><p>​        2）JDBC&#x2F;ODBC 也可以连接到Hive<br>​                现在主流都在倡导第二种 HiveServer2&#x2F;beeline<br>​                做基于用户名和密码安全的一个校验</p><p>​        3）Web Gui<br>​                hive给我们提供了一套简单的web页面<br>​                我们可以通过这套web页面访问hive 做的太简陋了</p></blockquote><h4 id="1-2-2-Metastore"><a href="#1-2-2-Metastore" class="headerlink" title="1.2.2    Metastore"></a>1.2.2    Metastore</h4><blockquote><p><strong>元数据</strong>包括表名、表所属的数据库（默认是default）、表的拥有者、列&#x2F;分区字段、表的类型（是否是 外部表）、表的数据所在目录等。</p><p>​        一般需要借助于其他的数据载体（数据库）</p><p>​        主要用于存放数据库的建表语句等信息</p><p>​        推荐使用Mysql数据库存放数据</p><p>​        连接数据库需要提供：uri username password driver</p></blockquote><h4 id="1-2-3-Driver"><a href="#1-2-3-Driver" class="headerlink" title="1.2.3    Driver"></a>1.2.3    Driver</h4><blockquote><p>元数据存储在数据库中，默认存在自带的derby数据库（单用户局限性）中，推荐使用Mysql进行存储。</p><p>​            1） 解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完 成，比如ANTLR；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</p><p>​            2） 编译器（Physical Plan）：将AST编译生成逻辑执行计划。</p><p>​            3） 优化器（Query Optimizer）：对逻辑执行计划进行优化。</p><p>​            4） 执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是 MR&#x2F;Spark。</p></blockquote><p><img src="https://s2.loli.net/2022/06/01/Ubx7kIWChHfcLmv.png" alt="image-20220531000823975.png"></p><h4 id="1-2-4-数据处理"><a href="#1-2-4-数据处理" class="headerlink" title="1.2.4    数据处理"></a>1.2.4    数据处理</h4><blockquote><p>Hive的数据存储在HDFS中，计算由MapReduce完成。HDFS和MapReduce是源码级别上的整合，两者结合最佳。解释器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。</p></blockquote><h3 id="二、Hive的安装和使用"><a href="#二、Hive的安装和使用" class="headerlink" title="二、Hive的安装和使用"></a>二、Hive的安装和使用</h3><h4 id="2-1-离线安装MySQL（已经安装过MySQL可以跳过此步骤）"><a href="#2-1-离线安装MySQL（已经安装过MySQL可以跳过此步骤）" class="headerlink" title="2.1    离线安装MySQL（已经安装过MySQL可以跳过此步骤）"></a>2.1    离线安装MySQL（已经安装过MySQL可以跳过此步骤）</h4><p>注：</p><p>如果在&#x2F;usr&#x2F;bin&#x2F;mysql_secure_installation 一直是下面报错后</p><p><strong>解决办法：</strong></p><p>ps aux | grep mysql<br> 然后KILLmysql相关全部进程 Pid是进程号<br> kill -9 pid1 pid2 …</p><p>比如 kill -9 8301 8302<br> 然后再从第4步重新操作。</p><h4 id="2-2-修改MySQL编码"><a href="#2-2-修改MySQL编码" class="headerlink" title="2.2    修改MySQL编码"></a>2.2    修改MySQL编码</h4><p>1、修改mysql编码为UTF-8</p><p>1.1 编辑配置文件</p><pre><code>vim /etc/my.cnf</code></pre><p>1.2 加入以下内容：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client]</span><br><span class="line"></span><br><span class="line">default-character-set = utf8mb4</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line"></span><br><span class="line">character-set-server = utf8mb4</span><br><span class="line"></span><br><span class="line">collation-server = utf8mb4_general_ci</span><br></pre></td></tr></table></figure><p>1.3 重启mysql</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart mysqld</span><br></pre></td></tr></table></figure><p>1.4 登录mysql</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql -uroot -p123456</span><br></pre></td></tr></table></figure><p>1.5 查看mysql当前字符集</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">show variables like <span class="string">&#x27;%char%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>1.6 修改mysql元数据库hive，让其hive支持utf-8编码以支持中文</p><p>登录mysql：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql -u root -p123456</span><br></pre></td></tr></table></figure><p>切换到hive数据库:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">use hive;</span><br></pre></td></tr></table></figure><p>1).修改字段注释字符集</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> COLUMNS_V2 modify <span class="keyword">column</span> COMMENT <span class="type">varchar</span>(<span class="number">256</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure><p>2).修改表注释字符集</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> TABLE_PARAMS modify <span class="keyword">column</span> PARAM_VALUE <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure><p>3).修改分区表参数，以支持分区键能够用中文表示</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> PARTITION_PARAMS modify <span class="keyword">column</span> PARAM_VALUE <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> PARTITION_KEYS modify <span class="keyword">column</span> PKEY_COMMENT <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure><p>4).修改索引注解(可选)</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> INDEX_PARAMS modify <span class="keyword">column</span> PARAM_VALUE <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure><h4 id="2-3-安装Hive"><a href="#2-3-安装Hive" class="headerlink" title="2.3    安装Hive"></a>2.3    安装Hive</h4><p>前提是：mysql和hadoop必须已经成功启动了</p><h5 id="1、解压hive的安装包："><a href="#1、解压hive的安装包：" class="headerlink" title="1、解压hive的安装包："></a>1、解压hive的安装包：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf apache-hive-1.2.1-bin.tar.gz </span><br><span class="line"></span><br><span class="line">修改目录名称：</span><br><span class="line"><span class="built_in">mv</span> apache-hive-1.2.1-bin hive-1.2.1</span><br></pre></td></tr></table></figure><h5 id="2、进入hive-1-2-1-x2F-conf目录，复制备份文件并重命名"><a href="#2、进入hive-1-2-1-x2F-conf目录，复制备份文件并重命名" class="headerlink" title="2、进入hive-1.2.1&#x2F;conf目录，复制备份文件并重命名"></a>2、进入hive-1.2.1&#x2F;conf目录，复制备份文件并重命名</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cp</span> hive-env.sh.template hive-env.sh</span><br><span class="line"><span class="built_in">cp</span> hive-default.xml.template hive-site.xml</span><br></pre></td></tr></table></figure><h5 id="3、配置hive的配置文件"><a href="#3、配置hive的配置文件" class="headerlink" title="3、配置hive的配置文件"></a>3、配置hive的配置文件</h5><p>可以在 vim非编辑模式输入**&#x2F;想要查找的具体配置**，这样可以定位并以高亮形式标出</p><h6 id="3-1、修改hive-env-sh"><a href="#3-1、修改hive-env-sh" class="headerlink" title="3.1、修改hive-env.sh"></a>3.1、修改hive-env.sh</h6><p>加入三行内容（根据自己的目录和实际情况来添加）</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">HADOOP_HOME=/usr/local/soft/hadoop-2.7.6</span><br><span class="line">JAVA_HOME=/usr/local/soft/jdk1.8.0_171</span><br><span class="line">HIVE_HOME=/usr/local/soft/hive-1.2.1</span><br></pre></td></tr></table></figure><h6 id="3-2、修改hive-site-xml"><a href="#3-2、修改hive-site-xml" class="headerlink" title="3.2、修改hive-site.xml"></a>3.2、修改hive-site.xml</h6><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">&lt;！--数据存储位置就是我们在HDFS上看的目录--&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">(注意：修改自己安装mysql的主机地址）</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://192.168.40.110:3306/hive?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useUnicode=true<span class="symbol">&amp;amp;</span>characterEncoding=utf8<span class="symbol">&amp;amp;</span>useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">(固定写法，mysql驱动类的位置)</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">（mysql的用户名）</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">（mysql的用户密码）</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">（你的hive安装目录的tmp目录）</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.querylog.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/soft/hive-1.2.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">（同上）</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.local.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/soft/hive-1.2.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">（同上）</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.downloaded.resources.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/soft/hive-1.2.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--指定这个的时候，为了启动metastore服务的时候不用指定端口--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--hive --service metastore -p 9083 &amp; | hive --service metastore--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>thrift://master:9083<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>修改hadoop中<strong>core-site.xml</strong>直接改，改完重启就行，为后面beeline连接做准备</p><p><strong>注意：三个节点上的都要改。</strong></p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--该参数表示可以通过httpfs接口hdfs的ip地址限制--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hadoop.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--通过httpfs接口访问的用户获得的群组身份--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hadoop.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h5 id="4、拷贝mysql驱动到-HIVE-HOME-x2F-lib目录下"><a href="#4、拷贝mysql驱动到-HIVE-HOME-x2F-lib目录下" class="headerlink" title="4、拷贝mysql驱动到$HIVE_HOME&#x2F;lib目录下"></a>4、拷贝mysql驱动到$HIVE_HOME&#x2F;lib目录下</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cp</span> /usr/local/soft/mysql-connector-java-5.1.49.jar ../lib/</span><br></pre></td></tr></table></figure><h5 id="5、将hive的jline-2-12-jar拷贝到hadoop对应目录下，hive的-jline-2-12-jar-位置在-："><a href="#5、将hive的jline-2-12-jar拷贝到hadoop对应目录下，hive的-jline-2-12-jar-位置在-：" class="headerlink" title="5、将hive的jline-2.12.jar拷贝到hadoop对应目录下，hive的 jline-2.12.jar 位置在 ："></a>5、将hive的jline-2.12.jar拷贝到hadoop对应目录下，hive的 jline-2.12.jar 位置在 ：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/local/soft/hive-1.2.1/lib/jline-2.12.jar</span><br></pre></td></tr></table></figure><h5 id="6、将hive的jar拷过去hadoop下："><a href="#6、将hive的jar拷过去hadoop下：" class="headerlink" title="6、将hive的jar拷过去hadoop下："></a>6、将hive的jar拷过去hadoop下：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cp</span> /usr/local/soft/hive-1.2.1/lib/jline-2.12.jar /usr/local/soft/hadoop-2.7.6/share/hadoop/yarn/lib/</span><br></pre></td></tr></table></figure><h5 id="7、配置环境变量"><a href="#7、配置环境变量" class="headerlink" title="7、配置环境变量"></a>7、配置环境变量</h5><p> vim &#x2F;etc&#x2F;profile</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">HIVE_HOME=/usr/local/soft/hive-1.2.1</span><br><span class="line"><span class="built_in">export</span> PATH=.:<span class="variable">$HIVE_HOME</span>/bin</span><br><span class="line"></span><br><span class="line">重新加载环境变量</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><h5 id="8、分发Hive"><a href="#8、分发Hive" class="headerlink" title="8、分发Hive"></a>8、分发Hive</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">拷贝到其他两个节点中去，因为可能我们会在其他的节点上当作客户端访问hive，注意，也需要配置环境变量，增加驱动jar包，将hadoop的jline-0.9.94.jar的jar替换成hive的版本</span><br></pre></td></tr></table></figure><h5 id="9、启动hive："><a href="#9、启动hive：" class="headerlink" title="9、启动hive："></a>9、启动hive：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">启动hadoop</span><br><span class="line"></span><br><span class="line">start-all.sh</span><br><span class="line"></span><br><span class="line">启动hive</span><br><span class="line"></span><br><span class="line">​hive --service metastore</span><br><span class="line"></span><br><span class="line">​<span class="built_in">nohup</span> hive --service metastore &gt;/dev/null &amp;</span><br><span class="line"></span><br><span class="line">​hive</span><br><span class="line"></span><br><span class="line">启动HiveServer2</span><br><span class="line"></span><br><span class="line">​hiveserver2</span><br><span class="line"></span><br><span class="line">​<span class="built_in">nohup</span> hiveserver2 &gt;/dev/null &amp;</span><br><span class="line"></span><br><span class="line">​beeline -u jdbc:hive2://master:10000 -n root</span><br></pre></td></tr></table></figure><h6 id="9-1Hive的三种交互方式"><a href="#9-1Hive的三种交互方式" class="headerlink" title="9.1Hive的三种交互方式"></a>9.1Hive的三种交互方式</h6><h6 id="1）第一种交互方式"><a href="#1）第一种交互方式" class="headerlink" title="1）第一种交互方式"></a><strong>1）第一种交互方式</strong></h6><blockquote><p>shell交互Hive，用命令hive启动一个hive的shell命令行，在命令行中输入sql或者命令来和Hive交互。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">服务端启动metastore服务（后台启动）：nohup hive --service metastore &gt; /usr/local/soft/mylogs 2&gt;&amp;1 &amp;</span><br><span class="line">进入命令:hive</span><br><span class="line">退出命令行：quit;</span><br></pre></td></tr></table></figure><h6 id="2）第二种交互方式"><a href="#2）第二种交互方式" class="headerlink" title="2）第二种交互方式"></a><strong>2）第二种交互方式</strong></h6><blockquote><p><strong>Hive启动为一个服务器，对外提供服务</strong>，其他机器可以通过客户端通过协议连接到服务器，来完成访问操作，这是生产环境用法最多的</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">服务端启动hiveserver2服务：</span><br><span class="line">nohup hive --service metastore &gt;/dev/null &amp;</span><br><span class="line">nohup hiveserver2 &gt;/dev/null &amp;</span><br><span class="line"></span><br><span class="line">需要稍等一下，启动服务需要时间：</span><br><span class="line">进入命令:1)先执行： beeline ，再执行： !connect jdbc:hive2://master:10000 </span><br><span class="line">        2)或者直接执行：  beeline -u jdbc:hive2://master:10000 -n root</span><br><span class="line">退出命令行：！exit</span><br></pre></td></tr></table></figure><h6 id="3）第三种交互方式"><a href="#3）第三种交互方式" class="headerlink" title="3）第三种交互方式"></a><strong>3）第三种交互方式</strong></h6><blockquote><p>使用 –e 参数来直接执行hql的语句</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hive -e &quot;show databases;&quot;</span><br></pre></td></tr></table></figure><blockquote><p>使用 –f 参数通过指定文本文件来执行hql的语句</p><p>特点：执行完sql后，回到linux命令行。</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim hive.sql</span><br><span class="line"></span><br><span class="line">use myhive;</span><br><span class="line">select * from test;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive -f hive.sql</span><br></pre></td></tr></table></figure><h6 id="4）hive-cli和beeline-cli的区别"><a href="#4）hive-cli和beeline-cli的区别" class="headerlink" title="4）hive cli和beeline cli的区别"></a>4）hive cli和beeline cli的区别</h6><p><img src="https://s2.loli.net/2022/06/01/CaqXyTQ47VhWeZ3.png" alt="image-20220531230402802.png"></p>]]></content>
    
    
    <summary type="html">对学习Hive的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hive" scheme="http://example.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive的基本操作</title>
    <link href="http://example.com/2022/05/31/Hive%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-1/"/>
    <id>http://example.com/2022/05/31/Hive%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-1/</id>
    <published>2022-05-30T16:00:00.000Z</published>
    <updated>2022-06-05T05:50:31.467Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1、Hive元数据"><a href="#1、Hive元数据" class="headerlink" title="1、Hive元数据"></a>1、Hive元数据</h3><p><strong>Hive元数据库中一些重要的表结构及用途</strong>，方便Impala、SparkSQL、Hive等组件访问元数据库的理解。</p><p>1、存储Hive版本的**元数据表(VERSION)**，该表比较简单，但很重要,如果这个表出现问题，根本进不来Hive-Cli。比如该表不存在，当启动Hive-Cli的时候，就会报错“Table ‘hive.version’ doesn’t exist”</p><p>2、Hive数据库相关的元数据表(DBS、DATABASE_PARAMS)</p><p>​        DBS：该表存储Hive中所有数据库的基本信息。</p><p>​        DATABASE_PARAMS：该表存储数据库的相关参数。</p><p>3、Hive表和视图相关的元数据表</p><p>​        主要有TBLS、TABLE_PARAMS、TBL_PRIVS，这三张表通过TBL_ID关联。<br>​        TBLS:该表中存储Hive表，视图，索引表的基本信息。<br>​        TABLE_PARAMS:该表存储表&#x2F;视图的属性信息。<br>​        TBL_PRIVS：该表存储表&#x2F;视图的授权信息。<br>4、Hive文件存储信息相关的元数据表</p><p>​        主要涉及SDS、SD_PARAMS、SERDES、SERDE_PARAMS，由于HDFS支持的文件格式很多，而建Hive表时候也可以指定各种文件格式，Hive在将HQL解析成MapReduce时候，需要知道去哪里，使用哪种格式去读写HDFS文件，而这些信息就保存在这几张表中。<br>​        SDS：该表保存文件存储的基本信息，如INPUT_FORMAT、OUTPUT_FORMAT、是否压缩等。TBLS表中的SD_ID与该表关联，可以获取Hive表的存储信息。<br>​        SD_PARAMS: 该表存储Hive存储的属性信息。<br>​        SERDES:该表存储序列化使用的类信息。<br>​        SERDE_PARAMS:该表存储序列化的一些属性、格式信息，比如:行、列分隔符。<br>5、Hive表字段相关的元数据表</p><p>​        主要涉及COLUMNS_V2：该表存储表对应的字段信息。</p><h3 id="2、Hive的基本操作"><a href="#2、Hive的基本操作" class="headerlink" title="2、Hive的基本操作"></a>2、Hive的基本操作</h3><h4 id="2-1-Hive库操作"><a href="#2-1-Hive库操作" class="headerlink" title="2.1    Hive库操作"></a>2.1    Hive库操作</h4><h5 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h5><blockquote><p>1）创建一个数据库，数据库在<strong>HDFS上的默认存储路径是&#x2F;hive&#x2F;warehouse&#x2F;*.db</strong>。</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> database testdb;</span><br></pre></td></tr></table></figure><blockquote><p>2）避免要创建的数据库已经存在错误，增加if not exists判断。<strong>（标准写法）</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> testdb; </span><br></pre></td></tr></table></figure><h5 id="创建数据库和位置"><a href="#创建数据库和位置" class="headerlink" title="创建数据库和位置"></a>创建数据库和位置</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> dept location <span class="string">&#x27;/testdb.db&#x27;</span>;</span><br></pre></td></tr></table></figure><h5 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h5><blockquote><p><strong>数据库的其他元数据信息都是不可更改的</strong>，包括数据库名和数据库所在的目录位置。</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> database dept <span class="keyword">set</span> dbproperties(<span class="string">&#x27;createtime&#x27;</span><span class="operator">=</span><span class="string">&#x27;20220531&#x27;</span>);</span><br></pre></td></tr></table></figure><h5 id="数据库详细信息"><a href="#数据库详细信息" class="headerlink" title="数据库详细信息"></a>数据库详细信息</h5><blockquote><p>1）显示数据库（show）</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> databases;</span><br></pre></td></tr></table></figure><blockquote><p>2）可以通过like进行过滤</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> databases <span class="keyword">like</span> <span class="string">&#x27;t*&#x27;</span>;</span><br></pre></td></tr></table></figure><blockquote><p>3）查看详情（desc）</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">desc</span> database testdb;</span><br></pre></td></tr></table></figure><blockquote><p>4）切换数据库（use）</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">use testdb;</span><br></pre></td></tr></table></figure><h5 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h5><blockquote><p>1）最简写法</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> database testdb;</span><br></pre></td></tr></table></figure><blockquote><p>2）如果删除的数据库不存在，最好使用if exists判断数据库是否存在。否则会报错：FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> database if <span class="keyword">exists</span> testdb;</span><br></pre></td></tr></table></figure><blockquote><p>3)如果数据库不为空，使用cascade命令进行强制删除。报错信息如下FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> database if <span class="keyword">exists</span> testdb cascade;</span><br></pre></td></tr></table></figure><h3 id="2-2-Hive数据类型"><a href="#2-2-Hive数据类型" class="headerlink" title="2.2    Hive数据类型"></a>2.2    Hive数据类型</h3><h4 id="基础数据类型"><a href="#基础数据类型" class="headerlink" title="基础数据类型"></a>基础数据类型</h4><table><thead><tr><th>类型</th><th>Java数据类型</th><th>描述</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>8位有符号整型。取值范围：-128~127。</td></tr><tr><td>SMALLINT</td><td>short</td><td>16位有符号整型。取值范围：-32768~32767。</td></tr><tr><td>INT</td><td>int</td><td>32位有符号整型。取值范围：-2 31 ~2 31 -1。</td></tr><tr><td><strong>BIGINT</strong></td><td>long</td><td>64位有符号整型。取值范围：-2 63 +1~2 63 -1。</td></tr><tr><td>BINARY</td><td></td><td>二进制数据类型，目前长度限制为8MB。</td></tr><tr><td>FLOAT</td><td>float</td><td>32位二进制浮点型。</td></tr><tr><td>DOUBLE</td><td>double</td><td>64位二进制浮点型。</td></tr><tr><td><strong>DECIMAL(precision,scale)</strong></td><td></td><td>10进制精确数字类型。precision：表示最多可以表示多少位的数字。取值范围：1 &lt;&#x3D; precision &lt;&#x3D; 38。scale：表示小数部分的位数。取值范围： 0 &lt;&#x3D; scale &lt;&#x3D; 38。如果不指定以上两个参数，则默认为decimal(10,0)。</td></tr><tr><td>VARCHAR(n)</td><td></td><td>变长字符类型，n为长度。取值范围：1~65535。</td></tr><tr><td>CHAR(n)</td><td></td><td>固定长度字符类型，n为长度。最大取值255。长度不足则会填充空格，但空格不参与比较。</td></tr><tr><td><strong>STRING</strong></td><td>string</td><td>字符串类型，目前长度限制为8MB。</td></tr><tr><td>DATE</td><td></td><td>日期类型，格式为<code>yyyy-mm-dd</code>。取值范围：0000-01-01~9999-12-31。</td></tr><tr><td>DATETIME</td><td></td><td>日期时间类型。取值范围：0000-01-01 00:00:00.000~9999-12-31 23.59:59.999，精确到毫秒。</td></tr><tr><td><strong>TIMESTAMP</strong></td><td></td><td>与时区无关的时间戳类型。取值范围：0000-01-01 00:00:00.000000000~9999-12-31 23.59:59.999999999，精确到纳秒。说明 对于部分时区相关的函数，例如cast(<a timestamp> as string)，要求TIMESTAMP按照与当前时区相符的方式来展现。</td></tr><tr><td><strong>BOOLEAN</strong></td><td>boolean</td><td>BOOLEAN类型。取值：True、False。</td></tr></tbody></table><h4 id="复杂的数据类型"><a href="#复杂的数据类型" class="headerlink" title="复杂的数据类型"></a>复杂的数据类型</h4><table><thead><tr><th>类型</th><th>定义方法</th><th>构造方法</th></tr></thead><tbody><tr><td>ARRAY</td><td><code>array&lt;int&gt;``array&lt;struct&lt;a:int, b:string&gt;&gt;</code></td><td><code>array(1, 2, 3)``array(array(1, 2), array(3, 4))</code></td></tr><tr><td>MAP</td><td><code>map&lt;string, string&gt;``map&lt;smallint, array&lt;string&gt;&gt;</code></td><td><code>map(“k1”, “v1”, “k2”, “v2”)``map(1S, array(‘a’, ‘b’), 2S, array(‘x’, ‘y’))</code></td></tr><tr><td>STRUCT</td><td></td><td>struct&lt;x:int, y:int&gt;<code>struct&lt;field1:bigint, field2:array&lt;int&gt;, field3:map&lt;int, int&gt;&gt;    named_struct(‘x’, 1, ‘y’, 2)</code>named_struct(‘field1’, 100L, ‘field2’, array(1, 2), ‘field3’, map(1, 100, 2, 200))</td></tr></tbody></table><blockquote><p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。还有一个uniontype&lt; 所有类型，所有类型… &gt; 。</p><p>​        数组：array&lt; 所有类型 &gt;；<br>​        Map &lt; 基本数据类型，所有数据类型 &gt;；<br>​        struct &lt; 名：所有类型[注释] &gt;;<br>​        uniontype&lt; 所有类型，所有类型… &gt;</p></blockquote><h3 id="2-3-Hive表操作"><a href="#2-3-Hive表操作" class="headerlink" title="2.3    Hive表操作"></a>2.3    Hive表操作</h3><blockquote><p>Hive的存储格式:</p><p>Hive没有专门的数据文件格式,常见的有以下几种:</p><p>​        <strong>TEXTFILE</strong><br>​        SEQUENCEFILE<br>​        AVRO<br>​        <strong>RCFILE</strong><br>​        <strong>ORCFILE</strong><br>​        <strong>PARQUET</strong></p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">TextFile:</span><br><span class="line">       TEXTFILE 即正常的文本格式，是Hive默认文件存储格式，因为大多数情况下源数据文件都是以text文件格式保存（便于查看验数和防止乱码）。此种格式的表文件在HDFS上是明文，可用hadoop fs -cat命令查看，从HDFS上get下来后也可以直接读取。</span><br><span class="line">        TEXTFILE 存储文件默认每一行就是一条记录，可以指定任意的分隔符进行字段间的分割。但这个格式无压缩，需要的存储空间很大。虽然可结合Gzip、Bzip2、Snappy等使用，使用这种方式，Hive不会对数据进行切分，从而无法对数据进行并行操作。</span><br><span class="line">一般只有与其他系统由数据交互的接口表采用TEXTFILE 格式，其他事实表和维度表都不建议使用。</span><br><span class="line"></span><br><span class="line">RCFile:</span><br><span class="line">Record Columnar的缩写。是Hadoop中第一个列文件格式。能够很好的压缩和快速的查询性能。通常写操作比较慢，比非列形式的文件格式需要更多的内存空间和计算量。 RCFile是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据`列式存储`，有利于数据压缩和快速的列存取。</span><br><span class="line"></span><br><span class="line">ORCFile:</span><br><span class="line">Hive从0.11版本开始提供了ORC的文件格式，ORC文件不仅仅是一种列式文件存储格式，最重要的是有着很高的压缩比，并且对于MapReduce来说是可切分（Split）的。因此，在Hive中使用ORC作为表的文件存储格式，不仅可以很大程度的节省HDFS存储资源，而且对数据的查询和处理性能有着非常大的提升，因为ORC较其他文件格式压缩比高，查询任务的输入数据量减少，使用的Task也就减少了。ORC能很大程度的节省存储和计算资源，但它在读写时候需要消耗额外的CPU资源来压缩和解压缩，当然这部分的CPU消耗是非常少的。</span><br><span class="line"></span><br><span class="line">Parquet:</span><br><span class="line">通常我们使用关系数据库存储结构化数据，而关系数据库中使用数据模型都是扁平式的，遇到诸如List、Map和自定义Struct的时候就需要用户在应用层解析。但是在大数据环境下，通常数据的来源是服务端的埋点数据，很可能需要把程序中的某些对象内容作为输出的一部分，而每一个对象都可能是嵌套的，所以如果能够原生的支持这种数据，这样在查询的时候就不需要额外的解析便能获得想要的结果。Parquet的灵感来自于2010年Google发表的Dremel论文，文中介绍了一种支持嵌套结构的存储格式，并且使用了列式存储的方式提升查询性能。Parquet仅仅是一种存储格式，它是语言、平台无关的，并且不需要和任何一种数据处理框架绑定。这也是parquet相较于orc的仅有优势：支持嵌套结构。Parquet 没有太多其他可圈可点的地方,比如他不支持update操作(数据写成后不可修改),不支持ACID等.</span><br><span class="line"></span><br><span class="line">SEQUENCEFILE:</span><br><span class="line">SequenceFile是Hadoop API 提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中。这种二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。它与Hadoop API中的MapFile 是互相兼容的。Hive 中的SequenceFile 继承自Hadoop API 的SequenceFile，不过它的key为空，使用value 存放实际的值， 这样是为了避免MR 在运行map 阶段的排序过程。SequenceFile支持三种压缩选择：NONE, RECORD, BLOCK。 Record压缩率低，一般建议使用BLOCK压缩。 SequenceFile最重要的优点就是Hadoop原生支持较好，有API，但除此之外平平无奇，实际生产中不会使用。</span><br><span class="line"></span><br><span class="line">AVRO:</span><br><span class="line">Avro是一种用于支持数据密集型的二进制文件格式。它的文件格式更为紧凑，若要读取大量数据时，Avro能够提供更好的序列化和反序列化性能。并且Avro数据文件天生是带Schema定义的，所以它不需要开发者在API 级别实现自己的Writable对象。Avro提供的机制使动态语言可以方便地处理Avro数据。最近多个Hadoop 子项目都支持Avro 数据格式，如Pig 、Hive、Flume、Sqoop和Hcatalog。</span><br></pre></td></tr></table></figure><p><strong>Hive的四大常用存储格式存储效率及执行速度对比</strong></p><blockquote><p>结论：ORCFILE存储文件读操作效率最高</p><p>耗时比较：ORC&lt;Parquet&lt;RC&lt;Text</p></blockquote><blockquote><p>结论：ORCFILE存储文件占用空间少，压缩效率高</p><p>占用空间：ORC&lt;Parquet&lt;RC&lt;Text</p></blockquote><h4 id="2-3-1-创建表"><a href="#2-3-1-创建表" class="headerlink" title="2.3.1    创建表"></a>2.3.1    创建表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[COMMENT table_comment] </span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span><span class="operator">|</span><span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] </span><br><span class="line">[<span class="type">ROW</span> FORMAT row_format] </span><br><span class="line">[STORED <span class="keyword">AS</span> file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">字段解释说明:</span><br><span class="line"><span class="operator">-</span> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> </span><br><span class="line">创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> 选项来忽略这个异常。</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> <span class="keyword">EXTERNAL</span></span><br><span class="line">关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION）</span><br><span class="line">创建内部表时，会将数据移动到数据仓库指向的路径（默认位置）；</span><br><span class="line">创建外部表时，仅记录数据所在的路径，不对数据的位置做任何改变。在</span><br><span class="line">删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> COMMENT：</span><br><span class="line">为表和列添加注释。</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> PARTITIONED <span class="keyword">BY</span></span><br><span class="line">创建分区表</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> CLUSTERED <span class="keyword">BY</span></span><br><span class="line">创建分桶表</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> SORTED <span class="keyword">BY</span></span><br><span class="line">不常用</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> <span class="type">ROW</span> FORMAT </span><br><span class="line">  DELIMITED [FIELDS TERMINATED <span class="keyword">BY</span> <span class="type">char</span>] [COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="type">char</span>] [MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="type">char</span>] [LINES TERMINATED <span class="keyword">BY</span> <span class="type">char</span>] <span class="operator">|</span> SERDE serde_name [<span class="keyword">WITH</span> SERDEPROPERTIES (property_name<span class="operator">=</span>property_value, property_name<span class="operator">=</span>property_value, ...)]</span><br><span class="line">用户在建表的时候可以自定义SerDe或者使用自带的SerDe。</span><br><span class="line">如果没有指定<span class="type">ROW</span> FORMAT 或者<span class="type">ROW</span> FORMAT DELIMITED，将会使用自带的SerDe。</span><br><span class="line">在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。</span><br><span class="line">SerDe是Serialize<span class="operator">/</span>Deserilize的简称，目的是用于序列化和反序列化。</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> STORED <span class="keyword">AS</span>指定存储文件类型</span><br><span class="line">常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</span><br><span class="line">如果文件数据是纯文本，可以使用STORED <span class="keyword">AS</span> TEXTFILE。</span><br><span class="line">如果数据需要压缩，使用 STORED <span class="keyword">AS</span> SEQUENCEFILE。</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> LOCATION ：</span><br><span class="line">指定表在HDFS上的存储位置。</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> <span class="keyword">LIKE</span></span><br><span class="line">允许用户复制现有的表结构，但是不复制数据。</span><br></pre></td></tr></table></figure><h5 id="建表1：全部使用默认建表方式"><a href="#建表1：全部使用默认建表方式" class="headerlink" title="建表1：全部使用默认建表方式"></a>建表1：全部使用默认建表方式</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table students</span><br><span class="line">(</span><br><span class="line">    id bigint,</span><br><span class="line">    name string,</span><br><span class="line">    age int,</span><br><span class="line">    gender string,</span><br><span class="line">    clazz string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;; // 必选，指定列分隔符 </span><br></pre></td></tr></table></figure><h5 id="建表2：指定location-（这种方式也比较常用）"><a href="#建表2：指定location-（这种方式也比较常用）" class="headerlink" title="建表2：指定location （这种方式也比较常用）"></a>建表2：指定location （这种方式也比较常用）</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table students2</span><br><span class="line">(</span><br><span class="line">    id bigint,</span><br><span class="line">    name string,</span><br><span class="line">    age int,</span><br><span class="line">    gender string,</span><br><span class="line">    clazz string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;</span><br><span class="line">LOCATION &#x27;/input1&#x27;; // 指定Hive表的数据的存储位置，一般在数据已经上传到HDFS，想要直接使用，会指定Location，通常Locaion会跟外部表一起使用，内部表一般使用默认的location</span><br></pre></td></tr></table></figure><h5 id="建表3：指定存储格式"><a href="#建表3：指定存储格式" class="headerlink" title="建表3：指定存储格式"></a>建表3：指定存储格式</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table students3</span><br><span class="line">(</span><br><span class="line">    id bigint,</span><br><span class="line">    name string,</span><br><span class="line">    age int,</span><br><span class="line">    gender string,</span><br><span class="line">    clazz string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;</span><br><span class="line">STORED AS rcfile; // 指定储存格式为rcfile，inputFormat:RCFileInputFormat,outputFormat:RCFileOutputFormat，如果不指定，默认为textfile，注意：除textfile以外，其他的存储格式的数据都不能直接加载，需要使用从表加载的方式。</span><br></pre></td></tr></table></figure><h5 id="建表4：create-table-xxxx-as-select-statement-SQL语句-这种方式比较常用"><a href="#建表4：create-table-xxxx-as-select-statement-SQL语句-这种方式比较常用" class="headerlink" title="建表4：create table xxxx as select_statement(SQL语句) (这种方式比较常用)"></a>建表4：create table xxxx as select_statement(SQL语句) (这种方式比较常用)</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table students4 as select * from students2;</span><br></pre></td></tr></table></figure><h5 id="建表5：create-table-xxxx-like-table-name-只想建表，不需要加载数据"><a href="#建表5：create-table-xxxx-like-table-name-只想建表，不需要加载数据" class="headerlink" title="建表5：create table xxxx like table_name  只想建表，不需要加载数据"></a>建表5：create table xxxx like table_name  只想建表，不需要加载数据</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table students5 like students;</span><br></pre></td></tr></table></figure><blockquote><p><strong>简单用户信息表创建：</strong></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_user(</span><br><span class="line">id <span class="type">int</span>,</span><br><span class="line">uname string,</span><br><span class="line">pwd string,</span><br><span class="line">gender string,</span><br><span class="line">age <span class="type">int</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1,admin,123456,男,18</span><br><span class="line">2,zhangsan,abc123,男,23</span><br><span class="line">3,lisi,654321,女,16</span><br></pre></td></tr></table></figure><p>复杂人员信息表创建：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> t_person(</span><br><span class="line">name string,</span><br><span class="line">friends <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>,</span><br><span class="line">children map<span class="operator">&lt;</span>string,<span class="type">int</span><span class="operator">&gt;</span>,</span><br><span class="line">address struct<span class="operator">&lt;</span>street:string ,city:string<span class="operator">&gt;</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">collection items terminated <span class="keyword">by</span> <span class="string">&#x27;_&#x27;</span></span><br><span class="line">map keys terminated <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:<span class="number">18</span>_xiaoxiao song:<span class="number">19</span>,beng bu_anhui</span><br><span class="line">yangyang,caicai_susu,xiao yang:<span class="number">18</span>_xiaoxiao yang:<span class="number">19</span>,he fei_anhui</span><br></pre></td></tr></table></figure></blockquote><h4 id="2-3-2-显示表"><a href="#2-3-2-显示表" class="headerlink" title="2.3.2    显示表"></a>2.3.2    显示表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> tables;</span><br><span class="line"><span class="keyword">show</span> tables <span class="keyword">like</span> <span class="string">&#x27;u&#x27;</span>;</span><br><span class="line"><span class="keyword">desc</span> t_person;</span><br><span class="line"><span class="keyword">desc</span> formatted t_person;</span><br></pre></td></tr></table></figure><h4 id="2-3-3-加载数据"><a href="#2-3-3-加载数据" class="headerlink" title="2.3.3    加载数据"></a>2.3.3    加载数据</h4><h5 id="1、使用hdfs-dfs-put-39-本地数据-39-39-hive表对应的HDFS目录下-39"><a href="#1、使用hdfs-dfs-put-39-本地数据-39-39-hive表对应的HDFS目录下-39" class="headerlink" title="1、使用hdfs dfs -put &#39;本地数据&#39; &#39;hive表对应的HDFS目录下&#39;"></a>1、使用<code>hdfs dfs -put &#39;本地数据&#39; &#39;hive表对应的HDFS目录下&#39;</code></h5><h5 id="2、使用-load-data-inpath"><a href="#2、使用-load-data-inpath" class="headerlink" title="2、使用 load data inpath"></a>2、使用 load data inpath</h5><blockquote><p>下列命令需要在hive shell里执行</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 将HDFS上的/input1目录下面的数据 移动至 students表对应的HDFS目录下，注意是 移动、移动、移动</span><br><span class="line">load data inpath &#x27;/input1/students.txt&#x27; into table students;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 清空表</span><br><span class="line">truncate table students;</span><br><span class="line">// 加上 local 关键字 可以将Linux本地目录下的文件 上传到 hive表对应HDFS 目录下 原文件不会被删除</span><br><span class="line">load data local inpath &#x27;/usr/local/soft/data/students.txt&#x27; into table students;</span><br><span class="line">// overwrite 覆盖加载</span><br><span class="line">load data local inpath &#x27;/usr/local/soft/data/students.txt&#x27; overwrite into table students;</span><br></pre></td></tr></table></figure><h5 id="3、create-table-xxx-as-SQL语句"><a href="#3、create-table-xxx-as-SQL语句" class="headerlink" title="3、create table xxx as SQL语句"></a>3、create table xxx as SQL语句</h5><h5 id="4、insert-into-table-xxxx-SQL语句-（没有as）"><a href="#4、insert-into-table-xxxx-SQL语句-（没有as）" class="headerlink" title="4、insert into table xxxx SQL语句 （没有as）"></a>4、insert into table xxxx SQL语句 （没有as）</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 将 students表的数据插入到students2 这是复制 不是移动 students表中的表中的数据不会丢失</span><br><span class="line">insert into table students2 select * from students;</span><br><span class="line"></span><br><span class="line">// 覆盖插入 把into 换成 overwrite</span><br><span class="line">insert overwrite table students2 select * from students;</span><br></pre></td></tr></table></figure><h4 id="2-3-4-修改列"><a href="#2-3-4-修改列" class="headerlink" title="2.3.4    修改列"></a>2.3.4    修改列</h4><blockquote><p>查询表结构</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">desc</span> students2;</span><br></pre></td></tr></table></figure><blockquote><p>添加列</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> students2 <span class="keyword">add</span> columns (education string);</span><br></pre></td></tr></table></figure><blockquote><p>查询表结构</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">desc</span> students2;</span><br></pre></td></tr></table></figure><blockquote><p>更新列</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> stduents2 change education educationnew string;</span><br></pre></td></tr></table></figure><h4 id="2-3-5-删除表"><a href="#2-3-5-删除表" class="headerlink" title="2.3.5    删除表"></a>2.3.5    删除表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> students2;</span><br></pre></td></tr></table></figure><h3 id="2-4-Hive内外部表"><a href="#2-4-Hive内外部表" class="headerlink" title="2.4    Hive内外部表"></a>2.4    Hive内外部表</h3><blockquote><p><strong>面试题：内部表和外部表的区别？如何创建外部表？工作中使用外部表</strong></p></blockquote><h4 id="2-4-1-hive内部表"><a href="#2-4-1-hive内部表" class="headerlink" title="2.4.1    hive内部表"></a>2.4.1    hive内部表</h4><blockquote><p>当<strong>创建好表的时候，HDFS会在当前表所属的库中创建一个文件夹</strong></p><p>当设置表路径的时候，如果直接指向一个已有的路径,可以直接去使用文件夹中的数据</p><p><strong>当load数据的时候，就会将数据文件存放到表对应的文件夹中</strong></p><p>而且<strong>数据一旦被load，就不能被修改</strong></p><p>我们查询数据也是查询文件中的文件,这些数据最终都会存放到HDFS</p><p>当我们<strong>删除表的时候，表对应的文件夹会被删除，同时数据也会被删除</strong></p><p><strong>默认建表的类型就是内部表</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> 内部表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> students_internal</span><br><span class="line">(</span><br><span class="line">    id <span class="type">bigint</span>,</span><br><span class="line">    name string,</span><br><span class="line">    age <span class="type">int</span>,</span><br><span class="line">    gender string,</span><br><span class="line">    clazz string</span><br><span class="line">)</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">LOCATION <span class="string">&#x27;/input2&#x27;</span>;</span><br><span class="line"></span><br><span class="line">hive<span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>usr<span class="operator">/</span><span class="keyword">local</span><span class="operator">/</span>soft<span class="operator">/</span>data<span class="operator">/</span>students.txt <span class="operator">/</span>input2<span class="operator">/</span>;</span><br></pre></td></tr></table></figure><h4 id="2-4-1-Hive外部表"><a href="#2-4-1-Hive外部表" class="headerlink" title="2.4.1    Hive外部表"></a>2.4.1    Hive外部表</h4><blockquote><p>外部表说明</p><p>​    <strong>外部表因为是指定其他的hdfs路径的数据加载到表中来，所以hive会认为自己不完全独占这份数据</strong></p><p>​    <strong>删除hive表的时候，数据仍然保存在hdfs中，不会删除。</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> 外部表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> students_external</span><br><span class="line">(</span><br><span class="line">    id <span class="type">bigint</span>,</span><br><span class="line">    name string,</span><br><span class="line">    age <span class="type">int</span>,</span><br><span class="line">    gender string,</span><br><span class="line">    clazz string</span><br><span class="line">)</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">LOCATION <span class="string">&#x27;/input3&#x27;</span>;</span><br><span class="line"></span><br><span class="line">hive<span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>usr<span class="operator">/</span><span class="keyword">local</span><span class="operator">/</span>soft<span class="operator">/</span>data<span class="operator">/</span>students.txt <span class="operator">/</span>input3<span class="operator">/</span>;</span><br></pre></td></tr></table></figure><blockquote><p>删除表测试一下：</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">drop</span> <span class="keyword">table</span> students_internal;</span><br><span class="line">Moved: <span class="string">&#x27;hdfs://master:9000/input2&#x27;</span> <span class="keyword">to</span> trash <span class="keyword">at</span>: hdfs:<span class="operator">/</span><span class="operator">/</span>master:<span class="number">9000</span><span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>root<span class="operator">/</span>.Trash<span class="operator">/</span><span class="keyword">Current</span></span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.474</span> seconds</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">drop</span> <span class="keyword">table</span> students_external;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.09</span> seconds</span><br><span class="line">hive<span class="operator">&gt;</span> </span><br></pre></td></tr></table></figure><blockquote><p>一般在公司中，使用外部表多一点，因为数据可以需要被多个程序使用，避免误删，通常外部表会结合location一起使用</p><p>外部表还可以将其他数据源中的数据 映射到 hive中，比如说：hbase，ElasticSearch……</p><p>设计外部表的初衷就是 让 表的元数据 与 数据 解耦</p></blockquote><ul><li>操作案例:  分别创建dept，emp，salgrade。并加载数据。</li></ul><blockquote><p>创建数据文件存放的目录</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hdfs dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p <span class="operator">/</span>shujia<span class="operator">/</span>bigdata17<span class="operator">/</span>dept</span><br><span class="line">hdfs dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p <span class="operator">/</span>shujia<span class="operator">/</span>bigdata17<span class="operator">/</span>emp</span><br><span class="line">hdfs dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p <span class="operator">/</span>shujia<span class="operator">/</span>bigdata17<span class="operator">/</span>salgrade</span><br></pre></td></tr></table></figure><ul><li>创建dept表</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> dept (</span><br><span class="line">  DEPTNO <span class="type">int</span>,</span><br><span class="line">  DNAME <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">  LOC <span class="type">varchar</span>(<span class="number">255</span>)</span><br><span class="line">) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">location <span class="string">&#x27;/shujia/bigdata17/dept&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="number">10</span>,ACCOUNTING,<span class="keyword">NEW</span> YORK</span><br><span class="line"><span class="number">20</span>,RESEARCH,DALLAS</span><br><span class="line"><span class="number">30</span>,SALES,CHICAGO</span><br><span class="line"><span class="number">40</span>,OPERATIONS,BOSTON</span><br></pre></td></tr></table></figure><ul><li>创建emp表</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> emp (</span><br><span class="line">   EMPNO <span class="type">int</span>,</span><br><span class="line">   ENAME <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">   JOB <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">   MGR <span class="type">int</span>,</span><br><span class="line">   HIREDATE <span class="type">date</span>,</span><br><span class="line">   SAL <span class="type">decimal</span>(<span class="number">10</span>,<span class="number">0</span>),</span><br><span class="line">   COMM <span class="type">decimal</span>(<span class="number">10</span>,<span class="number">0</span>),</span><br><span class="line">   DEPTNO <span class="type">int</span></span><br><span class="line"> ) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line"> location <span class="string">&#x27;/shujia/bigdata17/emp&#x27;</span>;</span><br><span class="line"> </span><br><span class="line"><span class="number">7369</span>,SMITH,CLERK,<span class="number">7902</span>,<span class="number">1980</span><span class="number">-12</span><span class="number">-17</span>,<span class="number">800</span>,<span class="keyword">null</span>,<span class="number">20</span></span><br><span class="line"><span class="number">7499</span>,ALLEN,SALESMAN,<span class="number">7698</span>,<span class="number">1981</span><span class="number">-02</span><span class="number">-20</span>,<span class="number">1600</span>,<span class="number">300</span>,<span class="number">30</span></span><br><span class="line"><span class="number">7521</span>,WARD,SALESMAN,<span class="number">7698</span>,<span class="number">1981</span><span class="number">-02</span><span class="number">-22</span>,<span class="number">1250</span>,<span class="number">500</span>,<span class="number">30</span></span><br><span class="line"><span class="number">7566</span>,JONES,MANAGER,<span class="number">7839</span>,<span class="number">1981</span><span class="number">-04</span><span class="number">-02</span>,<span class="number">2975</span>,<span class="keyword">null</span>,<span class="number">20</span></span><br><span class="line"><span class="number">7654</span>,MARTIN,SALESMAN,<span class="number">7698</span>,<span class="number">1981</span><span class="number">-09</span><span class="number">-28</span>,<span class="number">1250</span>,<span class="number">1400</span>,<span class="number">30</span></span><br><span class="line"><span class="number">7698</span>,BLAKE,MANAGER,<span class="number">7839</span>,<span class="number">1981</span><span class="number">-05</span><span class="number">-01</span>,<span class="number">2850</span>,<span class="keyword">null</span>,<span class="number">30</span></span><br><span class="line"><span class="number">7782</span>,CLARK,MANAGER,<span class="number">7839</span>,<span class="number">1981</span><span class="number">-06</span><span class="number">-09</span>,<span class="number">2450</span>,<span class="keyword">null</span>,<span class="number">10</span></span><br><span class="line"><span class="number">7788</span>,SCOTT,ANALYST,<span class="number">7566</span>,<span class="number">1987</span><span class="number">-07</span><span class="number">-13</span>,<span class="number">3000</span>,<span class="keyword">null</span>,<span class="number">20</span></span><br><span class="line"><span class="number">7839</span>,KING,PRESIDENT,<span class="keyword">null</span>,<span class="number">1981</span><span class="number">-11</span><span class="number">-17</span>,<span class="number">5000</span>,<span class="keyword">null</span>,<span class="number">10</span></span><br><span class="line"><span class="number">7844</span>,TURNER,SALESMAN,<span class="number">7698</span>,<span class="number">1981</span><span class="number">-09</span><span class="number">-08</span>,<span class="number">1500</span>,<span class="number">0</span>,<span class="number">30</span></span><br><span class="line"><span class="number">7876</span>,ADAMS,CLERK,<span class="number">7788</span>,<span class="number">1987</span><span class="number">-07</span><span class="number">-13</span>,<span class="number">1100</span>,<span class="keyword">null</span>,<span class="number">20</span></span><br><span class="line"><span class="number">7900</span>,JAMES,CLERK,<span class="number">7698</span>,<span class="number">1981</span><span class="number">-12</span><span class="number">-03</span>,<span class="number">950</span>,<span class="keyword">null</span>,<span class="number">30</span></span><br><span class="line"><span class="number">7902</span>,FORD,ANALYST,<span class="number">7566</span>,<span class="number">1981</span><span class="number">-12</span><span class="number">-03</span>,<span class="number">3000</span>,<span class="keyword">null</span>,<span class="number">20</span></span><br><span class="line"><span class="number">7934</span>,MILLER,CLERK,<span class="number">7782</span>,<span class="number">1982</span><span class="number">-01</span><span class="number">-23</span>,<span class="number">1300</span>,<span class="keyword">null</span>,<span class="number">10</span></span><br></pre></td></tr></table></figure><ul><li>创建salgrade表</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> salgrade (</span><br><span class="line">  GRADE <span class="type">int</span>,</span><br><span class="line">  LOSAL <span class="type">int</span>,</span><br><span class="line">  HISAL <span class="type">int</span></span><br><span class="line">) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">location <span class="string">&#x27;/shujia/bigdata17/salgrade&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="number">1</span>,<span class="number">700</span>,<span class="number">1200</span></span><br><span class="line"><span class="number">2</span>,<span class="number">1201</span>,<span class="number">1400</span></span><br><span class="line"><span class="number">3</span>,<span class="number">1401</span>,<span class="number">2000</span></span><br><span class="line"><span class="number">4</span>,<span class="number">2001</span>,<span class="number">3000</span></span><br><span class="line"><span class="number">5</span>,<span class="number">3001</span>,<span class="number">9999</span></span><br></pre></td></tr></table></figure><h3 id="2-5-Hive导出数据"><a href="#2-5-Hive导出数据" class="headerlink" title="2.5    Hive导出数据"></a>2.5    Hive导出数据</h3><blockquote><p>将表中的数据备份</p></blockquote><p>将查询结果存放到本地</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span>创建存放数据的目录</span><br><span class="line">mkdir <span class="operator">-</span>p <span class="operator">/</span>usr<span class="operator">/</span><span class="keyword">local</span><span class="operator">/</span>soft<span class="operator">/</span>shujia</span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>导出查询结果的数据(导出到Node01上)</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/usr/local/soft/shujia/person_data&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_person;</span><br></pre></td></tr></table></figure><p>按照指定的方式将数据输出到本地</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建存放数据的目录</span></span><br><span class="line">mkdir <span class="operator">-</span>p <span class="operator">/</span>usr<span class="operator">/</span><span class="keyword">local</span><span class="operator">/</span>soft<span class="operator">/</span>shujia</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 导出查询结果的数据</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/usr/local/soft/shujia/person&#x27;</span> </span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span> </span><br><span class="line">collection items terminated <span class="keyword">by</span> <span class="string">&#x27;-&#x27;</span> </span><br><span class="line">map keys terminated <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span> </span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span> </span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_person;</span><br></pre></td></tr></table></figure><p>将查询结果输出到HDFS</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建存放数据的目录</span></span><br><span class="line">hdfs dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p <span class="operator">/</span>shujia<span class="operator">/</span>bigdata17<span class="operator">/</span><span class="keyword">copy</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 导出查询结果的数据</span></span><br><span class="line"><span class="keyword">insert</span> overwrite directory <span class="string">&#x27;/shujia/bigdata17/user&#x27;</span> </span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span> </span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_user;</span><br></pre></td></tr></table></figure><p>直接使用HDFS命令保存表对应的文件夹</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">// 创建存放数据的目录</span><br><span class="line">hdfs dfs -mkdir -p /shujia/bigdata17/person</span><br><span class="line"></span><br><span class="line">// 使用HDFS命令拷贝文件到其他目录</span><br><span class="line">hdfs dfs -cp /hive/warehouse/t_person/*  /shujia/bigdata17/person</span><br></pre></td></tr></table></figure><p>将表结构和数据同时备份将数据导出到HDFS</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span>创建存放数据的目录</span><br><span class="line">hdfs dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p <span class="operator">/</span>shujia<span class="operator">/</span>bigdata17<span class="operator">/</span><span class="keyword">copy</span></span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>导出查询结果的数据</span><br><span class="line">export <span class="keyword">table</span> t_person <span class="keyword">to</span> <span class="string">&#x27;/shujia/bigdata17/copy&#x27;</span>;</span><br></pre></td></tr></table></figure><p>删除表结构</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> t_person;</span><br></pre></td></tr></table></figure><p>恢复表结构和数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">import <span class="keyword">from</span> <span class="string">&#x27;/shujia/bigdata17&#x27;</span>;</span><br></pre></td></tr></table></figure><blockquote><p>注意：时间不同步，会导致导入导出失败</p></blockquote><p>hive表查询时使用中文别名</p><p>在hive查询时 使用英文别名是没有任何问题的，</p><p>SELECT st.source_task_order A, st.creation_date B FROM tr_source_task st;</p><p>但是有某些特殊需求，需要使用中文别名时</p><p><strong>解决方法：</strong></p><p>将中文别名用<strong>反单引号</strong>（ tab键上面的那个键可以敲出来）引起来即可。</p><p>SELECT unit_name as <code>单位名称</code> FROM table_company_task;</p>]]></content>
    
    
    <summary type="html">对学习Hive的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hive" scheme="http://example.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>数据仓库概述</title>
    <link href="http://example.com/2022/05/31/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A6%82%E8%BF%B0/"/>
    <id>http://example.com/2022/05/31/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A6%82%E8%BF%B0/</id>
    <published>2022-05-30T16:00:00.000Z</published>
    <updated>2022-06-01T13:14:06.733Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h2><h3 id="一、数据库、数据仓库概述"><a href="#一、数据库、数据仓库概述" class="headerlink" title="一、数据库、数据仓库概述"></a>一、数据库、数据仓库概述</h3><blockquote><p>如今，随着诸如互联网以及物联网等技术的不断发展，越来越多的数据被生产出来。据统计，每天大约有超过2.5亿亿字节的各种各样数据产生。这些数据需要被存储起来并且能够被方便的分析和利用。</p><p>随着大数据技术的不断更新和迭代，数据管理工具得到了飞速的发展，相关概念如雨后春笋一般应运而生，如从最初决策支持系统(DSS)到商业智能(BI)、数据仓库、数据湖、数据中台等，这些概念特别容易混淆，本文对这些名词术语及内涵进行系统的解析，便于读者对数据平台相关的概念有全面的认识。</p></blockquote><h3 id="1-1-数据库"><a href="#1-1-数据库" class="headerlink" title="1.1 数据库"></a>1.1 数据库</h3><blockquote><p>关系数据库本质上是一个二元关系，说的简单一些，就是一个二维表格，对普通人来说，最简单的理解就是一个Excel表格。这种数据库类型，具有结构化程度高，独立性强，冗余度低等等优点，一下子就促进了计算机的发展。</p></blockquote><h3 id="1-2-操作型数据库和分析型数据库"><a href="#1-2-操作型数据库和分析型数据库" class="headerlink" title="1.2    操作型数据库和分析型数据库"></a>1.2    操作型数据库和分析型数据库</h3><p>随着关系数据库理论的提出，诞生了一系列经典的RDBMS，如Oracle，MySQL，SQL Server等。这些RDBMS被成功推向市场，并为社会信息化的发展做出的重大贡献。然而随着数据库使用范围的不断扩大，它被逐步划分为两大基本类型：</p><h4 id="操作型数据库"><a href="#操作型数据库" class="headerlink" title="操作型数据库"></a>操作型数据库</h4><blockquote><p>主要用于业务支撑。一个公司往往会使用并维护若干个操作型数据库，这些数据库保存着公司的日常操作数据，比如商品购买、酒店预订、学生成绩录入等；</p></blockquote><h4 id="分析型数据库"><a href="#分析型数据库" class="headerlink" title="分析型数据库"></a>分析型数据库</h4><blockquote><p>主要用于历史数据分析。这类数据库作为公司的单独数据存储，负责利用历史数据对公司各主题域进行统计分析；</p><p>那么为什么要”分家”？在一起不合适吗？能不能构建一个同样适用于操作和分析的统一数据库？答案是NO。一个显然的原因是它们会”打架”…如果操作型任务和分析型任务抢资源怎么办呢？再者，它们有太多不同，以致于早已”貌合神离”。</p></blockquote><h3 id="1-3-操作型数据库-VS-分析型数据库"><a href="#1-3-操作型数据库-VS-分析型数据库" class="headerlink" title="1.3    操作型数据库 VS 分析型数据库"></a>1.3    操作型数据库 VS 分析型数据库</h3><p>因为主导功能的不同(面向操作&#x2F;面向分析)，两类数据库就产生了很多细节上的差异。这就好像同样是人，但一个和尚和一个穆斯林肯定有很多行为&#x2F;观念上的不同。</p><p>接下来本文将详细分析两类数据库的不同点：</p><h4 id="1-数据组成差别-数据时间范围差别"><a href="#1-数据组成差别-数据时间范围差别" class="headerlink" title="1) 数据组成差别 - 数据时间范围差别"></a>1) 数据组成差别 - 数据时间范围差别</h4><blockquote><p>一般来讲，操作型数据库只会存放90天以内的数据，而分析型数据库存放的则是数年内的数据。这点也是将操作型数据和分析型数据进行物理分离的主要原因。</p></blockquote><h4 id="2-数据组成差别-数据细节层次差别"><a href="#2-数据组成差别-数据细节层次差别" class="headerlink" title="2) 数据组成差别 - 数据细节层次差别"></a>2) 数据组成差别 - 数据细节层次差别</h4><blockquote><p>操作型数据库存放的主要是细节数据，而分析型数据库中虽然既有细节数据，又有汇总数据，但对于用户来说，重点关注的是汇总数据部分。</p><p>操作型数据库中自然也有汇总需求，但汇总数据本身不存储而只存储其生成公式。这是因为操作型数据是动态变化的，因此汇总数据会在每次查询时动态生成。</p><p>而对于分析型数据库来说，因为汇总数据比较稳定不会发生改变，而且其计算量也比较大(因为时间跨度大)，因此它的汇总数据可考虑事先计算好，以避免重复计算。</p></blockquote><h4 id="3-数据组成差别-数据时间表示差别"><a href="#3-数据组成差别-数据时间表示差别" class="headerlink" title="3) 数据组成差别 - 数据时间表示差别"></a>3) 数据组成差别 - 数据时间表示差别</h4><blockquote><p>操作型数据通常反映的是现实世界的当前状态；而分析型数据库既有当前状态，还有过去各时刻的快照，分析型数据库的使用者可以综合所有快照对各个历史阶段进行统计分析。</p></blockquote><h4 id="4-技术差别-查询数据总量和查询频度差别"><a href="#4-技术差别-查询数据总量和查询频度差别" class="headerlink" title="4) 技术差别 - 查询数据总量和查询频度差别"></a>4) 技术差别 - 查询数据总量和查询频度差别</h4><blockquote><p>操作型查询的数据量少而频率多，分析型查询则反过来，数据量大而频率少。要想同时实现这两种情况的配置优化是不可能的，这也是将两类数据库物理分隔的原因之一。</p></blockquote><h4 id="5-技术差别-数据更新差别"><a href="#5-技术差别-数据更新差别" class="headerlink" title="5) 技术差别 - 数据更新差别"></a>5) 技术差别 - 数据更新差别</h4><blockquote><p>操作型数据库允许用户进行增，删，改，查；分析型数据库用户则只能进行查询。</p></blockquote><h4 id="6-技术差别-数据冗余差别"><a href="#6-技术差别-数据冗余差别" class="headerlink" title="6) 技术差别 - 数据冗余差别"></a>6) 技术差别 - 数据冗余差别</h4><blockquote><p>数据的意义是什么？就是减少数据冗余，避免更新异常。而如5所述，分析型数据库中没有更新操作。因此，减少数据冗余也就没那么重要了。</p><p>现在回到开篇是提到的第二个问题”某大公司Hadoop Hive里的关系表不完全满足完整&#x2F;参照性约束，也不完全满足范式要求，甚至第一范式都不满足。这种情况正常吗？”，答曰是正常的。因为Hive是一种数据仓库，而数据仓库和分析型数据库的关系非常紧密(后文会讲到)。它只提供查询接口，不提供更新接口，这就使得消除冗余的诸多措施不需要被特别严格地执行了。</p></blockquote><h4 id="7-功能差别-数据读者差别"><a href="#7-功能差别-数据读者差别" class="headerlink" title="7) 功能差别 - 数据读者差别"></a>7) 功能差别 - 数据读者差别</h4><blockquote><p>操作型数据库的使用者是业务环境内的各个角色，如用户，商家，进货商等；分析型数据库则只被少量用户用来做综合性决策。</p></blockquote><h4 id="8-功能差别-数据定位差别"><a href="#8-功能差别-数据定位差别" class="headerlink" title="8) 功能差别 - 数据定位差别"></a>8) 功能差别 - 数据定位差别</h4><blockquote><p>这里说的定位，主要是指以何种目的组织起来。操作型数据库是为了支撑具体业务的，因此也被称为”面向应用型数据库”；分析型数据库则是针对各特定业务主题域的分析任务创建的，因此也被称为”面向主题型数据库”。</p></blockquote><h3 id="2-1-数据仓库概述"><a href="#2-1-数据仓库概述" class="headerlink" title="2.1    数据仓库概述"></a>2.1    数据仓库概述</h3><p><strong>数据仓库之父比尔·恩门，1991年提出</strong></p><blockquote><p>数据仓库就是为了解决数据库不能解决的问题而提出的。那么数据库无法解决什么样的问题呢？这个我们得先说说什么是OLAP和OLTP。**(重点)**</p></blockquote><h3 id="2-2-OLTP和OLAP（重点）"><a href="#2-2-OLTP和OLAP（重点）" class="headerlink" title="2.2 OLTP和OLAP（重点）"></a>2.2 OLTP和OLAP（重点）</h3><h4 id="2-2-1-OLTP"><a href="#2-2-1-OLTP" class="headerlink" title="2.2.1 OLTP"></a>2.2.1 OLTP</h4><blockquote><p>OLTP（OnLine Transaction Processing 联机事务处理） 。简单一些，就是数据库的增删查改。举个例子，你到银行，去取一笔钱出来，或者转账，或者只是想查一下你还有多少存款，这些都是面向“事务”类型的操作。这样的操作有几个显著的特点:</p><p>首先要求速度很快， 基本上都是高可靠的在线操作（比如银行）， 还有这些操作涉及的数据内容不会特别大（否则速度也就相应的降低）， 最后，“事务”型的操作往往都要求是精准操作，比如你去银行取款，必须要求一个具体的数字，你是不可能对着柜台员工说我大概想取400到500快之间吧，那样人家会一脸懵逼。</p></blockquote><h4 id="2-2-2-OLAP"><a href="#2-2-2-OLAP" class="headerlink" title="2.2.2 OLAP"></a>2.2.2 OLAP</h4><blockquote><p>这个东西又是上面发明关系型数据库的科德发明的。OLAP略有复杂，但这里我举一个简单的例子，大家就很容易理解了。</p><p>比如说，沃尔玛超市的数据库里有很多张表格，记录着各个商品的交易记录。超市里销售一种运动饮料，我们不妨称之为红牛。数据库中有一张表A，记录了红牛在一年的各个月份的销售额；还有一张表B，记录了红牛每个月在美国各个州的销售额：；甚至还有一张表C，记录了这家饮料公司在每个州对红牛饮料的宣传资金投入；甚至后来沃尔玛又从国家气象局拿到了美国各个州的一年365天每天的天气表D。好，最后问题来了，请根据以上数据分析红牛在宣传资金不超过三百万的情况下，什么季节，什么天气，美国哪个州最好卖？凭借我们的经验，可能会得出，夏季的晴天，在美国的佛罗里达，最好卖，而且宣传资金投入越高销售额应该也会高。可能这样的结论是正确的，但决策者想要看到的是确凿的数据结论，而不是“可能”这样的字眼。</p><p>科学是不相信直觉的，如果我们人工进行手动分析，会发现这个要考虑的维度实在太多了，根本无法下手，何况这才四五个维度，要是更多了怎么办？OLAP就是为了解决这样的问题诞生的，但糟糕的是，传统数据库是无法满足OLAP所需要的数据信息的。</p></blockquote><h3 id="2-3-数据仓库概念"><a href="#2-3-数据仓库概念" class="headerlink" title="2.3    数据仓库概念"></a>2.3    数据仓库概念</h3><h4 id="2-3-1-概述"><a href="#2-3-1-概述" class="headerlink" title="2.3.1 概述"></a>2.3.1 概述</h4><p>数据库的大规模应用，使得信息行业的数据爆炸式的增长，为了研究数据之间的关系，挖掘数据隐藏的价值，人们越来越多的需要使用OLAP来为决策者进行分析，探究一些深层次的关系和信息。但很显然，不同的数据库之间根本做不到数据共享，就算同一家数据库公司，数据库之间的集成也存在非常大的挑战（最主要的问题是庞大的数据如何有效合并、存储）。</p><p>1988年，为解决企业的数据集成问题，IBM的两位研究员（Barry Devlin和Paul Murphy）创造性地提出了一个新的术语：数据仓库（Data Warehouse）。但只是将这个名词作为市场宣传的花哨概念，并没有在技术领域有什么实质性的研究和突破。</p><p>然而，尽管IBM不为所动，其他企业却在加紧对数据仓库的研究和开发，大家都想在这个领域寻找到第一桶金。终于，到了1992年，后来被誉为“数据仓库之父”的比尔 恩门（Bill Inmon）给出了数据仓库的定义，二十多年后的今天他的定义依然没有被时代淘汰。我们来看看他是怎么定义的：<strong>数据仓库是一个面向主题的、集成的、相对稳定的、反映历史变化的数据集合，用于支持管理中的决策制定。</strong></p><p>对于数据仓库的概念我们可以从两个层次予以理解：</p><p>首先,数据仓库用于支持决策,面向分析型数据处理,它不同于企业现有的操作型数据库; 其次,数据仓库是对多个异构的数据源有效集成,集成后按照主题进行了重组,并包含历史数据,而且存放在数据仓库中的数据一般不再修改。</p><p>我们可以不用管这个定义，简单的理解，其实就是我们为了进行OLAP，把分布在各个散落独立的数据库孤岛整合在了一个数据结构里面，称之为数据仓库。</p><p>这个数据仓库在技术上是怎么建立的读者朋友们并不需要关心，但是我们要知道，原来各个数据孤岛中的数据，可能会在物理位置（比如沃尔玛在各个州可能都有自己的数据中心）、存储格式（比如月份是数值类型，但但天气可能是字符类型）、商业平台（不同数据库可能用的是Oracle数据库，有的是微软SQL Server数据库）、编写的语言（Java或者Scale等）等等各个方面完全不同，数据仓库要做的工作就是将他们按照所需要的格式提取出来，再进行必要的转换（统一数据格式）、清洗（去掉无效或者不需要的数据）等，最后装载进数据仓库（我们所说的ETL工具就是用来干这个的）。这样，拿我们上面红牛的例子来说，所有的信息就统一放在了数据仓库中了。</p><p>自从数据仓库出现之后，信息产业就开始从以关系型数据库为基础的运营式系统慢慢向决策支持系统发展。这个决策支持系统，其实就是我们现在说的商务智能（Business Intelligence）即BI。</p><p>可以这么说，数据仓库为OLAP解决了数据来源问题，数据仓库和OLAP互相促进发展，进一步驱动了商务智能的成熟，但真正将商务智能赋予“智能”的，正是我们现在热谈的下一代技术：数据挖掘。</p><h4 id="2-3-2-数据仓库特点-重点"><a href="#2-3-2-数据仓库特点-重点" class="headerlink" title="2.3.2 数据仓库特点(重点)"></a>2.3.2 数据仓库特点(<strong>重点</strong>)</h4><h5 id="面向主题"><a href="#面向主题" class="headerlink" title="面向主题"></a><strong>面向主题</strong></h5><blockquote><p>面向主题特性是数据仓库和操作型数据库的根本区别。</p><p>操作型数据库是为了支撑各种业务而建立。</p><p>而分析型数据库则是为了对从各种繁杂业务中抽象出来的分析主题(如用户、成本、商品等)进行分析而建立；所谓主题：是指用户使用数据仓库进行决策时所关心的重点方面，如：收入、客户、销售渠道等；所谓面向主题，是指数据仓库内的信息是按主题进行组织的，而不是像业务支撑系统那样是按照业务功能进行组织的。</p></blockquote><h5 id="集成性"><a href="#集成性" class="headerlink" title="集成性"></a>集成性</h5><blockquote><p>集成性是指数据仓库会将不同源数据库中的数据汇总到一起；</p><p>具体来说，是指数据仓库中的信息不是从各个业务系统中简单抽取出来的，而是经过一系列加工、整理和汇总的过程，因此数据仓库中的信息是关于整个企业的一致的全局信息。</p></blockquote><h5 id="企业范围"><a href="#企业范围" class="headerlink" title="企业范围"></a>企业范围</h5><blockquote><p>数据仓库内的数据是面向公司全局的。比如某个主题域为成本，则全公司和成本有关的信息都会被汇集进来；</p></blockquote><h5 id="历史性"><a href="#历史性" class="headerlink" title="历史性"></a>历史性</h5><blockquote><p>较之操作型数据库，数据仓库的时间跨度通常比较长。前者通常保存几个月，后者可能几年甚至几十年；</p></blockquote><h5 id="时变性"><a href="#时变性" class="headerlink" title="时变性"></a>时变性</h5><blockquote><p>时变性是指数据仓库包含来自其时间范围不同时间段的数据快照。有了这些数据快照以后，用户便可将其汇总，生成各历史阶段的数据分析报告；</p><p>数据仓库内的信息并不只是反映企业当前的状态，而是记录了从过去某一时点到当前各个阶段的信息。通过这些信息，可以对企业的发展历程和未来趋势做出定量分析和预测。</p></blockquote><h3 id="2-4-数据仓库的趋势"><a href="#2-4-数据仓库的趋势" class="headerlink" title="2.4    数据仓库的趋势"></a>2.4    <strong>数据仓库的趋势</strong></h3><ul><li>实时数据仓库以满足实时化&amp;自动化决策需求；</li><li>大数据&amp;数据湖以支持大量&amp;复杂数据类型（文本、图像、视频、音频）；</li></ul><p><img src="https://s2.loli.net/2022/05/31/MVoEpGJ98mltv26.png" alt="image-20220530231142943.png"></p><h3 id="2-5-数据仓库的发展"><a href="#2-5-数据仓库的发展" class="headerlink" title="2.5    数据仓库的发展"></a>2.5    <strong>数据仓库的发展</strong></h3><blockquote><p>数据仓库有两个环节：<strong>数据仓库的构建</strong>与<strong>数据仓库的应用</strong>。</p><p>早期数据仓库构建主要指的是把企业的业务数据库如ERP、CRM、SCM等数据按照决策分析的要求建模并汇总到数据仓库引擎中，其应用以报表为主，目的是支持管理层和业务人员决策（中长期策略型决策）。</p><p>随着业务和环境的发展，这两方面都在发生着剧烈变化。</p><ul><li>随着IT技术走向互联网、移动化，数据源变得越来越丰富，在原来业务数据库的基础上出现了非结构化数据，比如网站log，IoT设备数据，APP埋点数据等，这些数据量比以往结构化的数据大了几个量级，对ETL过程、存储都提出了更高的要求；</li><li>互联网的在线特性也将业务需求推向了实时化，随时根据当前客户行为而调整策略变得越来越常见，比如大促过程中库存管理，运营管理等（即既有中远期策略型，也有短期操作型）；同时公司业务互联网化之后导致同时服务的客户剧增，有些情况人工难以完全处理，这就需要机器自动决策。比如欺诈检测和用户审核。</li></ul></blockquote><p><img src="https://s2.loli.net/2022/05/31/xkLpAqZB4QbJTdo.png" alt="image-20220530231227848.png"></p><p>总结来看，对数据仓库的需求可以抽象成两方面：<strong>实时产生结果、处理和保存大量异构数据</strong>。</p><h3 id="2-6-数据仓库建设方法论"><a href="#2-6-数据仓库建设方法论" class="headerlink" title="2.6    数据仓库建设方法论"></a>2.6    <strong>数据仓库建设方法论</strong></h3><blockquote><p><strong>1）面向主题</strong></p><p>从公司业务出发，是分析的宏观领域，比如供应商主题、商品主题、客户主题和仓库主题</p><p><strong>2）为多维数据分析服务</strong></p><p>数据报表；数据立方体，上卷、下钻、切片、旋转等分析功能。</p><p><strong>3）反范式数据模型</strong></p><p>以事实表和维度表组成的星型数据模型</p></blockquote><p><img src="https://s2.loli.net/2022/05/31/GZpvP38h4fKtlzk.png" alt="image-20220530231437440.png"></p><blockquote><p>数据仓库层的划分：</p></blockquote><p><img src="https://s2.loli.net/2022/06/01/E9Cn7JUyhBX31ck.png" alt="image-20220601194719608"></p>]]></content>
    
    
    <summary type="html">对学习数据仓库的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="数据仓库" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop的一些优化</title>
    <link href="http://example.com/2022/05/27/Hadoop%E6%A1%88%E4%BE%8B%E5%92%8C%E4%BC%98%E5%8C%96/"/>
    <id>http://example.com/2022/05/27/Hadoop%E6%A1%88%E4%BE%8B%E5%92%8C%E4%BC%98%E5%8C%96/</id>
    <published>2022-05-26T16:00:00.000Z</published>
    <updated>2022-05-31T15:55:02.889Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hadoop优化"><a href="#Hadoop优化" class="headerlink" title="Hadoop优化"></a>Hadoop优化</h1><h2 id="优化1：Combiner"><a href="#优化1：Combiner" class="headerlink" title="优化1：Combiner"></a>优化1：Combiner</h2><blockquote><p>减少了reduce 从map拉取数据的过程，提高计算效率。</p><p>hadoop 的计算特点：<strong>将计算任务向数据靠拢，而不是将数据向计算靠拢。</strong></p><p>特点：数据本地化，减少网络io。</p><p>首先需要知道，hadoop数据本地化是指的map任务，reduce任务并不具备数据本地化特征。<br>   通常输入的数据首先在<strong>逻辑上</strong>（<strong>注意这里不是真正物理上划分</strong>）将会分片split，每个分片上构建一个map任务，由该任务执行执行用户自定义的map函数，从而处理分片中的每条记录。<br>   那么切片的大小一般是趋向一个HDFS的block块的大小。为什么最佳的分片大小是趋向block块的大小呢？是因为这样能够确保单节点上最大输入块的大小，如果分片跨越两个数据块，没有一个block能够同时存储这两块数据，因此需要通过网络传输将部分数据传输到map任务节点上。这样明显比使用本地数据的map效率更低。<br>    注意，map任务执行后的结果并没有写到HDFS中，而是作为中间结果存储到本地硬盘，那为什么没有存储到HDFS呢？因为，该中间结果会被reduce处理后产生最终结果后，该中间数据会被删除，如果存储到HDFS中，他会进行备份，这样明显没有意义。如果map将中间结果传输到reduce过程中出现了错误，Hadoop会在另一个节点上重新执行map产生中间结果。<br>    那么为什么reduce没有数据本地化的特点呢？对于单个reduce任务来说，他的输入通常是所有mapper经过排序输出，这些输出通过网络传输到reduce节点，数据在reduce节点合并然后由reduce函数进行处理。最终结果输出到HDFS上。当多个有reduce任务的时候，map会针对输出进行分区partition，也就是为每个reduce构建一个分区，分区是由用户指定的partition函数，效率很高。<br>   同时为了高效传输可以指定combiner函数，他的作用就是，<strong>减少网络传输和本地传输</strong></p><p>假设文件是500mb</p><p>long bytesRemaining &#x3D; length; 500mb</p><p>​     while (((double) bytesRemaining)&#x2F;splitSize &gt; SPLIT_SLOP) {</p><p>​      int blkIndex &#x3D; getBlockIndex(blkLocations, length-bytesRemaining  );</p><p>​      splits.add(makeSplit(path, length-bytesRemaining 256 , splitSize 128,</p><p>​            blkLocations[blkIndex].getHosts(),</p><p>​            blkLocations[blkIndex].getCachedHosts()));</p><p>​      bytesRemaining &#x3D; bytesRemaining-splitSize;116</p><p>​     }</p><p><strong>注意：将reduce端的聚合操作，放到map 进行执行。适合求和，计数，等一些等幂操作。不适合求平均值，次幂等类似操作</strong></p></blockquote><h2 id="优化2：Join（数据倾斜）"><a href="#优化2：Join（数据倾斜）" class="headerlink" title="优化2：Join（数据倾斜）"></a>优化2：Join（数据倾斜）</h2><blockquote><p>MapReduce中的join</p><p>　　其实就是类似于关系型数据库中的连接查询一样。需要计算的数据可能存储在不同的文件中或不同表中，两个文件又有一些相同的字段可以相互关联，这时候我们就可以通过这些关联字段将两个文件中的数据组合到一起进行计算了。</p><p>　　我知道的mr有三种join方式。Map join、SemiJoin、reduce join。</p><p>Reduce Join（我们之前做的代码连接就是这个方式）</p><p>思路：</p><p>　　分为两个阶段</p><p>　　 （1）map函数主要是对不同文件中的数据打标签。</p><p>　　（2）reduce函数获取key相同的value list，进行笛卡尔积。</p><p>Map Join思路：</p><p>　　比如有两个表，有一个表非常大，而另一个表非常小，以至于小表可以直接存放到内存中。这样，我们可以将小表复制多份，让每个map task内存中保存一个hash map，将小表数据放入这个hash map中，key是小表与大表的内个连接字段，value是小表一条记录，然后只扫描大表：对于大表中的每一条记录key&#x2F;value，在hash map中查找是否有相同的key的记录，如果有，则连接输出即可。</p><p><strong>Semi Join 这个SemiJoin其实就是对reduce join的一种优化。</strong></p><p>　　就是在map端过滤掉不参加join操作的数据，则可以大大减少数据量，提高网络传输速度。</p><p>这三种join方式适用于不同的场景：</p><p>　　Reduce join要考虑数据量过大时的网络传输问题。</p><p>　　Map join和SemiJoin则要考虑数据量过大时的内存问题。 如果只考虑网络传输，忽略内存问题则。</p><p>　　Map join效率最高，其次是SemiJoin，最低的是reduce join。</p><p>DistributedCache DistributedCache是Hadoop提供的文件缓存工具，它能够自动将指定的文件分发到各个节点上，缓存到本地，供用户程序读取使用。一般用户数据字典的分发，和map join使用。一般缓存的文件都是只读。</p></blockquote><h2 id="优化3：根据实际情况调整切片大小"><a href="#优化3：根据实际情况调整切片大小" class="headerlink" title="优化3：根据实际情况调整切片大小"></a>优化3：根据实际情况调整切片大小</h2><blockquote><p><strong>为什么默认切片是128MB和blk大小一致？（优化）</strong></p><p>1 切片大小默认一致，是为了数据本地化，减少数据拉取消耗网络io</p><p>2 并不是越大越好，也不是越小越好。根据集群的资源情况而定。</p><p> 当集群计算资源充足的情况下：将切片的大小调小，增加map数量，提高读取效率。</p><p> 当集群计算资源紧张的情况下：将切片的大小调大，减少资源占用，让任务正常运转。</p><p> mapred.min.split.size、mapred.max.split.size、blockSize</p></blockquote><h2 id="优化4：可以设置yarn资源和队列。"><a href="#优化4：可以设置yarn资源和队列。" class="headerlink" title="优化4：可以设置yarn资源和队列。"></a>优化4：可以设置yarn资源和队列。</h2><blockquote><p>调整计算资源：<a href="https://blog.csdn.net/qq_36753550/article/details/83065546">https://blog.csdn.net/qq_36753550/article/details/83065546</a></p><p> 设置队列：<a href="https://blog.csdn.net/weixin_30607029/article/details/96507281">https://blog.csdn.net/weixin_30607029/article/details/96507281</a></p><p>mr运行日志信息：百分比是按照完成的m或r的任务的个数&#x2F;m或r的总个数。</p><p>MRv1&#x2F;MRv2&#x2F;YARN MRv1:</p><p>　　对于经典的MRv1它由三部分组成 :</p><p>　　　　编程模型、 数据处理引擎和运行时环境。</p><p>　　　　编程模型由新旧 API 两部分组成，新旧api只是代码封装上略有变化，性能没变化。</p><p>　　　　数据处理引擎由 MapTask 和 ReduceTask 组成。 运行时环境由 JobTracker 和 TaskTracker 两类服务组成。</p><p>　　MRv2:</p><p>　　　　由于MRv1对JobTracker的功能过多造成负载过重在扩展性、 资源利用率和多框架支持等方面存在不足，因此MRv2框架 的基本设计思想是将MRv1中的JobTracker包含的资源管理和应用管理两部分功能进行拆分，分别交给两个进程实现。 资源管理进程与具体应用程序无关，它负责整个集群的资源管理（内存、 CPU、 磁盘）。 应用管理进程负责管理应用程序，并且每个应用管理进程只管理一个作业。 由于资源管理可以共享给其他框架使用，因此MRv2将其做成了一个通用的系统YARN,YARN系统使得MRv2计算框架在可扩展性，资源利用率，多框架支持方面得到了很大改进。</p><p>　　YARN：yarn由4部分组成。</p><p>　　　　1. ResourceManager主要功能是：</p><p>　　　　　　（1）接收用户请求</p><p>　　　　　　（2）管理调度资源</p><p>　　　　　　（3）启动管理am　　　　</p><p>　　　　　　（4）管理所有nm,处理nm的状态汇报，向nm下达命令。</p><p>　2.Container：yarn的应用都是运行在容器上的，容器包含cpu，内存等信息。</p><p>　3.NodeManager：NM是每个节点上的资源和任务管理器，它会定时地向RM汇报本节点上的资源使用情况和各个容器的运行状态；同时负责对容器的启动和停止。</p><p>　　　　4. ApplicationMaster：管理应用程序。向RM获取资源、为应用程序分配任务、 监控所有任务运行状态。</p><ol><li>作业提交</li></ol><p>　　首先我们将任务提交给JobClient,JobClient会向RM获取一个appId。 然后我们的JobClient会对作业进行处理, 切分InputSplit, 将作业的Jar包, 配置文件和拷贝InputSplit信息拷贝到HDFS。 最后, 通过调用RM的submitApplication()来提交作业。</p><ol start="2"><li>作业初始化</li></ol><p>　　当RM收到submitApplciation()的请求时, 就将该请求发给调度器, 调度器分配第一个容器, 然后RM在该容器内启动ApplicationMaster进程。该进程上运行着一个MRAppMaster的Java应用。其通过创造一些bookkeeping对象来监控作业的进度。 然后通过hdfs得到由JobClient已经处理好的作业信息。为每个Inputsplit创建一个map任务, 并创建相应的reduce任务。然后ApplicationMaster会对整个作业量进行判断，<strong>如果作业量很小, ApplicationMaster会选择在其自己的JVM中运行任务</strong>, <strong>这种作业称作是uber task的方式</strong>。在任务运行之前, 作业的<strong>setup</strong>方法被调用来创建输出路径。</p><ol start="3"><li>任务分配</li></ol><p>　　如果不是小作业, 那么ApplicationMaster向RM请求更多的容器来运行所有的map和reduce任务，<strong>每个容器只能对应一个任务</strong>。这些请求是通过心跳来传输的, 包括每个map任务的数据位置, 比如Inputsplit的主机名和机架。调度器利用这些信息来调度任务, 尽量将任务分配给有存储数据的节点, 或者分配给和存放Inputsplit的节点相同机架的节点。</p><ol start="4"><li>任务运行</li></ol><p>　　当一个任务由RM的调度器分配了一个容器后, ApplicationMaster与NM通信来启动容器。任务由一个为YarnChild的Java应用执行。在运行任务之前首先本地化任务需要的资源, 比如作业配置, JAR文件, 以及hdfs中保存的任务所需的所有文件。最后, map任务或者reduce运行在一个叫YarnChild的进程当中。</p><ol start="5"><li>进度和状态更新</li></ol><p>　　每个NM会向applicationmaster汇报自己的工作状态，JobClient会每秒轮询检测applicationmaster，这样就能随时收到更新信息。</p><ol start="6"><li>作业完成</li></ol><p>　　除了向applicationmaster请求作业进度外, JobClient每5分钟都会通过调用waitForCompletion()来检查作业是否完成。作业完成之后,applicationmaster和NM会清理工作状态, OutputCommiter的作业清理方法也会被调用. 作业的信息会被作业历史服务器存储以备之后用户核查.</p><p><strong>yarn对异常task的处理（推测执行）？(重要！！！)</strong></p><p>　　推测执行是在分布式环境下，因为某种原因造成同一个job的多个task运行速度不一致，有的task运行速度明显慢于其他task，则这些task拖慢了整个job的执行进度，为了避免这种情况发生，Hadoop会为该task启动备份任务，让该speculative task与原始task同时处理一份数据，哪个先运行完，则将谁的结果作为最终结果。推测执行优化机制采用了<strong>典型的以空间换时间的优化策略</strong>，它同时启动多个相同task（备份任务）处理相同的数据块，哪个完成的早，则采用哪个task的结果，这样可防止拖后腿Task任务出现，进而提高作业计算速度，但是，这样却会占用更多的资源。</p><p><strong>yarn调度器的策略？(重要！！！)</strong></p><p>　　yarn默认是计算能力调度 FifoScheduler:根据先进先出排队，最简单的调度器。  FIFO</p><p>​        CapacityScheduler(计算能力调度)、FairScheduler(公平调度)：</p><p>　　相同点：</p><p>　　　　(1)都是多队列。</p><p>　　　　(2)都有资源最大最小上线限制。</p><p>　　　　(3)都是资源共享，每个队列剩余的资源可以给其他队列使用。</p><p>　　不同点：</p><p>　　　　(1)队列排序算法不同：计算能力调度资源使用量小的优先。公平调度根据公平排序算法排序。</p><p>　　　　(2)应该用选择算法不同：计算能力调度是先进先出。公平调度先进先出或者公平排序算法。</p><p>　　　　(3)资源抢占：公平调度如果当前队列有新应用提交后，会把共享出去的资源抢夺回来。</p></blockquote>]]></content>
    
    
    <summary type="html">对学习Hadoop的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://example.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-MapReduce</title>
    <link href="http://example.com/2022/05/26/Hadoop-MapReduce/"/>
    <id>http://example.com/2022/05/26/Hadoop-MapReduce/</id>
    <published>2022-05-25T16:00:00.000Z</published>
    <updated>2022-05-26T14:22:54.930Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、MapReduce设计理念"><a href="#一、MapReduce设计理念" class="headerlink" title="一、MapReduce设计理念"></a>一、MapReduce设计理念</h3><blockquote><p>map—&gt;映射</p><p>reduce—&gt;归纳</p><p>mapreduce必须构建在hdfs之上的一种大数据离线计算框架</p><p>​        在线：实时数据处理</p><p>​        离线：数据处理时效性没有在线那么强，但是相对也需要很快得到结果</p><p>mapreduce不会马上得到结果，他会有一定的延时（磁盘IO）</p><p>​        如果数据量小，使用mapreduce反而不合适</p><p>​        杀鸡焉用宰牛刀</p><p>原始数据–&gt;map(Key,Value)–&gt;Reduce</p><p>分布式i计算</p><p>​        将大的数据切分成多个小数据，交给更多的节点参与运算</p><p>计算向数据靠拢</p><p>​        将计算传递给有数据的节点上进行工作</p></blockquote><h3 id="二、MapReduce架构特点"><a href="#二、MapReduce架构特点" class="headerlink" title="二、MapReduce架构特点"></a>二、MapReduce架构特点</h3><h4 id="MapReduce1-x"><a href="#MapReduce1-x" class="headerlink" title="MapReduce1.x"></a>MapReduce1.x</h4><blockquote><p><strong>JobTracker</strong></p><p>　　　主节点，单点，负责调度所有的作用和监控整个集群的资源负载。</p><p><strong>TaskTracker</strong></p><p>　　　从节点，自身节点资源管理和JobTracker进行心跳联系，汇报资源和获取task。</p><p><strong>Client</strong></p><p>　　　以作业为单位，规划作业计算分布，提交作业资源到HDFS，最终提交作业到JobTracker。</p><h5 id="MapReduce1-x的弊端"><a href="#MapReduce1-x的弊端" class="headerlink" title="MapReduce1.x的弊端"></a>MapReduce1.x的弊端</h5><p>　　1.JobTracker负载过重，存在单点故障。</p><p>　　2.资源管理和计算调度强耦合，其它计算框架难以复用其资源管理。</p><p>　　3.不同框架对资源不能全局管理。</p></blockquote><h4 id="MapReduce2-x"><a href="#MapReduce2-x" class="headerlink" title="MapReduce2.x"></a>MapReduce2.x</h4><blockquote><p>ResourceManager</p><p>　　　主节点，负责整个集群的资源管理。</p><p>NodeManager</p><p>　　　与ResourceManager汇报资源，管理Container生命周期，计算框架中的角色都以Container表示。</p><p>Container</p><p>　　　默认NodeManager启动线程监控Container大小，超出申请资源额度会kill掉。支持Linux内核的Cgroup。</p><p>Client</p><p>　　　ResourceManager-client：请求资源创建ApplicationMaster-client。</p><p>　　　ApplicationMaster-client：与ApplicationMaster交互。</p><p><strong>YARN【Yet Another Resource Negotiator】：Hadoop 2.0新引入的资源管理系统，直接从MRv1演化而来的。</strong></p><p><strong>核心思想：将MRv1中JobTracker的资源管理和任务调度两个功能分开，分别由ResourceManager和ApplicationMaster进程实现：</strong></p><p>　　　ResourceManager：负责整个集群的资源管理和调度。</p><p>　　　ApplicationMaster：负责应用程序相关的事务，比如任务调度、任务监控和容错等。</p><p>YARN的引入，使得多个计算框架可运行在一个集群中 每个应用程序对应一个ApplicationMaster 目前多个计算框架可以运行在YARN上，比如MapReduce、Spark、Storm等。</p></blockquote><h3 id="三、扑克牌的问题"><a href="#三、扑克牌的问题" class="headerlink" title="三、扑克牌的问题"></a>三、扑克牌的问题</h3><p><strong>你想数出一摞牌中有多少张黑桃，红桃，方块，梅花。直观方式是一张一张检查并且数出分别有多少张。</strong><br><strong>MapReduce方法则是：</strong><br>        1.给在座的所有玩家中分配这摞牌<br>        2.让每个玩家数自己手中的牌有几张是黑桃，然后把这个数目汇报给你<br>        3.你把所有玩家告诉你的数字加起来，得到最后的结论</p><h3 id="四、MR的计算流程"><a href="#四、MR的计算流程" class="headerlink" title="四、MR的计算流程"></a>四、MR的计算流程</h3><h4 id="4-1-原始数据File-可以从网上找一篇英文的文章"><a href="#4-1-原始数据File-可以从网上找一篇英文的文章" class="headerlink" title="4.1    原始数据File(可以从网上找一篇英文的文章)"></a>4.1    原始数据File(可以从网上找一篇英文的文章)</h4><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">The books chronicle the adventures of the adolescent wizard Harry Potter and his best friends Ron Weasley and Hermione Granger, all of whom are students at Hogwarts School of Witchcraft and Wizardry. </span><br></pre></td></tr></table></figure><blockquote><p>1T数据被切分成块存放在HDFS上，每一个块有128M大小</p></blockquote><h4 id="4-2-数据块Block"><a href="#4-2-数据块Block" class="headerlink" title="4.2    数据块Block"></a>4.2    数据块Block</h4><blockquote><p>block块是hdfs上存储的一个单元，同一个文件块的大小都是相同</p><p>因为数据存储到HDFS上不可变，所以有可能快的数量和集群的计算能力不匹配</p><p>我们需要一个动态调整本次参与计算节点数量的单位</p><p>我们可以动态的改变这个单位—&gt;参与的节点</p></blockquote><h4 id="5-3-切片Split"><a href="#5-3-切片Split" class="headerlink" title="5.3    切片Split"></a>5.3    切片Split</h4><blockquote><p>目的：动态地控制计算单元的数量 </p></blockquote><blockquote><p>切片是个逻辑概念</p><p>在不改变现有数据存储的情况下，可以控制参与计算的节点数目</p><p>通过切片大小可以达到控制计算节点数量的目的</p></blockquote><p><strong>有多少切片就会有多少个Map任务</strong></p><blockquote><p>一般切片大小为Block的整数倍（2    1&#x2F;2）</p><p>防止多余创建和很多的数据连接</p><p>如果Split大小 &gt; Block大小，计算节点少了  </p><p>如果Split大小 &lt; Block大小，计算节点多了</p><p>默认情况下，Split切片的大小等于Block的大小，默认128M，如果读取到最后一个block块的时候，与前一个block块组合起来大小小于128*1.1M的话，它们会结合生成一个Split切片，生成一个map任务</p><p>一个切片对应一个MapTask</p></blockquote><h4 id="4-4-MapTask"><a href="#4-4-MapTask" class="headerlink" title="4.4    MapTask"></a>4.4    MapTask</h4><blockquote><p>map默认从所属切片读取数据，每次读取一行（默认读取器）到内存中（map中的逻辑作用在每一行上）我们可以根据自己书写的分词逻辑（空格，逗号等分隔符），计算每个单词出现的次数 （wordcount），这时会产生（map &lt;String，Integer&gt;）临时数据，存放带内存中</p></blockquote><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">the books chronicle the adventures of the adolescent wizard Harry Potter and his best friends Ron Weasley and Hermione Granger, all of whom are students at Hogwarts School of Witchcraft and Wizardry</span><br><span class="line"></span><br><span class="line">the 1</span><br><span class="line">books 1</span><br><span class="line">chronicle 1</span><br><span class="line">the 1</span><br><span class="line">adventures 1</span><br><span class="line">of 1</span><br><span class="line">...</span><br><span class="line">Wizardry 1</span><br></pre></td></tr></table></figure><blockquote><p>但是内存的大小是有限的，如果每个任务随机的去占用内存，会导致内存不可控。多个任务同时执行有可能内存溢出（OOM）</p><p>如果把数据都直接放到硬盘上，效率低</p><p>考虑到把内存和硬盘结合，可以先往内存中写入一部分数据，然后写到硬盘上</p></blockquote><h4 id="4-5-环形缓冲区（KV-Buffer）"><a href="#4-5-环形缓冲区（KV-Buffer）" class="headerlink" title="4.5    环形缓冲区（KV-Buffer）"></a>4.5    环形缓冲区（KV-Buffer）</h4><blockquote><p>可以循环利用这块内存区域，减少数据溢写时map的停止时间</p><p>每一个Map可以独享的一个内存区域</p><p>在内存中构建一个环形缓冲区（kv-Buffer），默认大小为100M</p><p>设置缓冲区的阈值为80%（设置阈值的目的是为了同时写入和写出），当缓冲区的数据达到80M开始溢写到硬盘</p><p>溢写的时候有20M的空间可以被使用并且使用效率不会被减缓，不用担心出现OOM问题</p></blockquote><h4 id="4-6-分区Partition（环形缓冲区做的）"><a href="#4-6-分区Partition（环形缓冲区做的）" class="headerlink" title="4.6    分区Partition（环形缓冲区做的）"></a>4.6    分区Partition（环形缓冲区做的）</h4><blockquote><p>根据Key直接计算出对应的Reduce</p><p>分区的数量和reduce的数量是相等的</p><p>hash(key) % partition(reduce的数量) &#x3D; num</p><p>默认分区的算法是Hash然后取余</p><p>Object的hashCode()—equals()</p><p>如果两个对象equals，那么两个对象的hashcode一定相等</p><p>如果两个对象的hashcode相等，但是对象不一定equals</p></blockquote><h4 id="4-7-排序Sort（环形缓冲区做的，快速排序，对前面分区后的编号进行排序，目的是使得相同编号在一起）"><a href="#4-7-排序Sort（环形缓冲区做的，快速排序，对前面分区后的编号进行排序，目的是使得相同编号在一起）" class="headerlink" title="4.7    排序Sort（环形缓冲区做的，快速排序，对前面分区后的编号进行排序，目的是使得相同编号在一起）"></a>4.7    排序Sort（环形缓冲区做的，快速排序，对前面分区后的编号进行排序，目的是使得相同编号在一起）</h4><blockquote><p>对要溢写的数据进行排序（QuickSort）</p><p>按照先Partition后Key的顺序排序–&gt;相同分区在一起，相同Key的在一起</p><p>将来溢写出来的小文件也是有序的</p></blockquote><h4 id="4-8-溢写Spill"><a href="#4-8-溢写Spill" class="headerlink" title="4.8    溢写Spill"></a>4.8    溢写Spill</h4><blockquote><p>将内存中的数据循环写到硬盘上，无需担心OOM问题</p><p>每次会产生一个80M的文件</p><p>如果本次Map产生的数据较多，可能会溢写多个文件</p></blockquote><h4 id="4-9-合并Merge"><a href="#4-9-合并Merge" class="headerlink" title="4.9    合并Merge"></a>4.9    合并Merge</h4><blockquote><p>因为溢写会产生很多有序（分区 key）的小文件，而且小文件的数目也不确定</p><p>后面向reduce传递数据带来很大问题</p><p>所以将小文件合并成一个大文件，将来来取得数据直接从大文件拉去即可 </p><p>合并小文件的时候同样进行排序（<strong>归并排序</strong>），最终产生一个有序的大文件</p></blockquote><h4 id="4-10-组合器Combiner"><a href="#4-10-组合器Combiner" class="headerlink" title="4.10 组合器Combiner"></a>4.10 组合器Combiner</h4><blockquote><p>a.    集群的带宽限制了mapreduce作业的数量 ，因此应该尽量避免map和reduce任务之间的数据传输，hadoop允许用户对map的输出数据进行处理，用户可自定义combiner函数（如同map函数和reduce函数一般），其逻辑一般和reduce函数一样，combiner的输入是map的输出，combiner的输出作为reduce的输入，很多情况下可以直接将reduce函数作为combiner函数来试用</p><p>（job.setCombinerClass(FlowCountReducer.class)）</p><p>b.    combiner属于优化方案，所以无法确定combiner函数会调用多少次，可以在环形缓冲区溢出文件时调用combiner函数，也可以在溢出的小文件合并成大文件时调用combiner，但是要保证不管调用多少次，combiner函数都不影响最终结果，所以不是所有处理逻辑都可以使用combiner组件，有些逻辑如果使用了combiner函数会影响最后reduce的输出结果（如求几个数的平均值，就不能先用combiner求一次各个map输出结果的平均值，再求这些平均值的平均值，那样会导致结果错误）</p><p>c.  combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量：</p><p>​        原先传给reduce的数据时a1 a1 a1 a1 a1</p><p>​        第一次combiner组合后变成a(1,1,1,1,1)</p><p>​        第二次combiner后传给reduce的数据变为a(5,5,6,7,23,…)</p></blockquote><h4 id="4-11-拉取Fetch"><a href="#4-11-拉取Fetch" class="headerlink" title="4.11    拉取Fetch"></a>4.11    拉取Fetch</h4><blockquote><p>我们需要将Map的临时结果拉取到Reduce节点</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;第一种方式：两两合并</span><br><span class="line">&gt;第二种方式：相同的进一个reduce</span><br><span class="line">&gt;第三种对第二种优化，排序</span><br><span class="line">&gt;第四种对第三种优化：如果一个reduce处理两种key，而key分布一个首一个尾，解决不连续的问题，给个编号，这个编号怎么算呢，`回到分区，排序`</span><br></pre></td></tr></table></figure><p>第一种方式：两两合并<br>第二种方式：相同的进一个reduce<br>第三种对第二种优化，排序<br>第四种对第三种优化：如果一个reduce处理两种key，而key分布一个首一个尾，解决不连续的问题，给个编号，这个编号怎么算呢，<code>回到分区，排序</code></p></blockquote><h4 id="4-12-合并Merge"><a href="#4-12-合并Merge" class="headerlink" title="4.12    合并Merge"></a>4.12    合并Merge</h4><blockquote><p>因为reduce拉取的时候，会从多个map拉取数据</p><p>那么每个map都会产生一个小文件,这些小文件（文件与文件之间无序，文件内部有序）</p><p>为了方便计算（没必要读取N个小文件）,需要合并文件</p><p>归并算法合并成2个(qishishilia)</p><p>相同的key都在一起</p></blockquote><h4 id="4-13-归并Reduce"><a href="#4-13-归并Reduce" class="headerlink" title="4.13    归并Reduce"></a>4.13    归并Reduce</h4><blockquote><p>将文件中的数据读取到内存中</p><p>一次性将相同的key全部读取到内存中</p><p>直接将相同的key得到结果-&gt;最终结果 </p></blockquote><h4 id="4-14-写出Output"><a href="#4-14-写出Output" class="headerlink" title="4.14    写出Output"></a>4.14    写出Output</h4><blockquote><p>每个reduce将自己计算的最终结果都会存放到HDFS上</p></blockquote><p><img src="https://s2.loli.net/2022/05/26/QXb2lUNZsR7PB8g.png" alt="image-20220526222212188"></p>]]></content>
    
    
    <summary type="html">对学习Hadoop的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://example.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop高可用集群搭建（HA）</title>
    <link href="http://example.com/2022/05/25/Hadoop%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88HA%EF%BC%89/"/>
    <id>http://example.com/2022/05/25/Hadoop%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88HA%EF%BC%89/</id>
    <published>2022-05-24T16:00:00.000Z</published>
    <updated>2022-05-26T12:37:40.407Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1、zookeeper搭建"><a href="#1、zookeeper搭建" class="headerlink" title="1、zookeeper搭建"></a>1、zookeeper搭建</h3><p>1、上传安装包到master并解压<br>    tar -xvf zookeeper-3.4.6.tar.gz</p><p>2、配置环境变量<br>    vim &#x2F;etc&#x2F;profile</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export ZOOKEEPER_HOME=/usr/local/soft/zookeeper-3.4.6</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br><span class="line"></span><br><span class="line">别忘记source /etc/profile</span><br></pre></td></tr></table></figure><p>3、修改配置文件<br>    cd conf<br>    cp  zoo_sample.cfg zoo.cfg</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">修改</span><br><span class="line">dataDir=/usr/local/soft/zookeeper-3.4.6/data</span><br><span class="line"></span><br><span class="line">增加</span><br><span class="line">server.0=master:2888:3888</span><br><span class="line">server.1=node1:2888:3888</span><br><span class="line">server.2=node2:2888:3888</span><br></pre></td></tr></table></figure><p>4、同步到其它节点</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp -r zookeeper-3.4.6 node1:`pwd`</span><br><span class="line">scp -r zookeeper-3.4.6 node2:`pwd`</span><br><span class="line"></span><br><span class="line">配置node1和node2的环境变量</span><br><span class="line">scp /etc/profile node1:/etc/</span><br><span class="line">scp /etc/profile node2:/etc/</span><br><span class="line"></span><br><span class="line">在所有节点执行</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>5、创建&#x2F;usr&#x2F;local&#x2F;soft&#x2F;zookeeper-3.4.6&#x2F;data目录,所有节点都要创建</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /usr/local/soft/zookeeper-3.4.6/data</span><br><span class="line"></span><br><span class="line">在data目录下创建myid文件</span><br><span class="line">vim myid </span><br><span class="line">master,node1,node2分别加上0，1，2</span><br></pre></td></tr></table></figure><p>6、启动zk，</p><pre><code>zkServer.sh start  三台都需要执行zkServer.sh status 查看状态当有一个leader的时候启动成功</code></pre><p>连接zk</p><pre><code>zkCli.shzk  是一个目录结构 ，每个节点可以存数据，同时可以有子节点</code></pre><p>zk shell</p><pre><code>创建目录create /test testcreate /test/a 1获取数据get /test ls /testdelete 只能删除没有子节点的节点rmr /test  删除节点</code></pre><p><strong>关闭命令</strong></p><p>zkServer.sh stop</p><p><strong>拍摄快照</strong></p><p><strong>重置zk</strong><br>1、杀掉所有zk进程<br>kiil -9 pid</p><p>2、删除data目录下的version文件, 所有节点都要删除<br>rm -rf &#x2F;usr&#x2F;local&#x2F;soft&#x2F;zookeeper-3.4.6&#x2F;data&#x2F;version-2</p><p>2、启动zk<br>zkServer.sh start</p><h3 id="2、Hadoop-HA"><a href="#2、Hadoop-HA" class="headerlink" title="2、Hadoop-HA"></a>2、Hadoop-HA</h3><table><thead><tr><th></th><th>ZK</th><th>NN</th><th>DN</th><th>RN</th><th>NM</th><th>JN</th><th>ZKFC</th></tr></thead><tbody><tr><td>master</td><td>1</td><td>1</td><td></td><td>1</td><td></td><td>1</td><td>1</td></tr><tr><td>node1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td>node2</td><td>1</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td></tr></tbody></table><h4 id="防火墙、时间同步、免密配置操作不再赘述"><a href="#防火墙、时间同步、免密配置操作不再赘述" class="headerlink" title="防火墙、时间同步、免密配置操作不再赘述"></a>防火墙、时间同步、免密配置操作不再赘述</h4><p>1、修改hadoop配置文件</p><p><strong>core-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://cluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/soft/hadoop-2.7.6/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1440<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:2181,node1:2181,node2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>hdfs-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定hdfs元数据存储的路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/soft/hadoop-2.7.6/data/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定hdfs数据存储的路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/soft/hadoop-2.7.6/data/datanode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 数据备份的个数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 关闭权限验证 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启WebHDFS功能（基于REST的接口服务） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- //////////////以下为HDFS HA的配置////////////// --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定hdfs的nameservices名称为mycluster --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定cluster的两个namenode的名称分别为nn1,nn2 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.cluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置nn1,nn2的rpc通信端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.cluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.cluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置nn1,nn2的http通信端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.cluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.cluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定namenode元数据存储在journalnode中的路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://master:8485;node1:8485;node2:8485/cluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定journalnode日志文件存储的路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/soft/hadoop-2.7.6/data/journal<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定HDFS客户端连接active namenode的java类 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.cluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置隔离机制为ssh --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定秘钥的位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启自动故障转移 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>yarn-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- NodeManager上运行的附属服务，需配置成mapreduce_shuffle才可运行MapReduce程序 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置Web Application Proxy安全代理（防止yarn被攻击） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启日志 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置日志删除时间为7天，-1为禁用，单位为秒 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 修改日志目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.remote-app-log-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置nodemanager可用的资源内存 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置nodemanager可用的资源CPU --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- //////////////以下为YARN HA的配置////////////// --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启YARN HA --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 启用自动故障转移 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN HA的名称 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarncluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定两个resourcemanager的名称 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置rm1，rm2的主机 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>node1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置YARN的http端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置zookeeper的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:2181,node1:2181,node2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置zookeeper的存储位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-state-store.parent-path<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/rmstore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启yarn resourcemanager restart --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置resourcemanager的状态存储到zookeeper中 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启yarn nodemanager restart --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置nodemanager IPC的通信端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0:45454<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>mapred-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定MapReduce计算框架使用YARN --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定jobhistory server的rpc地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定jobhistory server的http地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启uber模式（针对小作业的优化） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置启动uber模式的最大map数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.maxmaps<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>9<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置启动uber模式的最大reduce数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.maxreduces<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>2、删除hadoop数据存储目录下的文件  每个节点都需要删除</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">rm</span> -rf /usr/local/soft/hadoop-2.7.6/tmp</span><br></pre></td></tr></table></figure><p>3、启动zookeeper<strong>三台都需要启动</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zkServer.sh start启动</span><br><span class="line">zkServer.sh status查看状态</span><br></pre></td></tr></table></figure><p>4、启动 JN 存储hdfs元数据</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop-daemon.sh start journalnode每个节点都要执行</span><br></pre></td></tr></table></figure><p> 5、格式化 在一台NN上执行</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format在master上执行</span><br></pre></td></tr></table></figure><p>6、启动当前的NN</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop-daemon.sh start namenode在master上执行</span><br></pre></td></tr></table></figure><p>7、执行同步 没有格式化的NN上执行  在另外一个namenode上面执行</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs namenode -bootstrapStandby在node1上执行</span><br></pre></td></tr></table></figure><p>8、格式化ZK   在已经启动的namenode上面执行<br>    <strong>！！一定要先 把zk集群正常 启动起来！！</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs zkfc -formatZKmaster上执行</span><br></pre></td></tr></table></figure><p>9、启动hdfs集群,在启动了namenode的节点上执行</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">start-dfs.shmaster上执行</span><br></pre></td></tr></table></figure><h3 id="3、Hadoop-HA遇到的问题"><a href="#3、Hadoop-HA遇到的问题" class="headerlink" title="3、Hadoop-HA遇到的问题"></a>3、Hadoop-HA遇到的问题</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dfs.ha.fencing.methods</span><br><span class="line">表示：a list of scripts or Java classes which will be used to fence the Active NameNode during a failover</span><br><span class="line"></span><br><span class="line">而配置为shell(true)就是直接返回隔离成功，即表示没进行任何操作，为什么不会导致脑裂现象的发生，这是因为Quorun Journal方式内置了fencing功能，不需要实现单独的fencing机制（epoch number解决互斥问题）。</span><br><span class="line">而如果使用共享存储NAS+NFS那种方式的话，就需要配置具体的真正有fencing功能的，比如：sshfence，下面是sshfence的说明：</span><br><span class="line"></span><br><span class="line">sshfence - SSH to the Active NameNode and kill the process</span><br><span class="line">The sshfence option SSHes to the target node and uses fuser to kill the process listening on the service’s TCP port. In order for this fencing option to work, it must be able to SSH to the target node without providing a passphrase. Thus, one must also configure the dfs.ha.fencing.ssh.private-key-files option, which is a comma-separated list of SSH private key files. 即配置sshfence需要两个namenode之间配置无密码认证，如下:(hdfs-site.xml)</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;sshfence&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">但如果只配置sshfence，如果在机器宕机后不可达，则sshfence会返回false，即fence失败，所以得要配置成：</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;</span><br><span class="line">            sshfence</span><br><span class="line">            shell(/bin/true)</span><br><span class="line">        &lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">这样子配置，顺序执行时，如果可达就执行sshfence执行杀死namenode后返回true，不可达就直接shell(true)返回true。</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">对学习Hadoop的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://example.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop进程相关</title>
    <link href="http://example.com/2022/05/24/Hadoop%E8%BF%9B%E7%A8%8B%E7%9B%B8%E5%85%B3/"/>
    <id>http://example.com/2022/05/24/Hadoop%E8%BF%9B%E7%A8%8B%E7%9B%B8%E5%85%B3/</id>
    <published>2022-05-23T16:00:00.000Z</published>
    <updated>2022-05-24T14:55:23.451Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1、进程理解"><a href="#1、进程理解" class="headerlink" title="1、进程理解"></a>1、进程理解</h3><h4 id="HDFS相关（NN、DN、SNN）"><a href="#HDFS相关（NN、DN、SNN）" class="headerlink" title="HDFS相关（NN、DN、SNN）"></a>HDFS相关（NN、DN、SNN）</h4><h5 id="NameNode（NN）"><a href="#NameNode（NN）" class="headerlink" title="NameNode（NN）"></a>NameNode（NN）</h5><p><img src="https://s2.loli.net/2022/05/24/OAdzRlIwaLnFTB7.png" alt="image-20220524205656034"></p><blockquote><p>功能：</p><p>​    1、接收客户端的读&#x2F;写服务    因为NN知道数据文件与DN的对应（映射）关系</p><p>​    2、保存文件的时候会保存文件的元数据信息</p><p>​            a、文件的归属</p><p>​            b、文件的权限</p><p>​            c、文件的大小 、时间</p><p>​            d、Block块的信息，但是Block块的位置信息不会持久化，需要每次开启集群的时候DN向NN汇报。</p><p>​    3、收集Block块的位置信息</p><p>​        3.1    系统启动</p><p>​            a、NN关机的时候不会存储任何的Block块与DN的映射信息</p><p>​            b、DN启动的时候会自动将自己节点上存储的Block块信息汇报给NN</p><p>​            c、NN接收请求之后会重新生成映射关系</p><p>​                        File –&gt;Block</p><p>​                        Block–&gt;DN</p><p>​            d、如果数据块的副本数小于设置数，NN会将整个副本拷贝到其他节点</p><p>​        3.2    集群运行中</p><p>​            a、NN与DN保持心跳机制，三秒钟发送一次</p><p>​            b、如果客户端需要读取或者上传数据的时候，NN可以知道DN的健康情况</p><p>​            c、可以让客户端读取存活的DN节点</p><p>​            d、如果NN与DN三秒没有心跳反馈，就会认为DN出现异常（掉线），此时不会让新的数据写到这个异常的DN中，客户端访问的时候不提供异常的DN节点地址</p><p>​            e、如果超过十分钟没有心跳，那么NN会认为它宕机，会将当前DN节点存储的数据转移到其他节点</p><p>​    4、NameNode为了效率，将所有操作都在内存中进行</p><p>​        a、执行速度快</p><p>​        b、NameNode不会和磁盘进行任何的数据交换</p><p>​        但是会存在两个问题：</p><p>​        1、数据的持久化</p><p>​        2、数据保存在内存中，断电会丢失</p></blockquote><h5 id="DataNode（DN）"><a href="#DataNode（DN）" class="headerlink" title="DataNode（DN）"></a>DataNode（DN）</h5><blockquote><p>1、存放的是文件的数据信息，以及验证文件完整性的校验信息 </p><p>2、数据会存放在硬盘上</p><p>​        a、1m&#x3D;1条数据</p><p>​        b、1G&#x3D;1条数据</p><p>​        c、NN非常排斥存储小文件（能存，但是不推荐）</p><p>​            一般小文件在存储之前需要进行压缩</p><p>3、汇报</p><p>​        1、启动时</p><p>​                汇报之前会先验证Block文件是否损坏</p><p>​                向NN汇报当前DN上Block的信息</p><p>​        2、运行时</p><p>​                向NN保持心跳机制</p><p>4、当客户端读写数据的时候，首先会先去查询file与block与DN的映射，然后客户端直接与DN建立连接，然后读写数据</p></blockquote><h5 id="SecondaryNameNode（SNN）"><a href="#SecondaryNameNode（SNN）" class="headerlink" title="SecondaryNameNode（SNN）"></a>SecondaryNameNode（SNN）</h5><blockquote><h5 id="1、传统的内存持久化方案"><a href="#1、传统的内存持久化方案" class="headerlink" title="1、传统的内存持久化方案"></a>1、传统的内存持久化方案</h5><p>​    1）日志机制</p><p>​            a、做任何操作之前先记录日志</p><p>​            b、在数据改变之前先记录对应的日志，当NN停止的时候</p><p>​            c、当我下次启动的时候，只需要重新按照以前的日志”重做一遍”即可</p><p>​            <strong>缺点：</strong></p><p>​                a、log日志文件的大小不可控，随着时间的变化，集群启动的时间也会越来越长</p><p>​                b、日志中会存在大量无效日志</p><p>​            <strong>优点：</strong></p><p>​                a、不会丢失数据</p><h5 id="2）拍摄快照"><a href="#2）拍摄快照" class="headerlink" title="2）拍摄快照"></a>2）拍摄快照</h5><p>​            a、将内存中的数据写到硬盘上（序列化）</p><p>​            b、启动时还可以将硬盘上的数据写回到内存中（反序列化）</p><p>​            <strong>缺点</strong></p><p>​                a、关机时间长</p><p>​                b、如果时异常关机，数据还在内存中，没法写入到硬盘</p><p>​                c、如果写出的频率过高，导致内存使用效率低</p><p>​            <strong>优点</strong></p><p>​                启动时间较短</p><h5 id="2、SNN的解决方案"><a href="#2、SNN的解决方案" class="headerlink" title="2、SNN的解决方案"></a>2、SNN的解决方案</h5><p>​    1）解决思路</p><p>​            a、让日志大小可控（每64M）</p><p>​            b、快照需要定时保存（每隔1h）</p><p>​            c、日志+快照</p><p>​    2）解决方案 </p><p>​            a、当我们启动一个集群的时候，会产生4个文件 …&#x2F;name&#x2F;current&#x2F;</p><p><img src="https://s2.loli.net/2022/05/24/Us7rSuhcvWiNzdJ.png" alt="image-20220524222400282"></p><p>​            b、我们每次操作都会记录日志–&gt;edits-inprogress- edits_00000001，随着时间的推移，日志文件会越来越大-当达到阈值的时候（64M或3600秒），会生成新的日志文件，edits_inprogress-000000001 –&gt;edits_0000001，创建新的日志文件 edits_inprogress-0000000016。</p><p><img src="https://s2.loli.net/2022/05/24/N5eijCIE2SUzJ7v.png" alt="image-20220524222453522"></p></blockquote><h3 id="2、安全模式"><a href="#2、安全模式" class="headerlink" title="2、安全模式"></a>2、安全模式</h3><blockquote><p>安全模式是 HDFS 的一种工作状态，处于安全模式的状态下，只向客户端提供文件的只读视图，不接受对命名空间的修改；同时 NameNode 节点也不会进行数据块的复制或者删除，<br><strong>NameNode 启动时，</strong><br>        1）首先将镜像文件（ fsimage ）载入内存，并执行编辑日志（ edits ）中的各项操作。<br>        2）一旦在内存中成功建立文件系统元数据的映射，则创建一个新的 fsimage 文件和一个空的编辑日志。<br>        3）NameNode 开始监听 RPC 和 Http 请求。<br>        4）此时 NameNode 处于<strong>安全模式</strong>，只接受客户端的读请求。</p><p>​        5）处于这个状态是为了保护数据的安全所以只能被客户端访问读取数据</p></blockquote><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="section"># 对安全模式的理解</span></span><br><span class="line"><span class="section"># 1.工作流程</span></span><br><span class="line"><span class="code">a.启动 NameNode，NameNode 加载 fsimage 到内存，对内存数据执行 edits log 日 志中的事务操作。</span></span><br><span class="line"><span class="code">b.文件系统元数据内存镜像加载完毕，进行 fsimage 和 edits log 日志的合并，并创 建新的 fsimage 文件和一个空的 edits log 日志文件。</span></span><br><span class="line"><span class="code">c.NameNode 等待 DataNode 上传 block 列表信息，直到副本数满足最小副本条件。</span></span><br><span class="line"><span class="code">d.当满足了最小副本条件，再过 30 秒，NameNode 就会退出安全模式。最小副本条件指 整个文件系统中有 99.9%的 block 达到了最小副本数（默认值是 1，可设置）</span></span><br><span class="line"><span class="code"># 在 NameNode 安全模式（safemode）</span></span><br><span class="line"><span class="code">对文件系统元数据进行只读操作</span></span><br><span class="line"><span class="code">当文件的所有 block 信息具备的情况下，对文件进行只读操作</span></span><br><span class="line"><span class="code">不允许进行文件修改（写，删除或重命名文件）</span></span><br><span class="line"><span class="code"># 2.注意事项</span></span><br><span class="line"><span class="code">a.NameNode 不会持久化 block 位置信息；DataNode 保有各自存储的 block 列表信息。 正常操作时，NameNode 在内存中有一个 blocks 位置的映射信息（所有文件的所有文 件块的位置映射信息）。</span></span><br><span class="line"><span class="code">b.NameNode 在安全模式，NameNode 需要给 DataNode 时间来上传 block 列表信息到 NameNode。如果 NameNode 不等待 DataNode 上传这些信息的话，则会在 DataNode 之间进行 block 的复制，而这在大多数情况下都是非必须的（因为只需要等待 DataNode 上传就行了），还会造成资源浪费。</span></span><br><span class="line"><span class="code">c.在安全模式 NameNode 不会要求 DataNode 复制或删除 block。</span></span><br><span class="line"><span class="code">d.新格式化的 HDFS 不进入安全模式，因为 DataNode 压根就没有 block。</span></span><br><span class="line"><span class="code"># 4.命令操作</span></span><br><span class="line"><span class="code"># 通过命令查看 namenode 是否处于安全模式：</span></span><br><span class="line"><span class="code">hdfs dfsadmin -safemode get</span></span><br><span class="line"><span class="code">Safe mode is ON HDFS 的前端 webUI 页面也可以查看 NameNode 是否处于安全模式。 有时候我们希望等待安全模式退出，之后进行文件的读写操作，尤其是在脚本中，此时：</span></span><br><span class="line"><span class="code">`hdfs dfsadmin -safemode wait`</span></span><br><span class="line"><span class="code"># your read or write command goes here 管理员有权在任何时间让 namenode 进入或退出安全模式。进入安全模式：</span></span><br><span class="line"><span class="code">`hdfs dfsadmin -safemode enter`</span></span><br><span class="line"><span class="code">Safe mode is ON 这 样 做 可 以 让 namenode 一 直 处 于 安 全 模 式 ， 也 可 以 设 置 `dfs.namenode.safemode.threshold-pct` 为 1 做到这一点。 离开安全模式：</span></span><br><span class="line"><span class="code">`hdfs dfsadmin -safemode leave`</span></span><br><span class="line"><span class="code">Safe mode is OFF</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>系统中的数据块的位置并不是由 NameNode 维护的，而是以块列表的形式存储在 DataNode 中。</strong><br>[root@node01 ~]# rm -rf &#x2F;var&#x2F;yjx&#x2F;hadoop&#x2F;full&#x2F;dfs&#x2F;name&#x2F;current&#x2F;*<br>[root@node01 ~]# scp -r<br>root@node02:&#x2F;var&#x2F;yjx&#x2F;hadoop&#x2F;full&#x2F;dfs&#x2F;namesecondary&#x2F;current&#x2F;*<br>&#x2F;var&#x2F;yjx&#x2F;hadoop&#x2F;full&#x2F;dfs&#x2F;name&#x2F;current </p><p><strong>安全模式下</strong><br>        a. 安全模式下，各个 DataNode 会向 NameNode 发送自身的数据块列表<br>        b. 当 NameNode 有足够的数据块信息后，便在 30 秒后退出安全模式<br>        c. NameNode 发现数据节点过少会启动数据块复制过程<br><strong>如果 NN 收集的 Block 信息没有达到最少副本数，就会将缺失的副本 , 从有的 DN 上拷贝到其他 DN</strong><br>        a. dfs.replication.min&#x3D;2<br>        b. 但是默认最低副本数为 1<br>        c. 在拷贝的过程中系统还是处于安全模式<br><strong>安全模式相关命令</strong><br>hadoop dfsadmin -safemode leave 强制 NameNode 退出安全模式<br>hadoop dfsadmin -safemode enter 进入安全模式<br>hadoop dfsadmin -safemode get 查看安全模式状态<br>hadoop dfsadmin -safemode wait 等待一直到安全模式结束</p></blockquote><h3 id="3、HDFS的权限"><a href="#3、HDFS的权限" class="headerlink" title="3、HDFS的权限"></a>3、HDFS的权限</h3><blockquote><p>HDFS对权限的控制</p><p>​        a. 只能防止好人做错事</p><p>​        b. 不能防止坏人做坏事</p><p><strong>但是告诉你是谁，他就认为你是谁！！</strong></p></blockquote><h3 id="4、机架感知"><a href="#4、机架感知" class="headerlink" title="4、机架感知"></a>4、机架感知</h3><blockquote><p>机架感知是为了保证副本在集群中的安全性<br>我们需要将节点放在不同的DN节点上，节点也需要一定的考量<br>     可靠性，可用性，带宽消耗<br>第一个节点：<br>     集群内部（优先考虑和客户端相同的节点作为第一个节点）<br>     集群外部（选择资源丰富且不繁忙的节点作为第一个节点）<br>第二个节点：<br>     第二个节点选择与第一个节点不同机架的其他节点<br>第三个节点：<br>     与第二个相同机架相同的其他节点<br>第N个节点：<br>     与前面节点不重复的其他节点</p></blockquote><h3 id="5、HDFS的读写流程（重点）"><a href="#5、HDFS的读写流程（重点）" class="headerlink" title="5、HDFS的读写流程（重点）"></a>5、HDFS的读写流程（重点）</h3><h4 id="写数据"><a href="#写数据" class="headerlink" title="写数据"></a>写数据</h4><blockquote><p> <strong>写数据就是将客户端上的数据上传到HDFS</strong></p><h4 id="宏观过程"><a href="#宏观过程" class="headerlink" title="宏观过程"></a>宏观过程</h4><p> <img src="https://s2.loli.net/2022/05/24/wTPqf3aR9eGsKv7.png" alt="image-20220524222751307"></p><p> <strong>1.客户端向HDFS发送写数据请求</strong></p><p>   hdfs dfs -put students.txt &#x2F;shujia&#x2F;</p><p> <strong>2. Filesystem通过rpc调用namenode的put方法</strong></p><p> a. nn首先检查是否有足够的空间权限等条件创建这个文件,或者这个路径是否已经存在，权限</p><p> b. 有：NN会针对这个文件创建一个空的Entry对象,并返回成功状态给DFS        </p><p> c. 没有：直接抛出对应的异常，给予客户端错误提示信息</p><p> <strong>3.如果DFS接收到成功的状态，会创建一个FSDataOutputStream的对象给客户端使用</strong></p><p> <strong>4.客户端要向nn询问第一个Block存放的位置</strong></p><p> ​    NN通过机架感知策略 (node1 node 2 node3)</p><p> <strong>5.需要将客户端和DN节点创建连接</strong></p><pre><code>pipeline(管道)客户端 和 node1 创建连接 socketnode1 和 node2 创建连接 socketnode2 和 Node3 创建连接 socket</code></pre><p> <strong>6.客户端按照文件块切分数据，但是按照packet发送数据</strong><br>    默认一个packet大小为64K,Block128M为2048个packet</p><p> <strong>7.客户端通过pipeline管道开始使用FDSOutputStream对象将数据输出</strong></p><pre><code>    1. 客户端首先将一个 packet 发送给 node1, 同时给予 node1 一个 ack 状态    2. node1接受数据后会将数据继续传递给 node2, 同时给予 node2 一个 ack 状态    3. node2接受数据后会将数据继续传递给 node3, 同时给予 node3 一个 ack 状态    4. node3将这个 packet 接受完成后，会响应这个 ack 给 node2 为 true    5. node2会响应给 node1 , 同理 node1 响应给客户端</code></pre><p> <strong>8.客户端接收到成功的状态 , 就认为某个 packet 发送成功了，直到当前块所有的 packet 都发送完成</strong></p><p> ​    1. 如果客户端接收到最后一个 pakcet 的成功状态 , 说明当前 block 传输完成，管道就会被撤销</p><p> ​    2. 客户端会将这个消息传递给 NN ， NN 确认传输完成</p><p> ​        1. NN会将 block 的信息记录到 Entry, 客户端会继续向 NN 询问第二个块的存储位置 , 依次类推</p><p> ​                block1 (node1 node2 node3)</p><p> ​                block2 (node1 node3 node6)</p><p> ​                ….</p><p> ​                blockn(node1 node4 node6)</p><pre><code> 3. 当所有的 block 传输完成后， NN 在 Entry 中存储所有的 File 与 Block 与 DN 的映射关系关闭FsDataOutPutStream</code></pre><h4 id="微观过程（如何保证package发送的时候不出错呢？）"><a href="#微观过程（如何保证package发送的时候不出错呢？）" class="headerlink" title="微观过程（如何保证package发送的时候不出错呢？）"></a>微观过程（如何保证package发送的时候不出错呢？）</h4><p> <strong>1.客户端首先从自己的硬盘中以流的形式将自己的数据读取到缓存中</strong><br> <strong>2.然后将缓存中的数据以chunk(512B)和checksum(4B)的方式放入到packet（64k)</strong></p> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. chunk:checksum=128:1</span><br><span class="line">2. checksum:在数据处理和数据通信领域中，用于校验目的的一组数据项的和</span><br><span class="line">3. Packet中的数据分为两类，一类是实际数据包，另一类是 header 包。</span><br><span class="line">4. 一个 Packet 数据包的组成结构（分两类，一类是实际的数据包，另一类是header包。）</span><br></pre></td></tr></table></figure><p> <strong>一个数据包的组成结构：</strong></p><p> <img src="https://s2.loli.net/2022/05/24/jfzr86gaTiWdvlD.png" alt="image-20220524225301985"></p><p> <strong>参数理解：</strong></p><p> <img src="https://s2.loli.net/2022/05/24/yp9kOTUjeMYHxEZ.png" alt="image-20220524225333906"></p><p> <strong>3.（默认生成的快，发送的慢）当packet满的时候添加到dataqueue</strong><br> <strong>4.datastreamer开始从dataqueue队列上读取一个packet,通过FDSDataOPS发送到Poepleline</strong><br>     在取出的时候，也会将 packet 加入到 ackQueue, 典型的生产者消费者模式</p><p> ​    客户端发送一个 Packet 数据包以后开始接收 ack ，会有一个用来接收 ack 的 ResponseProcessor 进<br> 程，如果收到成功的 ack </p> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 如果某一个 packet 的 ack 为 true, 那么就从 ackqueue 删除掉这个 packet</span><br><span class="line">2. 如果某一个 packet 的 ack 为 false, 将 ackqueue 中所有的 packet 重新挂载到 发送队列 , 重新发送</span><br></pre></td></tr></table></figure><p> <img src="https://s2.loli.net/2022/05/24/1W63lkyhUTdDBGg.png" alt="image-20220524225407656"></p><p> <strong>最终DFS保存的数据格式：</strong></p><p> <img src="https://s2.loli.net/2022/05/24/9lJKUgNxXzPv16Q.png" alt="image-20220524225430756"></p><p> <strong>读数据</strong></p><p> <img src="https://s2.loli.net/2022/05/24/C9qYBsOL6RjZyM4.png" alt="image-20220524225514935"></p><p> <strong>1.首先客户端发送请求到 DFS ，申请读取某一个文件</strong><br> <strong>2.DFS 去 NN 查找这个文件的信息 ( 权限 , 文件是否存在 )</strong><br>     如果文件不存在，抛出指定的错误<br>     如果文件存在，返回成功状态<br> <strong>3.DFS 创建 FSDataInputStream 对象，客户端通过这个对象读取数据</strong><br> <strong>4.客户端获取文件第一个 Block 信息 , 返回 DN1 DN2 DN8</strong><br> <strong>5.客户端直接就近原则选择 DN1 对应的数据即可</strong><br> <strong>6.依次类推读取其他块的信息，直到最后一个块 , 将 Block 合并成一个文件</strong><br> <strong>7.关闭 FSDataInputStream</strong></p></blockquote>]]></content>
    
    
    <summary type="html">对学习Hadoop的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://example.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-2.7.6-基础</title>
    <link href="http://example.com/2022/05/23/Hadoop-2.7.6-%E5%9F%BA%E7%A1%80/"/>
    <id>http://example.com/2022/05/23/Hadoop-2.7.6-%E5%9F%BA%E7%A1%80/</id>
    <published>2022-05-22T16:00:00.000Z</published>
    <updated>2022-05-24T11:45:23.600Z</updated>
    
    <content type="html"><![CDATA[<p>hadoop的<strong>特点</strong>：</p><p><strong>扩容能力</strong></p><p>扩容能力(Scalable)：能可靠(reliably)地存储和处理PB级别的数据。如果数据量更大，存储不下了,再增加节点就可以了。</p><p><strong>成本低</strong></p><p>成本低(Economical):可以通过普通机器组成的服务器集群来分发以及处理数据.这些服务器集群可达数千个节点。</p><p><strong>高效率</strong></p><p>高效率(Efficient):通过分发计算程序,hadoop可以在数据所在节点上(本地)并行地(parallel)处理他们,这使得处理非常的迅速</p><p><strong>可靠性</strong></p><p>可靠性(Reliable):hadoop能够自动地维护数据的多份副本,并且在任务失败后能够自动地重新部署(redeploy)计算任务</p><p>作者Doug Cutting 受Google三篇论文的启发，开发了hadoop</p><blockquote><p><strong>Google FS</strong></p><p><strong>MapReduce</strong></p><p><strong>BigTable</strong></p></blockquote><p>hadoop是一个统称，目前hadoop主要包含<strong>三大组件</strong></p><blockquote><p><strong>hdfs</strong>：是一个分布式存储框架，适合海量数据存储</p><p><strong>mapreduce</strong>：是一个分布式计算框架，适合海量数据计算</p><p><strong>yarn</strong>：是一个资源调度平台，负责给计算框架分配计算资源</p></blockquote><p>HDFS具有<strong>主从架构</strong>。HDFS集群由单个名称节点组成，主服务器管理文件系统名称空间并控制客户机对文件的访问。此外，还有许多数据节点，通常是集群中每个节点一个，它们管理连接到运行它们的节点的存储。</p><p><img src="https://s2.loli.net/2022/05/17/SODawkZY6AnX4RL.png" alt="image-20220517200532901"></p><p>hadoop的三种启动（停止）方式</p><p>第一种：全部启动集群所有进程</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">启动：sbin/start-all.sh</span><br><span class="line">停止：sbin/stop-all.sh</span><br></pre></td></tr></table></figure><p>第二种：单独启动hdfs【web端口50070】和【web端口8088】的相关进程</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">启动：sbin/start-dfs.sh sbin/start-yarn.sh</span><br><span class="line">停止：sbin/stop-dfs.sh  sbin/stop-yarn.sh</span><br><span class="line">**每次重新启动集群的时候使用**</span><br></pre></td></tr></table></figure><p>第三种：单独启动某一个进程</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">启动hdfs：sbin/hadoop-daemon.sh start (namenode | datanode)</span><br><span class="line">停止hdfs：sbin/hadoop-daemon.sh stop (namenode | datanode)</span><br><span class="line">启动yarn：sbin/hadoop-daemon.sh start (resourcemanager | nodemanager)</span><br><span class="line">停止yarn：sbin/hadoop-daemon.sh stop (resourcemanager | nodemanager)</span><br><span class="line">**用于当某个进程启动失败或者down掉的时候，重启进程**</span><br></pre></td></tr></table></figure><p><strong>hdfs shell</strong></p><p>调用文件系统(FS)Shell命令应使用 bin&#x2F;hdfs dfs -xxx 的形式。</p><p>所有的FS shell命令使用URI路径作为参数。</p><p>URI格式是scheme:&#x2F;&#x2F;authority&#x2F;path。HDFS的scheme是hdfs，对本地文件系统，scheme是file。其中scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。</p><p>例如：&#x2F;parent&#x2F;child可以表示成hdfs:&#x2F;&#x2F;namenode:namenodePort&#x2F;parent&#x2F;child，或者更简单的&#x2F;parent&#x2F;child（假设配置文件是namenode:namenodePort）</p><p>大多数FS Shell命令的行为和对应的Linux Shell命令类似。</p><p>常用操作</p><p>-ls            查看hdfs上目录，如hdfs dfs -ls &#x2F;</p><p>-put         将本地文件上传到hdfs，如hdfs dfs -put 本地文件路径 hdfs路径</p><p>-get         将hdfs文件下载到本地，如hdfs dfs -get hdfs的文件路径 本地文件路径</p><p>-mkdir    在hdfs上创建文件夹，如hdfs dfs -mkdir &#x2F;test</p><p>-cp          将hdfs文件或目录复制，如hdfs dfs -cp &#x2F;test.txt &#x2F;a&#x2F;</p><p>-cat         查看hdfs上文件内容，如hdfs dfs -cat &#x2F;test.txt</p><p><strong>运行word count实例</strong></p><p>hadoop jar  &#x2F;usr&#x2F;local&#x2F;soft&#x2F;hadoop-2.7.6&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.7.6.jar wordcount  inputpath outputpath</p><p>运行结果：</p><p><img src="https://s2.loli.net/2022/05/17/ix2tcnZwKjelkMo.png" alt="image-20220517211619031"></p>]]></content>
    
    
    <summary type="html">对学习Hadoop的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://example.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop集群搭建（完全分布式）</title>
    <link href="http://example.com/2022/05/22/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%EF%BC%89/"/>
    <id>http://example.com/2022/05/22/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%EF%BC%89/</id>
    <published>2022-05-21T16:00:00.000Z</published>
    <updated>2022-05-24T12:03:01.365Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、准备工作"><a href="#一、准备工作" class="headerlink" title="一、准备工作"></a>一、准备工作</h3><ul><li><p>三台虚拟机：master、node1、node2</p></li><li><p>时间同步</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ntpdate ntp.aliyun.com</span><br></pre></td></tr></table></figure></li><li><p>调整时区</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp  /usr/share/zoneinfo/Asia/Shanghai  /etc/localtime</span><br></pre></td></tr></table></figure></li><li><p>jdk1.8</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></li><li><p>修改主机名</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">三台分别执行 vim /etc/hostname 并将内容指定为对应的主机名</span><br></pre></td></tr></table></figure></li><li><p>关闭防火墙：systemctl stop firewalld   </p><ul><li>查看防火墙状态：systemctl status firewalld </li><li>取消防火墙自启：systemctl disable firewalld</li></ul></li><li><p>静态IP配置（两种方式）</p><ul><li><p>1、直接使用图形化界面配置</p></li><li><p>2、手动编辑配置文件进行配置</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、编辑网络配置文件</span><br><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br><span class="line"></span><br><span class="line">TYPE=Ethernet</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">HWADDR=00:0C:29:E2:B8:F2</span><br><span class="line">NAME=ens33</span><br><span class="line">DEVICE=ens33</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.168.190.100</span><br><span class="line">GATEWAY=192.168.190.2</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">DNS1=192.168.190.2</span><br><span class="line">DNS2=223.6.6.6</span><br><span class="line"></span><br><span class="line">需要修改：HWADDR（mac地址,centos7不需要手动指定mac地址）</span><br><span class="line">IPADDR（根据自己的网段，自定义IP地址）</span><br><span class="line">GATEWAY（根据自己的网段填写对应的网关地址）</span><br><span class="line"></span><br><span class="line">2、关闭NetworkManager，并取消开机自启</span><br><span class="line">systemctl stop NetworkManager</span><br><span class="line">systemctl disable NetworkManager</span><br><span class="line"></span><br><span class="line">3、重启网络服务</span><br><span class="line">systemctl restart network</span><br></pre></td></tr></table></figure></li></ul></li><li><p>免密登录</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1、生成密钥</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line"># 2、配置免密登录</span><br><span class="line">ssh-copy-id master</span><br><span class="line">ssh-copy-id node1</span><br><span class="line">ssh-copy-id node2</span><br><span class="line"># 3、测试免密登录</span><br><span class="line">ssh node1</span><br></pre></td></tr></table></figure></li><li><p>配置好映射文件：&#x2F;etc&#x2F;hosts</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">192.168.151.81 master</span><br><span class="line">192.168.151.82 node1</span><br><span class="line">192.168.151.83 node2</span><br></pre></td></tr></table></figure></li></ul><h3 id="二、搭建Hadoop集群"><a href="#二、搭建Hadoop集群" class="headerlink" title="二、搭建Hadoop集群"></a>二、搭建Hadoop集群</h3><blockquote><p>NameNode：接受客户端的读&#x2F;写服务,收集 DataNode 汇报的 Block 列表信息</p><p>DataNode：真实数据存储的地方（block）</p><p>SecondaryNameNode：做持久化的时候用到</p></blockquote><table><thead><tr><th>进程</th><th>master（主）</th><th>node1（从）</th><th>node2（从）</th></tr></thead><tbody><tr><td>NameNode</td><td>√</td><td></td><td></td></tr><tr><td>SecondaryNameNode</td><td>√</td><td></td><td></td></tr><tr><td>ResourceManager</td><td>√</td><td></td><td></td></tr><tr><td>DataNode</td><td></td><td>√</td><td>√</td></tr><tr><td>NodeManager</td><td></td><td>√</td><td>√</td></tr></tbody></table><h3 id="2-1-完全分布式搭建"><a href="#2-1-完全分布式搭建" class="headerlink" title="2.1    完全分布式搭建"></a>2.1    完全分布式搭建</h3><h4 id="1、上传安装包并解压"><a href="#1、上传安装包并解压" class="headerlink" title="1、上传安装包并解压"></a>1、上传安装包并解压</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 使用xftp上传压缩包至master的/usr/local/soft/packages/</span><br><span class="line">cd /urs/local/soft/packages/</span><br><span class="line"># 解压</span><br><span class="line">tar -zxvf hadoop-2.7.6.tar.gz -C /usr/local/soft/</span><br></pre></td></tr></table></figure><h4 id="2、配置环境变量"><a href="#2、配置环境变量" class="headerlink" title="2、配置环境变量"></a>2、配置环境变量</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">JAVA_HOME=/usr/local/soft/jdk1.8.0_171</span><br><span class="line">HADOOP_HOME=/usr/local/soft/hadoop-2.7.6</span><br><span class="line">export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</span><br><span class="line"></span><br><span class="line"># 重新加载环境变量</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h4 id="3、修改Hadoop配置文件"><a href="#3、修改Hadoop配置文件" class="headerlink" title="3、修改Hadoop配置文件"></a>3、修改Hadoop配置文件</h4><ul><li><p><code>cd /usr/local/soft/hadoop-2.7.6/etc/hadoop/</code></p></li><li><p>core-site.xml</p><blockquote><p>fs.defaultFS： 默认文件系统的名称。其方案和权限决定文件系统实现的URI。uri的方案确定命名文件系统实现类的配置属性（fs.scheme.impl）。uri的权限用于确定文件系统的主机、端口等。</p><p>hadoop.tmp.dir：是 hadoop文件系统依赖的基本配置，很多配置路径都依赖它，它的默认位置是在 &#x2F;tmp&#x2F;{$user}下面，注意这是个临时目录！！！</p><p>因此，它的持久化配置很重要的！ 如果选择默认，一旦因为断电等外在因素影响，&#x2F;tmp&#x2F;{$user}下的所有东西都会丢失。</p><p>fs.trash.interval：启用垃圾箱配置，dfs命令删除的文件不会立即从HDFS中删除。相反，HDFS将其移动到垃圾目录（每个用户在<code>/user/&lt;username&gt;/.Trash</code>下都有自己的垃圾目录）。只要文件保留在垃圾箱中，文件可以快速恢复。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://master:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/usr/local/soft/hadoop-2.7.6/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1440&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li><li><p>hadoop-env.sh</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/soft/jdk1.8.0_171</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/05/24/ABd2KfenPMkySoT.png" alt="image.png"></p></li><li><p>hdfs-site.xml</p></li><li><blockquote><p>dfs.replication：每个datanode上只能存放一个副本。我这里就2个datanode</p><p>dfs.permissions：如果为“true”，则在HDFS中启用权限检查。如果为“false”，则关闭权限检查，但所有其他行为保持不变。从一个参数值切换到另一个参数值不会更改文件或目录的模式、所有者或组。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.permissions&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li><li><p>mapred-site.xml.template</p></li><li><blockquote><p>mapreduce.framework.name：用于执行MapReduce作业的运行时框架。</p><p>mapreduce.jobhistory.address：Hadoop自带了一个历史服务器，可以通过历史服务器查看已经运行完的Mapreduce作业记录，比如用了多少个Map、用了多少个Reduce、作业提交时间、作业启动时间、作业完成时间等信息。默认情况下，Hadoop历史服务器是没有启动的，我们可以通过*mr-<strong>jobhistory-daemon.sh start historyserver</strong>命令来启动Hadoop历史服务器。我们可以通过Hadoop jar的命令来实现我们的程序jar包的运行，关于运行的日志，我们一般都需要通过启动一个服务来进行查看，就是我们的JobHistoryServer，我们可以启动一个进程，专门用于查看我们的任务提交的日志。mapreduce.jobhistory.address和mapreduce.jobhistory.webapp.address默认的值分别是0.0.0.0:10020和0.0.0.0:19888</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1、重命名文件</span><br><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line"># 2、修改</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;  </span><br><span class="line">        &lt;value&gt;master:10020&lt;/value&gt;  </span><br><span class="line">    &lt;/property&gt;  </span><br><span class="line"></span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;  </span><br><span class="line">        &lt;value&gt;master:19888&lt;/value&gt;  </span><br><span class="line">    &lt;/property&gt; </span><br></pre></td></tr></table></figure></li><li><p>slaves</p></li><li><blockquote><p>从节点的信息</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">node1</span><br><span class="line">node2</span><br></pre></td></tr></table></figure></li><li><p>yarn-site.xml</p></li><li><blockquote><p>yarn.resourcemanager.hostname：指定yarn主节点</p></blockquote></li></ul><blockquote><p>yarn.nodemanager.aux-services：NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序。默认值：“”</p><p>yarn.log-aggregation-enable：yarn日志聚合功能开关</p><p>yarn.log-aggregation.retain-seconds：日志保留时限，默认7天</p></blockquote>  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;master&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h4 id="4、分发Hadoop到node1、node2"><a href="#4、分发Hadoop到node1、node2" class="headerlink" title="4、分发Hadoop到node1、node2"></a>4、分发Hadoop到node1、node2</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /usr/local/soft/</span><br><span class="line">scp -r hadoop-2.7.6/ node1:`pwd`</span><br><span class="line">scp -r hadoop-2.7.6/ node2:`pwd`</span><br></pre></td></tr></table></figure><h4 id="5、格式化namenode（第一次启动的时候需要执行，以及每次修改核心配置文件后都需要）"><a href="#5、格式化namenode（第一次启动的时候需要执行，以及每次修改核心配置文件后都需要）" class="headerlink" title="5、格式化namenode（第一次启动的时候需要执行，以及每次修改核心配置文件后都需要）"></a>5、格式化namenode（第一次启动的时候需要执行，以及每次修改核心配置文件后都需要）</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/05/24/tQ9ZOuvozXcPIB2.png" alt="image.png"></p><h4 id="6、启动Hadoop集群"><a href="#6、启动Hadoop集群" class="headerlink" title="6、启动Hadoop集群"></a>6、启动Hadoop集群</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure><h4 id="7、检查master、node1、node2上的进程"><a href="#7、检查master、node1、node2上的进程" class="headerlink" title="7、检查master、node1、node2上的进程"></a>7、检查master、node1、node2上的进程</h4><ul><li><p>master：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master soft]# jps</span><br><span class="line">2597 NameNode</span><br><span class="line">2793 SecondaryNameNode</span><br><span class="line">2953 ResourceManager</span><br><span class="line">3215 Jps</span><br></pre></td></tr></table></figure></li><li><p>node1：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node1 jdk1.8.0_171]# jps</span><br><span class="line">11361 DataNode</span><br><span class="line">11459 NodeManager</span><br><span class="line">11559 Jps</span><br></pre></td></tr></table></figure></li><li><p>node2：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node2 ~]# jps</span><br><span class="line">11384 DataNode</span><br><span class="line">11482 NodeManager</span><br><span class="line">11582 Jps</span><br></pre></td></tr></table></figure></li></ul><h4 id="8、访问HDFS的WEB界面"><a href="#8、访问HDFS的WEB界面" class="headerlink" title="8、访问HDFS的WEB界面"></a>8、访问HDFS的WEB界面</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http://master:50070</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/05/24/7MHt8o3gIjN2rBn.png" alt="image.png"></p><h4 id="9、访问YARN的WEB界面"><a href="#9、访问YARN的WEB界面" class="headerlink" title="9、访问YARN的WEB界面"></a>9、访问YARN的WEB界面</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http://master:8088</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/05/24/6bOFBZ7GKSyxYEU.png" alt="image.png"></p><h3 id="强制格式化集群（遇到问题的简单暴力的方法）"><a href="#强制格式化集群（遇到问题的简单暴力的方法）" class="headerlink" title="强制格式化集群（遇到问题的简单暴力的方法）"></a>强制格式化集群（遇到问题的简单暴力的方法）</h3><blockquote><p>1、停止正在运行的集群</p><p>​    <strong>stop-all.sh</strong></p><p>2、删除所有节点hadoop根目录中的tmp文件夹</p><p>3、在主节点（master）中hadoop的根目录中的bin目录下，重新格式化HDFS</p><p>​    <strong>.&#x2F;hdfs namenode -format</strong></p><p>4、启动集群</p><p>​    <strong>start-all.sh</strong></p></blockquote>]]></content>
    
    
    <summary type="html">对学习Hadoop的一些知识笔记</summary>
    
    
    
    <category term="大数据" scheme="http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="Hadoop" scheme="http://example.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Redis数据库</title>
    <link href="http://example.com/2022/05/22/Redis%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    <id>http://example.com/2022/05/22/Redis%E6%95%B0%E6%8D%AE%E5%BA%93/</id>
    <published>2022-05-21T16:00:00.000Z</published>
    <updated>2022-05-23T06:35:18.851Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-NoSQL的引言"><a href="#1-NoSQL的引言" class="headerlink" title="1.  NoSQL的引言"></a>1.  NoSQL的引言</h3><p><strong>NoSQL</strong>(<code> Not Only SQL</code> )，意即<strong>不仅仅是SQL</strong>, 泛指非关系型的数据库。Nosql这个技术门类,早期就有人提出,发展至2009年趋势越发高涨。</p><h3 id="2-为什么是NoSQL"><a href="#2-为什么是NoSQL" class="headerlink" title="2. 为什么是NoSQL"></a>2. 为什么是NoSQL</h3><p>随着互联网网站的兴起，传统的关系数据库在应付动态网站，特别是超大规模和高并发的纯动态网站已经显得力不从心，暴露了很多难以克服的问题。如<code>商城网站中对商品数据频繁查询</code>、<code>对热搜商品的排行统计</code>、<code>订单超时问题</code>、以及微信朋友圈（音频，视频）存储等相关使用传统的关系型数据库实现就显得非常复杂，虽然能实现相应功能但是在性能上却不是那么乐观。nosql这个技术门类的出现，更好的解决了这些问题，它告诉了世界不仅仅是sql。</p><h3 id="3-NoSQL的四大分类"><a href="#3-NoSQL的四大分类" class="headerlink" title="3. NoSQL的四大分类"></a>3. NoSQL的四大分类</h3><h4 id="3-1键值-Key-Value-存储数据库"><a href="#3-1键值-Key-Value-存储数据库" class="headerlink" title="3.1键值(Key-Value)存储数据库"></a>3.1键值(Key-Value)存储数据库</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1.说明: </span><br><span class="line">- 这一类数据库主要会使用到一个哈希表，这个表中有一个特定的键和一个指针指向特定的数据。</span><br><span class="line"></span><br><span class="line"># 2.特点</span><br><span class="line">- Key/value模型对于IT系统来说的优势在于简单、易部署。  </span><br><span class="line">- 但是如果DBA只对部分值进行查询或更新的时候，Key/value就显得效率低下了。</span><br><span class="line"></span><br><span class="line"># 3.相关产品</span><br><span class="line">- Tokyo Cabinet/Tyrant,</span><br><span class="line">- Redis  基于内存的    运行软件---&gt;磁盘---&gt;内存中</span><br><span class="line">- SSDB   基于磁盘的    直接与磁盘做交互--&gt; IO</span><br><span class="line">- Voldemort </span><br><span class="line">- Oracle BDB</span><br></pre></td></tr></table></figure><h4 id="3-2列存储数据库-gt-Hbase"><a href="#3-2列存储数据库-gt-Hbase" class="headerlink" title="3.2列存储数据库-&gt;Hbase"></a>3.2列存储数据库-&gt;Hbase</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1.说明- 这部分数据库通常是用来应对分布式存储的海量数据。</span><br><span class="line">2.特点- 键仍然存在，但是它们的特点是指向了多个列。这些列是由列家族来安排的。列簇- rowkey</span><br><span class="line">3.相关产品- Cassandra、`HBase`、Riak.</span><br></pre></td></tr></table></figure><h4 id="3-3文档型数据库"><a href="#3-3文档型数据库" class="headerlink" title="3.3文档型数据库"></a>3.3文档型数据库</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1.说明</span><br><span class="line">- 文档型数据库的灵感是来自于Lotus Notes办公软件的，而且它同第一种键值存储相类似该类型的数据模型是版本化的文档，半结构化的文档以特定的格式存储，比如JSON。文档型数据库可 以看作是键值数据库的升级版，允许之间嵌套键值。而且文档型数据库比键值数据库的查询效率更高</span><br><span class="line">&#123;&#x27;id&#x27;:1001,&#x27;name&#x27;:xiaohu&#125;</span><br><span class="line">&#123;&#x27;id&#x27;:1001,&#x27;name&#x27;:&#x27;xiaohu2,&#x27;address&#x27;:&#x27;anhuihefei&#x27;,&#x27;likes&#x27;:[&#x27;play&#x27;,&#x27;eat&#x27;],&#x27;study&#x27;:&#123;&#x27;yuyan&#x27;:java,&#x27;ruanjian&#x27;:&#x27;mysql&#x27;&#125;&#125;</span><br><span class="line">文档数据库对于单条数据来说，他的事务支持并没有那么强大</span><br><span class="line">目前的mongodb5，支持了单条数据的事务，但是多条不行</span><br><span class="line"></span><br><span class="line"># 2.特点</span><br><span class="line">- 以文档形式存储</span><br><span class="line"></span><br><span class="line"># 3.相关产品</span><br><span class="line">- MongoDB、CouchDB、 MongoDb(4.x). 国内也有文档型数据库SequoiaDB，已经开源。</span><br></pre></td></tr></table></figure><h4 id="3-4图形-Graph-数据库"><a href="#3-4图形-Graph-数据库" class="headerlink" title="3.4图形(Graph)数据库"></a>3.4图形(Graph)数据库</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1.说明</span><br><span class="line">- 图形结构的数据库同其他行列以及刚性结构的SQL数据库不同，它是使用灵活的图形模型，并且能够扩展到多个服务器上。</span><br><span class="line"></span><br><span class="line"># 2.特点</span><br><span class="line">- NoSQL数据库没有标准的查询语言(SQL)，因此进行数据库查询需要制定数据模型。许多NoSQL数据库都有REST式的数据接口或者查询API。</span><br><span class="line"></span><br><span class="line"># 3.相关产品</span><br><span class="line">- Neo4J、InfoGrid、 Infinite Graph</span><br></pre></td></tr></table></figure><h3 id="4-NoSQL应用场景"><a href="#4-NoSQL应用场景" class="headerlink" title="4.NoSQL应用场景"></a>4.NoSQL应用场景</h3><p>数据模型比较简单 </p><p>需要灵活性更强的IT系统 </p><p>对数据库性能要求较高</p><p>不需要高度的数据一致性（NoSQL数据库对事物的支持不是很好）</p><h3 id="5-什么是Redis"><a href="#5-什么是Redis" class="headerlink" title="5.什么是Redis"></a>5.什么是Redis</h3><p>redis是一个内存型的数据库，开源遵循BSD基于内存数据存储被用于作为数据库缓存消息中间件 </p><h3 id="6-Redis特点"><a href="#6-Redis特点" class="headerlink" title="6.Redis特点"></a>6.Redis特点</h3><p>Redis是一个高性能Key-Value内存型数据库在redis中，所有的数据形式都是以键值对的方式来存储的</p><p>redis支持丰富的数据类型string，list，set，sorted set 其中指的是键值对中的值的类型</p><p>redis支持持久化（将数据落盘）</p><p>redis单线程，单进程    由于是单进程和单进程的，所以它是线程安全的，在java中的多线程安全在分布式中不起作用，当时只针对一个JVM有效。</p><h3 id="7-Redis数据库相关指令"><a href="#7-Redis数据库相关指令" class="headerlink" title="7.Redis数据库相关指令"></a>7.Redis数据库相关指令</h3><h4 id="7-1数据库操作指令"><a href="#7-1数据库操作指令" class="headerlink" title="7.1数据库操作指令"></a>7.1数据库操作指令</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1.Redis中库说明</span><br><span class="line">- 使用redis的默认配置器动redis服务后,默认会存在16个库,编号从0-15 配置问价中有个database相关的</span><br><span class="line">- 可以使用select 库的编号 来选择一个redis的库</span><br><span class="line"></span><br><span class="line"># 2.Redis中操作库的指令</span><br><span class="line">- 清空当前的库  FLUSHDB</span><br><span class="line">- 清空全部的库  FLUSHALL</span><br><span class="line"></span><br><span class="line"># 3.redis客户端显示中文</span><br><span class="line">-./redis-cli  -p 7000 --raw</span><br></pre></td></tr></table></figure><h4 id="7-2操作key相关指令"><a href="#7-2操作key相关指令" class="headerlink" title="7.2操作key相关指令"></a>7.2操作key相关指令</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1.DEL指令</span><br><span class="line">- 语法 :  DEL key [key ...] </span><br><span class="line">- 作用 :  删除给定的一个或多个key 。不存在的key 会被忽略。多个key之间使用空格隔开</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值： 被删除key 的数量。 </span><br><span class="line"></span><br><span class="line"># 2.EXISTS指令</span><br><span class="line">- 语法:  EXISTS key</span><br><span class="line">- 作用:  检查给定key 是否存在。多个key之间使用空格隔开，只要有一个key存在，返回值就是1</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值： 若key 存在，返回1 ，否则返回0。 </span><br><span class="line"></span><br><span class="line"># 3.EXPIRE</span><br><span class="line">- 语法:  EXPIRE key seconds</span><br><span class="line">- 作用:  为给定key 设置生存时间，当key 过期时(生存时间为0 )，它会被自动删除。</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 时间复杂度： O(1)</span><br><span class="line">- 返回值：设置成功返回1 。</span><br><span class="line"></span><br><span class="line"># 4.KEYS</span><br><span class="line">- 语法 :  KEYS pattern</span><br><span class="line">- 作用 :  查找所有符合给定模式pattern 的key 。</span><br><span class="line">- 语法:</span><br><span class="line">KEYS * 匹配数据库中所有key 。</span><br><span class="line">KEYS h?llo 匹配hello ，hallo 和hxllo 等。</span><br><span class="line">KEYS h*llo 匹配hllo 和heeeeello 等。</span><br><span class="line">KEYS h[ae]llo 匹配hello 和hallo ，但不匹配hillo 。特殊符号用 &quot;\&quot; 隔开</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值： 符合给定模式的key 列表。</span><br><span class="line"></span><br><span class="line"># 5.MOVE</span><br><span class="line">- 语法 :  MOVE key db  （move name 1----将name键移动到1号库）</span><br><span class="line">- 作用 :  将当前数据库的key 移动到给定的数据库db 当中。</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值： 移动成功返回1 ，失败则返回0 。</span><br><span class="line"></span><br><span class="line"># 6.PEXPIRE</span><br><span class="line">- 语法 :  PEXPIRE key milliseconds</span><br><span class="line">- 作用 :  这个命令和EXPIRE 命令的作用类似，但是它以毫秒为单位设置key 的生存时间，而不像EXPIRE 命令那样，以秒为单位。</span><br><span class="line">- 可用版本： &gt;= 2.6.0</span><br><span class="line">- 时间复杂度： O(1)</span><br><span class="line">- 返回值：设置成功，返回1  key 不存在或设置失败，返回0</span><br><span class="line"></span><br><span class="line"># 7.PEXPIREAT</span><br><span class="line">- 语法 :  PEXPIREAT key milliseconds-timestamp</span><br><span class="line">- 作用 :  这个命令和EXPIREAT 命令类似，但它以毫秒为单位设置key 的过期unix 时间戳，而不是像EXPIREAT那样，以秒为单位。</span><br><span class="line">- 可用版本： &gt;= 2.6.0</span><br><span class="line">- 返回值：如果生存时间设置成功，返回1 。当key 不存在或没办法设置生存时间时，返回0 。(查看EXPIRE 命令获取更多信息)</span><br><span class="line"></span><br><span class="line"># 8.TTL</span><br><span class="line">- 语法 :   TTL key</span><br><span class="line">- 作用 :   以秒为单位，返回给定key 的剩余生存时间(TTL, time to live)。</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值：</span><br><span class="line">当key 不存在时，返回-2 。</span><br><span class="line">当key 存在但没有设置剩余生存时间时，返回-1 。</span><br><span class="line">否则，以秒为单位，返回key 的剩余生存时间。</span><br><span class="line">- Note : 在Redis 2.8 以前，当key 不存在，或者key 没有设置剩余生存时间时，命令都返回-1 。</span><br><span class="line"></span><br><span class="line"># 9.PTTL</span><br><span class="line">- 语法 :  PTTL key</span><br><span class="line">- 作用 :  这个命令类似于TTL 命令，但它以毫秒为单位返回key 的剩余生存时间，而不是像TTL 命令那样，以秒为单位。</span><br><span class="line">- 可用版本： &gt;= 2.6.0</span><br><span class="line">- 返回值： 当key 不存在时，返回-2 。当key 存在但没有设置剩余生存时间时，返回-1 。</span><br><span class="line">- 否则，以毫秒为单位，返回key 的剩余生存时间。</span><br><span class="line">- 注意 : 在Redis 2.8 以前，当key 不存在，或者key 没有设置剩余生存时间时，命令都返回-1 。</span><br><span class="line"></span><br><span class="line"># 10.RANDOMKEY</span><br><span class="line">- 语法 :  RANDOMKEY</span><br><span class="line">- 作用 :  从当前数据库中随机返回(不删除) 一个key 。</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值：当数据库不为空时，返回一个key 。当数据库为空时，返回nil 。</span><br><span class="line"></span><br><span class="line"># 11.RENAME</span><br><span class="line">- 语法 :  RENAME key newkey</span><br><span class="line">- 作用 :  将key 改名为newkey 。当key 和newkey 相同，或者key 不存在时，返回一个错误。当newkey 已经存在时，RENAME 命令将覆盖旧值。</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值： 改名成功时提示OK ，失败时候返回一个错误。</span><br><span class="line"></span><br><span class="line"># 12.TYPE</span><br><span class="line">- 语法 :  TYPE key</span><br><span class="line">- 作用 :  返回key 所储存的值的类型。</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值：</span><br><span class="line">none (key 不存在)</span><br><span class="line">string (字符串)</span><br><span class="line">list (列表)</span><br><span class="line">set (集合)</span><br><span class="line">zset (有序集)</span><br><span class="line">hash (哈希表)</span><br></pre></td></tr></table></figure><h4 id="7-3-String类型"><a href="#7-3-String类型" class="headerlink" title="7.3 String类型"></a>7.3 String类型</h4><p><img src="https://s2.loli.net/2022/05/18/q9nKXHYv7iJgRDT.png"></p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>set</td><td>设置一个key&#x2F;value</td></tr><tr><td>get</td><td>根据key获得对应的value</td></tr><tr><td>mset</td><td>一次设置多个key value</td></tr><tr><td>mget</td><td>一次获得多个key的value</td></tr><tr><td>getset</td><td>获得原始key的值，同时设置新值</td></tr><tr><td>strlen</td><td>获得对应key存储value的长度</td></tr><tr><td>append</td><td>为对应key的value追加内容</td></tr><tr><td>getrange 索引0开始</td><td>截取value的内容    到末尾-1</td></tr><tr><td>setex</td><td>设置一个key存活的有效期（秒）</td></tr><tr><td>psetex</td><td>设置一个key存活的有效期（毫秒）</td></tr><tr><td>setnx</td><td>存在不做任何操作,不存在添加</td></tr><tr><td>msetnx原子操作(只要有一个存在不做任何操作)</td><td>可以同时设置多个key,只有有一个存在都不保存</td></tr><tr><td>decr</td><td>进行数值类型的-1操作</td></tr><tr><td>decrby</td><td>根据提供的数据进行减法操作</td></tr><tr><td>Incr</td><td>进行数值类型的+1操作</td></tr><tr><td>incrby</td><td>根据提供的数据进行加法操作</td></tr><tr><td>Incrbyfloat</td><td>根据提供的数据加入浮点数（不是四舍五入）</td></tr></tbody></table><h4 id="7-4-List类型"><a href="#7-4-List类型" class="headerlink" title="7.4 List类型"></a>7.4 List类型</h4><blockquote><p>list 列表 相当于java中list 集合  特点  元素有序  且 可以重复，key还是一个字符串，值是一个list</p></blockquote><p><img src="https://s2.loli.net/2022/05/18/g3awqjvZztbsJpr.png" alt="image-20200623161114380"></p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>lpush</td><td>将某个值加入到一个key列表头部  lpush list xiaohu xiaohei xiaoming    当列表不存在的时候会进行创建</td></tr><tr><td>lpushx</td><td>同lpush,但是必须要保证这个key存在  必须在列表进行存在的情况下从左插入</td></tr><tr><td>rpush</td><td>将某个值加入到一个key列表末尾</td></tr><tr><td>rpushx</td><td>同rpush,但是必须要保证这个key存在</td></tr><tr><td>lpop</td><td>返回和移除列表左边的第一个元素</td></tr><tr><td>rpop</td><td>返回和移除列表右边的第一个元素</td></tr><tr><td>lrange</td><td>获取某一个下标区间内的元素   lrange list 0 -1</td></tr><tr><td>llen</td><td>获取列表元素个数</td></tr><tr><td>lset</td><td>设置某一个指定索引的值(索引必须存在)</td></tr><tr><td>lindex</td><td>获取某一个指定索引位置的元素</td></tr><tr><td>lrem</td><td>删除重复元素</td></tr><tr><td>ltrim</td><td>保留列表中特定区间内的元素</td></tr><tr><td>linsert</td><td>在某一个元素之前，之后插入新元素</td></tr></tbody></table><h4 id="7-5-Set类型"><a href="#7-5-Set类型" class="headerlink" title="7.5 Set类型"></a>7.5 Set类型</h4><blockquote><p>特点: Set类型 Set集合 元素无序  不可以重复</p></blockquote><p><img src="https://s2.loli.net/2022/05/18/bIPlvO4NeKDwndE.png" alt="image-20200623193634316"></p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>sadd</td><td>为集合添加元素</td></tr><tr><td>smembers</td><td>显示集合中所有元素 无序</td></tr><tr><td>scard</td><td>返回集合中元素的个数</td></tr><tr><td>spop</td><td>随机返回一个元素 并将元素在集合中删除</td></tr><tr><td>smove</td><td>从一个集合中向另一个集合移动元素  必须是同一种类型</td></tr><tr><td>srem</td><td>从集合中删除一个元素</td></tr><tr><td>sismember</td><td>判断一个集合中是否含有这个元素</td></tr><tr><td>srandmember</td><td>随机返回元素   后面可以加数字 表示每次返回的个数</td></tr><tr><td>sdiff</td><td>去掉第一个集合中其它集合含有的相同元素</td></tr><tr><td>sinter</td><td>求交集</td></tr><tr><td>sunion</td><td>求和集</td></tr></tbody></table><h4 id="7-6-ZSet类型"><a href="#7-6-ZSet类型" class="headerlink" title="7.6 ZSet类型"></a>7.6 ZSet类型</h4><p><img src="https://s2.loli.net/2022/05/18/MyIESsQJzqaoh2K.png" alt="image-20200623194903967"></p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>zadd</td><td>添加一个有序集合元素     zadd zset 2 xiaohu 3 xiaohu2</td></tr><tr><td>zcard</td><td>返回集合的元素个数</td></tr><tr><td>zrange 升序 zrevrange 降序</td><td>返回一个范围内的元素       如果想看看分数 withscores</td></tr><tr><td>zrangebyscore</td><td>按照分数查找一个范围内的元素  zrangebyscore zset 0 20 withscores limit 0 2</td></tr><tr><td>zrank</td><td>返回排名</td></tr><tr><td>zrevrank</td><td>倒序排名</td></tr><tr><td>zscore</td><td>显示某一个元素的分数</td></tr><tr><td>zrem</td><td>移除某一个元素</td></tr><tr><td>zincrby</td><td>给某个特定元素加分</td></tr></tbody></table><h4 id="7-7-hash类型"><a href="#7-7-hash类型" class="headerlink" title="7.7 hash类型"></a>7.7 hash类型</h4><p><img src="https://s2.loli.net/2022/05/18/7Ga5zsBAPJCfSqh.png" alt="image-20220511234124908"></p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>hset</td><td>设置一个key&#x2F;value对</td></tr><tr><td>hget</td><td>获得一个key对应的value</td></tr><tr><td>hgetall</td><td>获得所有的key&#x2F;value对</td></tr><tr><td>hdel</td><td>删除某一个key&#x2F;value对</td></tr><tr><td>hexists</td><td>判断一个key是否存在</td></tr><tr><td>hkeys</td><td>获得所有的key</td></tr><tr><td>hvals</td><td>获得所有的value</td></tr><tr><td>hmset</td><td>设置多个key&#x2F;value</td></tr><tr><td>hmget</td><td>获得多个key的value</td></tr><tr><td>hsetnx</td><td>设置一个不存在的key的值</td></tr><tr><td>hincrby</td><td>为value进行加法运算</td></tr><tr><td>hincrbyfloat</td><td>为value加入浮点值</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">对学习Redis的一些知识笔记</summary>
    
    
    
    <category term="数据库" scheme="http://example.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
    <category term="Redis" scheme="http://example.com/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Shell编程</title>
    <link href="http://example.com/2022/05/21/Shell%E7%BC%96%E7%A8%8B/"/>
    <id>http://example.com/2022/05/21/Shell%E7%BC%96%E7%A8%8B/</id>
    <published>2022-05-20T16:00:00.000Z</published>
    <updated>2022-05-23T06:40:37.007Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Shell编程"><a href="#Shell编程" class="headerlink" title="Shell编程"></a>Shell编程</h1><h3 id="1-1-Shell名词解释"><a href="#1-1-Shell名词解释" class="headerlink" title="1.1 Shell名词解释"></a>1.1 Shell名词解释</h3><p>• Kernel</p><p>​        Linux内核主要是为了和硬件打交道</p><p>• Shell</p><p>​        命令器(command interpreter)</p><p>​        Shell是一个用C语言编写的程序，它是用户使用Linux的桥梁。Shell既是一种命令语言， 又是一种程序设计语言.</p><p>​        Shell是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问操作集作系统内核的服务。</p><p>• shell两大主流：</p><p>​        sh:</p><p>​            ■ Bourne shell（sh） ,Solaris,hpux默认shell</p><p>​            ■ Bourne again shell（bash） ,Linux系统默认shell</p><p>​        bash:</p><p>​            ■ C shell(csh)</p><p>​            ■ tc shell(tcsh)</p><p>• #!声明</p><p>告诉系统其后路径所指定的程序即是解释此脚本文件的Shell程序</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello world!&quot;</span></span><br></pre></td></tr></table></figure><h3 id="1-2-Shell本的执行"><a href="#1-2-Shell本的执行" class="headerlink" title="1.2 Shell本的执行"></a>1.2 Shell本的执行</h3><p>• 输入脚本的绝对路径或相对路径</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/root/helloworld.sh</span><br><span class="line"></span><br><span class="line">./helloworld.sh</span><br><span class="line"></span><br><span class="line">注意：执行的必须是一个可执行文件</span><br></pre></td></tr></table></figure><p>• bash或sh +脚本</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sh helloworld.sh</span><br><span class="line"></span><br><span class="line">注意：当脚本没有X权限时，root和文件所有者通过该方式可以正常执行</span><br></pre></td></tr></table></figure><p>•在脚本的路径前再加”或source</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source helloworld.sh</span><br></pre></td></tr></table></figure><p>查看当前正在执行的进程：ps -ef</p><p>•区别</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">第一种和第二种会新开一个bash,不同bash中的变量无法共享。</span><br><span class="line"></span><br><span class="line">第三种是在同一个shell里面执行的</span><br></pre></td></tr></table></figure><p>•export :可以将当前进程的变量传递给子进程去使用</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">将来配置profile的时候所有的变量前必须加export</span><br></pre></td></tr></table></figure><h1 id="2-Shell基础入门"><a href="#2-Shell基础入门" class="headerlink" title="2. Shell基础入门"></a>2. Shell基础入门</h1><h3 id="2-1-shell变量"><a href="#2-1-shell变量" class="headerlink" title="2.1. shell变量"></a>2.1. shell变量</h3><p>定义变量时，变量名不加美元符号</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。</span><br><span class="line"></span><br><span class="line">​中间不能有空格，可以使用下划线（_）。</span><br><span class="line"></span><br><span class="line">​不能使用标点符号。</span><br><span class="line"></span><br><span class="line">​不能使用bash里的关键字（可用help命令查看保留关键字）</span><br></pre></td></tr></table></figure><p>变量的类型</p><p>​    局部变量</p><p>​        局部变量在脚本或命令中定义，仅在当前shell实例中有效，其他shell启动的程序不能访问局部变量。</p><p>​    环境变量</p><p>​        所有的程序，包括shell启动的程序，都能访问环境变量，有些程序需要环境变量来保证其正常运行。</p><p>​    Shell变量</p><p>​        shell变量是由shell程序设置的特殊变量。shell变量中有一部分是环境变量，有一部分是局部变量</p><p>(时间同步 ntpdate cn.ntp.org.cn)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#变量的声明</span></span><br><span class="line">name=<span class="string">&quot;zhangsan&quot;</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> `<span class="built_in">ls</span> /etc` </span><br><span class="line">或</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> $(<span class="built_in">ls</span> /etc)</span><br><span class="line"></span><br><span class="line"><span class="comment">#变量的调用 (推荐不省略大括号)</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$name</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;name&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> skill <span class="keyword">in</span> Ada Coffe Action Java; <span class="keyword">do</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;I am good at <span class="variable">$&#123;skill&#125;</span>Script&quot;</span> </span><br><span class="line"><span class="keyword">done</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># x /bin/sh: NAME: This variable is read only. </span></span><br><span class="line">url=<span class="string">&quot;https://www.google.com&quot;</span> </span><br><span class="line"><span class="built_in">readonly</span> url </span><br><span class="line">url=<span class="string">&quot;https://www.runoob.com&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除变量 </span></span><br><span class="line"><span class="built_in">unset</span> name</span><br></pre></td></tr></table></figure><h3 id="2-2-Shell的字符串"><a href="#2-2-Shell的字符串" class="headerlink" title="2.2. Shell的字符串"></a>2.2. Shell的字符串</h3><p>字符串是shell编程中最常用最有用的数据类型，字符串可以用单引号，也可以用双引号，也可以不用引号。</p><p>单引号</p><p>​    单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的；</p><p>​    单引号字串中不能出现单独一个的单引号，但可成对出现，作为字符串拼接使用。</p><p>双引号</p><p>​    双引号里可以有变量</p><p>​    双引号里可以出现转义字符</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 声明字符串 </span></span><br><span class="line">str1=<span class="string">&quot;hello world 1&quot;</span> </span><br><span class="line">str2=<span class="string">&#x27;hello world 2&#x27;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 字符串拼接--双引号 </span></span><br><span class="line">name=<span class="string">&#x27;sunwukong&#x27;</span> </span><br><span class="line">name1=<span class="string">&quot;hello, &quot;</span><span class="variable">$name</span><span class="string">&quot; !&quot;</span> </span><br><span class="line">name2=<span class="string">&quot;hello, <span class="variable">$&#123;name&#125;</span> !&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 字符串拼接--单引号 </span></span><br><span class="line">passwd=<span class="string">&#x27;123456&#x27;</span> </span><br><span class="line">passwd1=<span class="string">&#x27;hello, &#x27;</span><span class="variable">$passwd</span><span class="string">&#x27; !&#x27;</span></span><br><span class="line">passwd2=<span class="string">&#x27;hello, $&#123;passwd&#125; !&#x27;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$passwd2</span> <span class="comment"># hello, $&#123;passwd&#125; ! </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 字符串的长度 </span></span><br><span class="line">email=<span class="string">&quot;123456@qq.com&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;#email&#125;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;email:1:4&#125;</span></span><br></pre></td></tr></table></figure><h3 id="2-3-Shell数组（尾对象）伪数组"><a href="#2-3-Shell数组（尾对象）伪数组" class="headerlink" title="2.3  Shell数组（尾对象）伪数组"></a>2.3  Shell数组（尾对象）伪数组</h3><p>bash支持一维数组（不支持多维数组），并且没有限定数组的大小。</p><p>数组元素的下标由 0 开始编号。获取数组中的元素要利用下标，下标可以是整数或算术表达式，其值应大于或等于 0。 </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义数组 括号来表示数组，数组元素用&quot;空格&quot;符号分割开 </span></span><br><span class="line">数组名=(值1 值2 ... 值n) </span><br><span class="line">favs=(<span class="string">&quot;足球&quot;</span> <span class="string">&quot;蓝球&quot;</span> <span class="string">&quot;乒乓球&quot;</span> <span class="string">&quot;保龄球&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数组 $&#123;数组名[下标]&#125; </span></span><br><span class="line">fav=<span class="variable">$&#123;favs[1]&#125;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 @ 符号可以获取数组中的所有元素 </span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;favs[@]&#125;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数组的长度 </span></span><br><span class="line">length1=<span class="variable">$&#123;#favs[@]&#125;</span> </span><br><span class="line">length2=<span class="variable">$&#123;#favs[*]&#125;</span></span><br></pre></td></tr></table></figure><h3 id="2-4-Shell的注释"><a href="#2-4-Shell的注释" class="headerlink" title="2.4  Shell的注释"></a>2.4  Shell的注释</h3><p>以 <strong>#</strong> 开头的行就是注释，会被解释器忽略。</p><p>通过每一行加一个 <strong>#</strong> 号设置多行注释</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#-------------------------------------------- </span></span><br><span class="line"><span class="comment"># 这是一个注释 </span></span><br><span class="line"><span class="comment"># author： </span></span><br><span class="line"><span class="comment"># site： </span></span><br><span class="line"><span class="comment">#-------------------------------------------- </span></span><br><span class="line"><span class="comment">##### 服务器配置-start #####</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">##### 服务器配置-end ##### </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 特殊的多行注释 </span></span><br><span class="line"><span class="comment"># end of file</span></span><br><span class="line">:&lt;&lt;<span class="string">EOF  </span></span><br><span class="line"><span class="string">注释内容... </span></span><br><span class="line"><span class="string">注释内容... </span></span><br><span class="line"><span class="string">注释内容... </span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">:&lt;&lt;! </span><br><span class="line">注释内容... </span><br><span class="line">注释内容... </span><br><span class="line">注释内容... </span><br><span class="line">!</span><br></pre></td></tr></table></figure><h3 id="2-5-Shell参数传递"><a href="#2-5-Shell参数传递" class="headerlink" title="2.5  Shell参数传递"></a>2.5  Shell参数传递</h3><p>执行 Shell 脚本时，向脚本传递参数，脚本内获取参数的格式为：**$n<strong>。</strong>n** 代表一个数字</p><table><thead><tr><th><strong>参数处理</strong></th><th><strong>参数说明</strong></th></tr></thead><tbody><tr><td>$#</td><td>传递到脚本的参数个数</td></tr><tr><td>$*</td><td>以一个单字符串显示所有向脚本传递的参数。</td></tr><tr><td>$$</td><td>脚本运行的当前进程ID号</td></tr><tr><td>$!</td><td>后台运行的最后一个进程的ID号</td></tr><tr><td>$?</td><td>显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误。</td></tr><tr><td>$0</td><td>执行的文件名</td></tr></tbody></table> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Shell 传递参数实例！&quot;</span>; </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;执行的文件名：<span class="variable">$0</span>&quot;</span>; </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第一个参数为：<span class="variable">$1</span>&quot;</span>; </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第二个参数为：<span class="variable">$2</span>&quot;</span>; </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第三个参数为：<span class="variable">$3</span>&quot;</span>; </span><br><span class="line"></span><br><span class="line"><span class="comment"># ./hello.sh 11 22 33 44</span></span><br></pre></td></tr></table></figure><h1 id="3-Shell高级进阶"><a href="#3-Shell高级进阶" class="headerlink" title="3  Shell高级进阶"></a>3  Shell高级进阶</h1><h3 id="3-1-Shell运算符"><a href="#3-1-Shell运算符" class="headerlink" title="3.1 Shell运算符"></a>3.1 Shell运算符</h3><p>运算符的分类</p><p>​    算数运算符</p><table><thead><tr><th>运算符</th><th>说明</th><th>举例</th></tr></thead><tbody><tr><td>+</td><td>加法</td><td>‘expr $a + $b’ 为 30。</td></tr><tr><td>-</td><td>减法</td><td>‘expr $a-$b’结果为-10。</td></tr><tr><td>*</td><td>乘法</td><td>‘expr $a * $b’ 结果为 200。</td></tr><tr><td>&#x2F;</td><td>除法</td><td>‘expr$b&#x2F;$a’结果为2。</td></tr><tr><td>%</td><td>取余</td><td>‘expr $b % $a’ 结果为0。</td></tr><tr><td>&#x3D;</td><td>赋值</td><td>a&#x3D;$b将把变量b的值赋给a</td></tr><tr><td>&#x3D;&#x3D;</td><td>相等，用于比较两个数字，相同返回true</td><td>[$a &#x3D;&#x3D; $b]返回false。</td></tr><tr><td>!&#x3D;</td><td>不相等,用于比较两个数字，不相同返回true</td><td>[$a !&#x3D; $b]返回true。</td></tr></tbody></table><p>​    关系运算符</p><table><thead><tr><th>运算符</th><th>说明</th><th>举例</th></tr></thead><tbody><tr><td>-eq</td><td>检测两个数是否相等，相等返回true</td><td>[$a -eq $b ]返回 false。</td></tr><tr><td>-ne</td><td>检测两个数是否不相等，不相等返回true</td><td>[$a -ne $b ]返回 true。</td></tr><tr><td>-gt</td><td>检测左边的数是否大于右边的，如果是，返回true</td><td>[$a -gt $b ]返回 false.</td></tr><tr><td>-lt</td><td>检测左边的数是否小于右边的，如果是，返回true</td><td>[$a -It $b ]返回 true。</td></tr><tr><td>-ge</td><td>检测左边的数是否大于等于右边的，如果是，返回true</td><td>[$a -ge $b ]返回 false。</td></tr><tr><td>-le</td><td>检测左边的数是否小于等于右边的，如果是，返回true</td><td>[$a -le $b ]返回 true.</td></tr></tbody></table><p>​    布尔运算符</p><table><thead><tr><th>运算符</th><th>说明</th><th>举例</th></tr></thead><tbody><tr><td>！</td><td>非运算，表达式为true则返回false,否则退回true。</td><td>[! false ]返回 true。</td></tr><tr><td>-o</td><td>或运算，有一个表达式为true则返回true。</td><td>[$a -It 20 -o $b -gt100 ]返回 true。</td></tr><tr><td>-a</td><td>与运算，两个表达式都为true才返回true.</td><td>[$a -It 20 -a $b -gt100 J 返回 false。</td></tr></tbody></table><p>​    字符串运算符</p><table><thead><tr><th>运算符</th><th>说明</th><th>举例</th></tr></thead><tbody><tr><td>&amp;&amp;</td><td>逻辑的AND</td><td>[[$a -It 100 &amp;&amp; $b-gt 100 ]]返回 false</td></tr><tr><td>||</td><td>逻辑的OR</td><td>[[$a -It 100 || $b -gt 100 ]]返回 true</td></tr></tbody></table><p>​    文件测试运算符</p><table><thead><tr><th>运算符</th><th>说明</th><th>举例</th></tr></thead><tbody><tr><td>&#x3D;</td><td>检测两个字符串是否相等，相等返回true。</td><td>[$a &#x3D; $b ]返回 false。</td></tr><tr><td>!&#x3D;</td><td>检测两个字符串是否相等，不相等返回true。</td><td>[$a !&#x3D; $b ]返回 true。</td></tr><tr><td>-z</td><td>检测字符串长度是否为0,为0返回true。</td><td>[-z $a ]返回 false。</td></tr><tr><td>-n</td><td>检测字符串长度是否不为不为0返回true。</td><td>[n “$a”]返回 true.</td></tr><tr><td>$</td><td>检测字符串是否为空，不为空返回trueo</td><td>[$a]返回 true.</td></tr></tbody></table><h4 id="3-1-1-算数运算符"><a href="#3-1-1-算数运算符" class="headerlink" title="3.1.1 算数运算符"></a>3.1.1 算数运算符</h4><p> expr 是一款表达式计算工具，使用它能完成表达式的求值操作。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line">a=10 </span><br><span class="line">b=20 </span><br><span class="line"></span><br><span class="line">val=`<span class="built_in">expr</span> <span class="variable">$a</span> + <span class="variable">$b</span>` </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a + b : <span class="variable">$val</span>&quot;</span> </span><br><span class="line"></span><br><span class="line">val=`<span class="built_in">expr</span> <span class="variable">$a</span> - <span class="variable">$b</span>` </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a - b : <span class="variable">$val</span>&quot;</span> </span><br><span class="line"></span><br><span class="line">val=`<span class="built_in">expr</span> <span class="variable">$a</span> \* <span class="variable">$b</span>` </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a * b : <span class="variable">$val</span>&quot;</span> </span><br><span class="line"></span><br><span class="line">val=`<span class="built_in">expr</span> <span class="variable">$b</span> / <span class="variable">$a</span>` </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;b / a : <span class="variable">$val</span>&quot;</span> </span><br><span class="line"></span><br><span class="line">val=`<span class="built_in">expr</span> <span class="variable">$b</span> % <span class="variable">$a</span>` </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;b % a : <span class="variable">$val</span>&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> == <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a 等于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> != <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a 不等于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h4 id="3-1-2-关系运算符"><a href="#3-1-2-关系运算符" class="headerlink" title="3.1.2 关系运算符"></a>3.1.2 关系运算符</h4><p>关系运算符只支持数字，不支持字符串，除非字符串的值是数字。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"></span><br><span class="line">a=10 </span><br><span class="line">b=20 </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -eq <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -eq <span class="variable">$b</span> : a 等于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -eq <span class="variable">$b</span>: a 不等于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -ne <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -ne <span class="variable">$b</span>: a 不等于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -ne <span class="variable">$b</span> : a 等于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -gt <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -gt <span class="variable">$b</span>: a 大于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -gt <span class="variable">$b</span>: a 不大于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -lt <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -lt <span class="variable">$b</span>: a 小于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -lt <span class="variable">$b</span>: a 不小于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -ge <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -ge <span class="variable">$b</span>: a 大于或等于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -ge <span class="variable">$b</span>: a 小于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -le <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -le <span class="variable">$b</span>: a 小于或等于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -le <span class="variable">$b</span>: a 大于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h4 id="3-1-3布尔运算符"><a href="#3-1-3布尔运算符" class="headerlink" title="3.1.3布尔运算符"></a>3.1.3布尔运算符</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"></span><br><span class="line">a=10 </span><br><span class="line">b=20 </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> != <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> != <span class="variable">$b</span> : a 不等于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> == <span class="variable">$b</span>: a 等于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -lt 100 -a <span class="variable">$b</span> -gt 15 ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> 小于 100 且 <span class="variable">$b</span> 大于 15 : 返回 true&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> 小于 100 且 <span class="variable">$b</span> 大于 15 : 返回 false&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -lt 100 -o <span class="variable">$b</span> -gt 100 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> 小于 100 或 <span class="variable">$b</span> 大于 100 : 返回 true&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> 小于 100 或 <span class="variable">$b</span> 大于 100 : 返回 false&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -lt 5 -o <span class="variable">$b</span> -gt 100 ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> 小于 5 或 <span class="variable">$b</span> 大于 100 : 返回 true&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> 小于 5 或 <span class="variable">$b</span> 大于 100 : 返回 false&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h4 id="3-1-4-逻辑运算符"><a href="#3-1-4-逻辑运算符" class="headerlink" title="3.1.4 逻辑运算符"></a>3.1.4 逻辑运算符</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"></span><br><span class="line">a=10 </span><br><span class="line">b=20 </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$a</span> -lt 100 &amp;&amp; <span class="variable">$b</span> -gt 100 ]] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;返回 true&quot;</span> elseecho <span class="string">&quot;返回 false&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$a</span> -lt 100 || <span class="variable">$b</span> -gt 100 ]] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;返回 true&quot;</span> elseecho <span class="string">&quot;返回 false&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h4 id="3-1-5-字符串运算符"><a href="#3-1-5-字符串运算符" class="headerlink" title="3.1.5 字符串运算符"></a>3.1.5 字符串运算符</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"></span><br><span class="line">a=<span class="string">&quot;abc&quot;</span> </span><br><span class="line">b=<span class="string">&quot;efg&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> = <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> = <span class="variable">$b</span> : a 等于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> = <span class="variable">$b</span>: a 不等于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> != <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> != <span class="variable">$b</span> : a 不等于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> != <span class="variable">$b</span>: a 等于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="variable">$a</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;-z <span class="variable">$a</span> : 字符串长度为 0&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;-z <span class="variable">$a</span> : 字符串长度不为 0&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$a</span>&quot;</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;-n <span class="variable">$a</span> : 字符串长度不为 0&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;-n <span class="variable">$a</span> : 字符串长度为 0&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> : 字符串不为空&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> : 字符串为空&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h4 id="3-1-6-文件测试运算符"><a href="#3-1-6-文件测试运算符" class="headerlink" title="3.1.6 文件测试运算符"></a>3.1.6 文件测试运算符</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"></span><br><span class="line">file=<span class="string">&quot;/var/node/test.sh&quot;</span> </span><br><span class="line"><span class="keyword">if</span> [ -r <span class="variable">$file</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件可读&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件不可读&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -w <span class="variable">$file</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件可写&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件不可写&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -x <span class="variable">$file</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件可执行&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件不可执行&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -f <span class="variable">$file</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件为普通文件&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件为特殊文件&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -d <span class="variable">$file</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件是个目录&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件不是个目录&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -s <span class="variable">$file</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件不为空&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件为空&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h4 id="3-2-echo打印数据"><a href="#3-2-echo打印数据" class="headerlink" title="3.2 echo打印数据"></a>3.2 echo打印数据</h4><p> Shell的echo指令用于字符串的输出。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 显示普通字符串 </span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello World&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 显示转义字符 </span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;\&quot;Hello World\&quot;&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 显示变量 </span></span><br><span class="line">name=<span class="string">&quot;zhangsan&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$name</span> Hello World&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 显示换行 </span></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;OK! \n&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello World&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 显示不换行 </span></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;OK! \c&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello World&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 显示结果定向至文件 </span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello World&quot;</span> &gt; myfile </span><br><span class="line"><span class="comment">## &gt; 代表覆盖</span></span><br><span class="line"><span class="comment"># &gt;&gt; 追加写入</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 原样输出字符串 </span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;$name\&quot;&#x27;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 显示命令执行结果 </span></span><br><span class="line"><span class="built_in">echo</span> `<span class="built_in">date</span>`</span><br></pre></td></tr></table></figure><h3 id="3-4-Shell流程控制"><a href="#3-4-Shell流程控制" class="headerlink" title="3.4 Shell流程控制"></a>3.4 Shell流程控制</h3><h4 id="3-4-1-if"><a href="#3-4-1-if" class="headerlink" title="3.4.1  if"></a>3.4.1  if</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> conditionl</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">commandl</span><br><span class="line"><span class="keyword">elif</span> condition2</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">command2</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">commandN</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> == <span class="variable">$b</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a 等于 b&quot;</span></span><br><span class="line"><span class="keyword">elif</span> [ <span class="variable">$a</span> -gt <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a 大于 b&quot;</span></span><br><span class="line"><span class="keyword">elif</span> [ <span class="variable">$a</span> -lt <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a 小于 bn&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;没有符合的条件&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h4 id="Shell-case语句为多选择语句。"><a href="#Shell-case语句为多选择语句。" class="headerlink" title="Shell case语句为多选择语句。"></a>Shell case语句为多选择语句。</h4><h4 id="可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。"><a href="#可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。" class="headerlink" title="可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。"></a>可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> 值 <span class="keyword">in</span> </span><br><span class="line">模式1)</span><br><span class="line">command1 </span><br><span class="line">command2 </span><br><span class="line">... </span><br><span class="line">commandN ;; </span><br><span class="line"></span><br><span class="line">模式2）</span><br><span class="line">command1 </span><br><span class="line">command2 </span><br><span class="line">... </span><br><span class="line">commandN </span><br><span class="line">;; </span><br><span class="line"><span class="keyword">esac</span> </span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;输入 1 到 4 之间的数字:&#x27;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;你输入的数字为:&#x27;</span> </span><br><span class="line"><span class="built_in">read</span> num </span><br><span class="line"><span class="keyword">case</span> <span class="variable">$num</span> <span class="keyword">in</span> </span><br><span class="line">1) <span class="built_in">echo</span> <span class="string">&#x27;你选择了 1&#x27;</span> </span><br><span class="line">;;</span><br><span class="line">2) <span class="built_in">echo</span> <span class="string">&#x27;你选择了 2&#x27;</span> </span><br><span class="line">;;</span><br><span class="line">3) <span class="built_in">echo</span> <span class="string">&#x27;你选择了 3&#x27;</span> </span><br><span class="line">;;</span><br><span class="line">4) <span class="built_in">echo</span> <span class="string">&#x27;你选择了 4&#x27;</span> </span><br><span class="line">;;</span><br><span class="line">*) <span class="built_in">echo</span> <span class="string">&#x27;你没有输入 1 到 4 之间的数字&#x27;</span> </span><br><span class="line">;; </span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><h4 id="3-4-2-for"><a href="#3-4-2-for" class="headerlink" title="3.4.2 for"></a>3.4.2 for</h4><p>当变量值在列表里，for循环即执行一次所有命令，使用变量名获取列表中的当前取值。</p><p>命令可为田可有效的shell命令和语句。in列表可以包含替换、字符串和文件名。</p><p>in列表是可选的，如果不用它，for循环使用命令行的位置参数。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> item1 item2 ... itemN </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line">command1 </span><br><span class="line">command2 </span><br><span class="line">... </span><br><span class="line">commandN </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> loop <span class="keyword">in</span> 1 2 3 4 5 </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;The value is: <span class="variable">$loop</span>&quot;</span> </span><br><span class="line"><span class="keyword">done</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> str <span class="keyword">in</span> <span class="string">&#x27;This is a string&#x27;</span> <span class="string">&#x27;hello moto&#x27;</span> </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$str</span> </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h4 id="3-4-3-while循环"><a href="#3-4-3-while循环" class="headerlink" title="3.4.3 while循环"></a>3.4.3 while循环</h4><p>while循环用于不断执行一系列命令，也用于从输入文件中读取数据；命令通常为测试条件。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> condition </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line"><span class="built_in">command</span> </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Bash let 命令，它用于执行一个或多个表达式，变量计算中不需要加上 $ 来表示变量 </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#!/bin/bash </span></span><br><span class="line">int=1 </span><br><span class="line"><span class="keyword">while</span>(( <span class="variable">$int</span>&lt;=5 )) </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$int</span> </span><br><span class="line"><span class="built_in">let</span> <span class="string">&quot;int++&quot;</span> </span><br><span class="line"><span class="keyword">done</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 无限循环 </span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">true</span> </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line"><span class="built_in">command</span> </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h4 id="3-4-4-break"><a href="#3-4-4-break" class="headerlink" title="3.4.4 break"></a>3.4.4 break</h4><p>break命令允许跳出所有循环（终止执行后面的所有循环）。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"><span class="keyword">while</span> : </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line"><span class="built_in">echo</span> -n <span class="string">&quot;输入 1 到 5 之间的数字:&quot;</span> </span><br><span class="line"><span class="built_in">read</span> aNum </span><br><span class="line"><span class="keyword">case</span> <span class="variable">$aNum</span> <span class="keyword">in</span> </span><br><span class="line">1|2|3|4|5) <span class="built_in">echo</span> <span class="string">&quot;你输入的数字为 <span class="variable">$aNum</span>!&quot;</span> </span><br><span class="line">;;</span><br><span class="line">*) <span class="built_in">echo</span> <span class="string">&quot;你输入的数字不是 1 到 5 之间的! 游戏结束&quot;</span> </span><br><span class="line"><span class="built_in">break</span> </span><br><span class="line">;; </span><br><span class="line"><span class="keyword">esac</span> </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h4 id="3-4-5-continue"><a href="#3-4-5-continue" class="headerlink" title="3.4.5 continue"></a>3.4.5 continue</h4><p>continue命令不会跳出所有循环，仅仅跳出当前循环。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"><span class="keyword">while</span> : </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line"><span class="built_in">echo</span> -n <span class="string">&quot;输入 1 到 5 之间的数字: &quot;</span> </span><br><span class="line"><span class="built_in">read</span> aNum </span><br><span class="line"><span class="keyword">case</span> <span class="variable">$aNum</span> <span class="keyword">in</span> </span><br><span class="line">1|2|3|4|5) <span class="built_in">echo</span> <span class="string">&quot;你输入的数字为 <span class="variable">$aNum</span>!&quot;</span> </span><br><span class="line">;;</span><br><span class="line">*) <span class="built_in">echo</span> <span class="string">&quot;你输入的数字不是 1 到 5 之间的!&quot;</span></span><br><span class="line">        <span class="built_in">continue</span> </span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;游戏结束&quot;</span> </span><br><span class="line">        ;; </span><br><span class="line">    <span class="keyword">esac</span> </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h2 id="3-5-Shell函数"><a href="#3-5-Shell函数" class="headerlink" title="3.5 Shell函数"></a>3.5 Shell函数</h2><p>linux shell 可以用户定义函数，然后在shell脚本中可以随便调用。</p><p>可以带function fun() 定义，也可以直接fun() 定义,不带任何参数。</p><p>参数返回，可以显示加：return 返回，如果不加，将以最后一条命令运行结果，作为返回值。return后跟数值n(0-255</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 第一个函数------------------------------ </span></span><br><span class="line"><span class="function"><span class="title">demoFun</span></span>()&#123; </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;这是我的第一个 shell 函数!&quot;</span> </span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;-----函数开始执行-----&quot;</span> </span><br><span class="line">demoFun </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;-----函数执行完毕-----&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 函数返回值------------------------------ </span></span><br><span class="line"><span class="function"><span class="title">funWithReturn</span></span>()&#123; </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;这个函数会对输入的两个数字进行相加运算...&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;输入第一个数字: &quot;</span> </span><br><span class="line"><span class="built_in">read</span> aNum </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;输入第二个数字: &quot;</span> </span><br><span class="line"><span class="built_in">read</span> anotherNum  </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;两个数字分别为 <span class="variable">$aNum</span> 和 <span class="variable">$anotherNum</span> !&quot;</span> </span><br><span class="line"><span class="built_in">return</span> $((<span class="variable">$aNum</span>+<span class="variable">$anotherNum</span>)) </span><br><span class="line">&#125;</span><br><span class="line">funWithReturn </span><br><span class="line"><span class="comment"># 函数返回值在调用该函数后通过 $? 来获得。 </span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;输入的两个数字之和为 $? !&quot;</span> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 函数参数------------------------------ </span></span><br><span class="line"><span class="function"><span class="title">funWithParam</span></span>()&#123; </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第一个参数为 <span class="variable">$1</span> !&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第二个参数为 <span class="variable">$2</span> !&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第十个参数为 <span class="variable">$10</span> !&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第十个参数为 <span class="variable">$&#123;10&#125;</span> !&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第十一个参数为 <span class="variable">$&#123;11&#125;</span> !&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;参数总数有 <span class="variable">$#</span> 个!&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;作为一个字符串输出所有参数 $* !&quot;</span> </span><br><span class="line">&#125;</span><br><span class="line">funWithParam 1 2 3 4 5 6 7 8 9</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">对学习Shell编程的一些知识笔记</summary>
    
    
    
    <category term="Shell" scheme="http://example.com/categories/Shell/"/>
    
    
    <category term="Shell编程" scheme="http://example.com/tags/Shell%E7%BC%96%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>shell中系统任务设置</title>
    <link href="http://example.com/2022/05/21/shell%E4%BB%BB%E5%8A%A1%E7%9B%B8%E5%85%B3%E8%AE%BE%E7%BD%AE/"/>
    <id>http://example.com/2022/05/21/shell%E4%BB%BB%E5%8A%A1%E7%9B%B8%E5%85%B3%E8%AE%BE%E7%BD%AE/</id>
    <published>2022-05-20T16:00:00.000Z</published>
    <updated>2022-05-23T06:40:31.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="shell中系统任务设置"><a href="#shell中系统任务设置" class="headerlink" title="shell中系统任务设置"></a>shell中系统任务设置</h1><h3 id="1、系统启动流程"><a href="#1、系统启动流程" class="headerlink" title="1、系统启动流程"></a>1、系统启动流程</h3><p>启动计算机的硬件(BIOS)</p><p>​        读取时间</p><p>​        选择对应的启动模式(USB HDD EFI）</p><p>如果是Linux系统，回去找&#x2F;boot目录.引导这个系统启动</p><p>计算机系统开始启动,读取初始化配置文件</p><p>​        vim &#x2F;etc&#x2F;inittab</p><p>​        启动时控制着计算机的运行级别 runlevel</p><table><thead><tr><th>0</th><th>halt(关机)</th></tr></thead><tbody><tr><td>1</td><td>Single user mode(单用户模式)</td></tr><tr><td>2</td><td>Multiuser, without NFS(多用户模式，但是无网络状态) FS–&gt;FileSystem</td></tr><tr><td>3</td><td>Full multiuser mode(多用户完整版模式)</td></tr><tr><td>4</td><td>unused (保留模式)</td></tr><tr><td>5</td><td>X11(用户界面模式)</td></tr><tr><td>6</td><td>reboot(重启模式)</td></tr></tbody></table><p>​        id:3:initdefault: 默认runlevel为3 </p><p>​        以runlevel&#x3D;3开始启动对应的服务和组件</p><p>开始默认引导公共的组件或者服务</p><p>​        vim &#x2F;etc&#x2F;rc.d&#x2F;rc.sysinit</p><p>开始加载对应runlevel的服务</p><p>​        vi &#x2F;etc&#x2F;rc3.d&#x2F;</p><p>​            K:关机时需要关闭的服务</p><p>​            S:启动时需要开启的服务</p><p>​            数字代表了开启或者关闭的顺序</p><p>​            所有的文件都是软链接，链接的地址为 &#x2F;etc&#x2F;init.d</p><p>当启动完毕，所有的服务也被加载完成</p><h3 id="2、系统服务"><a href="#2、系统服务" class="headerlink" title="2、系统服务"></a>2、系统服务</h3><p>​    我们可以使用chkconfig命令查看当前虚拟机的服务</p><p>​    通过查看可以得知不同的级别对应到每一个服务确定本次开机自动启动</p><p>​    开机结束后，我们需要使用service（Centos6）Systemctl(Centos7)命令控制服务的开启或者关闭</p><h3 id="3、-开机自启动服务"><a href="#3、-开机自启动服务" class="headerlink" title="3、 开机自启动服务"></a>3、 开机自启动服务</h3><h5 id="rc-local"><a href="#rc-local" class="headerlink" title="rc.local"></a>rc.local</h5><p>​        首先创建脚本存放的文件夹</p><p>​                mkdir -p &#x2F;usr&#x2F;local&#x2F;scripts</p><p>​        在文件夹中创建脚本文件</p><p>​                vim hello.sh</p><p>​                给予执行权限</p><p>​        去&#x2F;etc&#x2F;rc.d&#x2F;rc.local文件中添加脚本的绝对路径</p><p>​                给予rc.local执行权限</p><p>​        创建一个文件夹</p><p>​                mkdir &#x2F;usr&#x2F;local&#x2F;soft&#x2F;ceshitest</p><p>​        重启虚拟机</p><p>​                reboot</p><h5 id="chkconfig"><a href="#chkconfig" class="headerlink" title="chkconfig"></a>chkconfig</h5><p>​        创建开机自启动脚本文件</p><p>​        vim schoolntpdate.sh</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"><span class="comment">#chkconfig: 2345 88 99 </span></span><br><span class="line"><span class="comment">#description:auto_run </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开机创建文件夹</span></span><br><span class="line"><span class="built_in">mkdir</span> /usr/local/soft/ceshitest2</span><br></pre></td></tr></table></figure><p>​        给其设置执行权限</p><p>​                chmod u+x schoolntpdate.sh</p><p>​        将脚本拷贝到 &#x2F;etc&#x2F;init.d    下</p><p>​                cp schoolntpdate.sh &#x2F;etc&#x2F;init.d&#x2F;</p><p>​        添加到服务</p><p>​                chkconfig –add &#x2F;etc&#x2F;init.d&#x2F;schoolntpdate.sh</p><p>​        重启服务器</p><p>​                reboot</p><h3 id="4、定时任务"><a href="#4、定时任务" class="headerlink" title="4、定时任务"></a>4、定时任务</h3><blockquote><p>在linux中最小时间是到分钟的</p></blockquote><p>在系统服务中心，crond负责周期任务</p><p>​        systemctl status crond.service</p><p>添加任务，编辑当前用户的任务列表</p><p>​        crontab -e</p><p>编辑任务</p><p>​        星 星 星 星 星 command</p><p>​        分 时 日 月 周 命令</p><p>​        第1列表示分钟1～59 每分钟用*或者 *&#x2F;2表示</p><p>​        第2列表示小时1～23（0表示0点）</p><p>​        第3列表示日期1～31</p><p>​        第4列表示月份1～12</p><p>​        第5列标识号星期0～6（0表示星期天）</p><p>​        第6列要运行的命令</p><p>​        *：表示任意时间都，实际上就是“每”的意思。可以代表00-23小时或者00-12每月或者00-59分</p><p>​        -：表示区间，是一个范围，00 17-19 * * * cmd，就是每天17,18,19点的整点执行命令</p><p>​        ,：是分割时段，30 3,19,21 * * * cmd，就是每天凌晨3和晚上19,21点的半点时刻执行命令</p><p>​        &#x2F;n：表示分割，可以看成除法，*&#x2F;5 * * * * cmd，每隔五分钟执行一次</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">30 21 * * * /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">上面的例子表示每晚的21:30重启apache。 </span><br><span class="line"></span><br><span class="line">45 4 1,10,22 * * /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">上面的例子表示每月1、10、22日的4 : 45重启apache。 </span><br><span class="line"></span><br><span class="line">10 1 * * 6,0 /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">上面的例子表示每周六、周日的1 : 10重启apache。 </span><br><span class="line"></span><br><span class="line">0,30 18-23 * * * /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">上面的例子表示在每天18 : 00至23 : 00之间每隔30分钟重启apache。 </span><br><span class="line"></span><br><span class="line">0 23 * * 6 /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">上面的例子表示每星期六的11 : 00 pm重启apache。 </span><br><span class="line"></span><br><span class="line">* */2 * * * /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">每两小时重启apache </span><br><span class="line"></span><br><span class="line">* 23-7/1 * * * /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">晚上11点到早上7点之间，每隔一小时重启apache </span><br><span class="line"></span><br><span class="line">0 11 4 * mon-wed /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">每月的4号与每周一到周三的11点重启apache </span><br><span class="line"></span><br><span class="line">0 4 1 jan * /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">一月一号的4点重启apache</span><br><span class="line"></span><br><span class="line">需求：每分钟要干一些事情</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--（功能描述：显示年月日时分秒） </span><br><span class="line"><span class="built_in">date</span> <span class="string">&quot;+%Y%m%d%H%M%S&quot;</span></span><br></pre></td></tr></table></figure><p>重启crontab，使配置生效</p><p>​        systemctl restart crond.service</p><p>通过crontab -l</p><p>​        查看当前的定时任务</p><p>查看任务的历史</p><p>​        vim &#x2F;var&#x2F;spool&#x2F;mail&#x2F;root</p><p>清除任务</p><p>​        crontab -r</p>]]></content>
    
    
    <summary type="html">对学习shell中系统任务设置的一些知识笔记</summary>
    
    
    
    <category term="Shell" scheme="http://example.com/categories/Shell/"/>
    
    
    <category term="shell中系统任务设置" scheme="http://example.com/tags/shell%E4%B8%AD%E7%B3%BB%E7%BB%9F%E4%BB%BB%E5%8A%A1%E8%AE%BE%E7%BD%AE/"/>
    
  </entry>
  
  <entry>
    <title>Redis持久化</title>
    <link href="http://example.com/2022/05/20/Redis%E6%8C%81%E4%B9%85%E5%8C%96/"/>
    <id>http://example.com/2022/05/20/Redis%E6%8C%81%E4%B9%85%E5%8C%96/</id>
    <published>2022-05-19T16:00:00.000Z</published>
    <updated>2022-05-23T06:34:45.699Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-持久化机制"><a href="#1-持久化机制" class="headerlink" title="1.持久化机制"></a>1.持久化机制</h3><p>Redis官方提供了两种不同的持久化方法来将内存的数据存储到硬盘里</p><p><strong>快照（Snapshot）</strong></p><p><strong>AOF（Append Only File）</strong>只追加日志文件</p><h4 id="1-1-快照（Snapshot）"><a href="#1-1-快照（Snapshot）" class="headerlink" title="1.1 快照（Snapshot）"></a>1.1 快照（Snapshot）</h4><h5 id="1-特点"><a href="#1-特点" class="headerlink" title="1.特点"></a>1.特点</h5><blockquote><p>这种方式可以将某一时刻的所有数据都写入硬盘中，这是Redis的默认开启持久化的方式，保存的文件时以**.rdb**后缀的文件，所以这种方式也成为RDB方式。</p></blockquote><blockquote><p>官方说法叫快照持久化</p></blockquote><p><img src="https://s2.loli.net/2022/05/18/F62H8pT9SQ5XAcy.png" alt="image-20220512214029142"></p><h5 id="2-快照生成方式"><a href="#2-快照生成方式" class="headerlink" title="2.快照生成方式"></a>2.快照生成方式</h5><blockquote><p>客户端方式：BGSAVE和SAVE指令</p></blockquote><blockquote><p>服务器配置自动触发</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、客户端方式-BGSAVE</span><br><span class="line">-客户端可以使用BGSAVE命令来创建一个快照，当接收到客户端的BGSAVE命令时，redis会调用fork来创建一个子进程，然后子进程负责将快照写入磁盘中，而父进程则继续处理命令请求</span><br><span class="line"></span><br><span class="line">名词解释：*fork*。当一个进程创建子进程的时候，底层的操作系统会创建该进程的一个副本，在类似于unix系统中创建子进程的操作会进行优化：在刚开始的时候，父子进程共享相同内存，知道父进程或子进程对内存进行了写之后，对被写入的内存才会结束服务。</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/05/18/QLUB12mTMPFrE45.png" alt="image-20220512214729500"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2、客户端方式-SAVE</span><br><span class="line">-客户端还可以使用SAVE命令来创建一个快照，接收到SAVE命令的redis服务器在快照创建完毕之前将不再响应任何其他的命令</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/05/18/671QPakdDzq8Hfg.png" alt="image-20220512214914633"></p><p>注意：SAVE命令并不常用，使用SAVE命令在快照创建完毕之前，redis处于阻塞状态，无法对外服务</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">3、服务器配置-满足自动触发</span><br><span class="line">-如果用户在redis.conf中设置了save配置选项，redis会在save选项条件满足之后自动触发一次BGSAVE命令，如果设置多个save配置选项，当任意一个save配置选项条件满足，redis也会触发一次BGSAVE命令</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">4、服务器接收客户端shutdown指令</span><br><span class="line">-当redis通过shutdown指令接收到关闭服务器的请求时，会执行一个save命令，阻塞所有的客户端，不再执行客户端执行发送的任何命令，并且在save命令执行完毕之后关闭服务器</span><br></pre></td></tr></table></figure><h5 id="3-配置生成快照名称和位置"><a href="#3-配置生成快照名称和位置" class="headerlink" title="3.配置生成快照名称和位置"></a>3.配置生成快照名称和位置</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、修改生成快照名称</span><br><span class="line">-dbfilename dump.rbd</span><br><span class="line">2、修改生成位置</span><br><span class="line">-dir ./</span><br><span class="line"></span><br><span class="line">经过断电操作测试后，发现这个持久化并不是太好，可能会造成数据丢失问题（刚刚做完一次快照，又来一次写数据请求断电）</span><br></pre></td></tr></table></figure><h4 id="1-2-AOF只追加日志文件"><a href="#1-2-AOF只追加日志文件" class="headerlink" title="1.2 AOF只追加日志文件"></a>1.2 AOF只追加日志文件</h4><h5 id="1-特点-1"><a href="#1-特点-1" class="headerlink" title="1.特点"></a>1.特点</h5><p>这种方式可以将所有客户端执行的<strong>写命令</strong>记录到日志文件中，AOF持久化会被执行的写命令写到AOF的文件末尾，以此来记录数据发生的变化，因此只要redis从头到尾执行一次AOF文件所包含的所有写命令，就可以恢复AOF文件的记录的数据集</p><p><img src="https://s2.loli.net/2022/05/18/yJIqWEfzsv3Kajp.png"></p><h5 id="2-开启AOF持久化"><a href="#2-开启AOF持久化" class="headerlink" title="2.开启AOF持久化"></a>2.开启AOF持久化</h5><p>在开启redis的默认配置中AOF持久化机制是没有开启的，需要在配置中开启</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、开启AOF持久化</span><br><span class="line">-修改 appendonly yes 开启持久化</span><br><span class="line">-修改 appendfilename &quot;appendonly.aof&quot; 指定生成文件名称</span><br></pre></td></tr></table></figure><h5 id="3-日志追加频率"><a href="#3-日志追加频率" class="headerlink" title="3.日志追加频率"></a>3.日志追加频率</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1.always 【谨慎使用】</span><br><span class="line">- 说明: 每个redis写命令都要同步写入硬盘,严重降低redis速度</span><br><span class="line">- 解释: 如果用户使用了always选项,那么每个redis写命令都会被写入硬盘,从而将发生系统崩溃时出现的数据丢失减到最少;遗憾的是,因为这种同步策略需要对硬盘进行大量的写入操作,所以redis处理命令的速度会受到硬盘性能的限制;</span><br><span class="line">- 注意: 转盘式硬盘在这种频率下200左右个命令/s ; 固态硬盘(SSD) 几百万个命令/s;</span><br><span class="line">- 警告: 使用SSD用户请谨慎使用always选项,这种模式不断写入少量数据的做法有可能会引发严重的`写入放大`问题,导致将固态硬盘的寿命从原来的几年降低为几个月。</span><br><span class="line"></span><br><span class="line"># 2.everysec 【推荐默认】</span><br><span class="line">- 说明: 每秒执行一次同步显式的将多个写命令同步到磁盘</span><br><span class="line">- 解释： 为了兼顾数据安全和写入性能,用户可以考虑使用everysec选项,让redis每秒一次的频率对AOF文件进行同步;redis每秒同步一次AOF文件时性能和不使用任何持久化特性时的性能相差无几,而通过每秒同步一次AOF文件,redis可以保证,即使系统崩溃,用户最多丢失一秒之内产生的数据。 </span><br><span class="line"></span><br><span class="line"># 3.no【不推荐】</span><br><span class="line">- 说明: 由操作系统决定何时同步 </span><br><span class="line">- 解释：最后使用no选项,将完全有操作系统决定什么时候同步AOF日志文件,这个选项不会对redis性能带来影响但是系统崩溃时,会丢失不定数量的数据,甚至丢失全部数据，另外如果用户硬盘处理写入操作不够快的话,当缓冲区被等待写入硬盘数据填满时,redis会处于阻塞状态,并导致redis的处理命令请求的速度变慢。</span><br></pre></td></tr></table></figure><h4 id="1-3-AOF文件的重写"><a href="#1-3-AOF文件的重写" class="headerlink" title="1.3 AOF文件的重写"></a>1.3 AOF文件的重写</h4><h5 id="1-AOF带来的问题"><a href="#1-AOF带来的问题" class="headerlink" title="1. AOF带来的问题"></a>1. AOF带来的问题</h5><p>AOF的方式也同时带来了另一个问题。持久化文件会变得越来越大。例如我们调用incr test命令100次，文件中必须保存全部的100条命令，其实有99条都是多余的。因为要恢复数据库的状态其实文件中保存一条set test 100就够了。为了压缩AOF的持久化文件，Redis提供了AOF重写（ReWrite）机制。</p><h5 id="2-AOF重写"><a href="#2-AOF重写" class="headerlink" title="2. AOF重写"></a>2. AOF重写</h5><p>用来在一定程度上减小AOF文件的体积,并且还能保证数据不丢失</p><h5 id="3-触发重写方式"><a href="#3-触发重写方式" class="headerlink" title="3. 触发重写方式"></a>3. 触发重写方式</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1.客户端方式触发重写</span><br><span class="line">-执行BGREWRIEAOF命令  不会阻塞redis的服务</span><br><span class="line">2.服务器配置方式自动触发</span><br><span class="line">-配置redis.conf中的auto-aof-rewrite-percentage选项</span><br><span class="line">-如果设置auto-aof-rewrite-percentage值为100和auto-aof-rewrite-min-size 64mb,并且启用的AOF持久化时,那么当AOF文件体积大于64MB,并且AOF文件的体积比上一次重写之后体积大了至少一倍(100%)时,会自动触发,如果重写过于频繁,用户可以考虑将auto-aof-rewrite-percentage设置为更大</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/05/18/YiLCvp4arWw6eBE.png" alt="image-20220512225431013"></p><h5 id="4-重写原理"><a href="#4-重写原理" class="headerlink" title="4. 重写原理"></a>4. 重写原理</h5><p>从Redis 7.0.0开始,Redis使用了多部分AOF机制.也就是将原来的单个AOF文件拆分为基础文件(最多一个)和增量文件(可能不止一个).</p><p>基本文件表示重写AOF时存在的数据的初始(RDB或AOF格式)快照.</p><p>增量文件包含自创建最后一个基本AOF文件以来的增量更改.所有这些文件都放在一个单独的目录中,并由清单文件跟踪</p><p>从 Redis 7.0.0 开始，在调度 AOF 重写时，Redis 父进程会打开一个新的增量 AOF 文件继续写入。子进程执行重写逻辑并生成新的基础 AOF。Redis 将使用一个临时清单文件来跟踪新生成的基础文件和增量文件。当它们准备好后，Redis 会执行原子替换操作，使这个临时清单文件生效。为了避免在 AOF 重写重复失败和重试的情况下创建大量增量文件的问题，Redis 引入了 AOF 重写限制机制，以确保失败的 AOF 重写以越来越慢的速度重试。</p><h4 id="日志重写"><a href="#日志重写" class="headerlink" title="日志重写"></a>日志重写</h4><p>注意AOF文件的操作,并没有读取旧的AO文件,而是将整个内存中的数据库内容用命令的方式写了一个新的aof文件替换原有的文件这点和快照有点类似</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">重写流程</span><br><span class="line">-1 redis调用fork,现在又父子两个进程,子进程根据内存中的数据库快照,往临时文件中写入重建数据库状态的命令</span><br><span class="line">-2 父进程继续处理client请求,除了把写命令写入到原来的aof文件中.同时把接收到的写命令缓存起来.这样就能保证如果子进程重写失败的话,不会丢失数据</span><br><span class="line">-3 当子进程把快照内容写入己命令写到临时文件中后,子进程发信号通知父进程,然后父进程把缓存的命令也写到临时文件中</span><br><span class="line">-4 现在父进程可以使用临时文件替换旧的aof文件,并重命名,后面收到的写命令也开始往新的aof文件中追加.</span><br></pre></td></tr></table></figure><p><strong>Redis7.0.0之前：</strong></p><p><img src="https://s2.loli.net/2022/05/18/5dzkXMxvcObFf6A.png" alt="image-20220512225149515"></p><p><strong>Redis7.0.0之后：</strong></p><p><img src="https://s2.loli.net/2022/05/18/bovIWStHZL1hFaO.png" alt="image-20220513120013948"></p><h4 id="1-4-持久化总结"><a href="#1-4-持久化总结" class="headerlink" title="1.4 持久化总结"></a>1.4 持久化总结</h4><p>两种持久化方案既可以同时使用(aof),又可以单独使用,在某种情况下也可以都不使用,具体使用哪种持久化方案取决于用户的数据和用用决定.</p><p>无论使用AOF还是快照机制持久化,将数据持久化到硬盘都是有必要的,除了持久化之外,用户还应该对持久化的文件进行备份(异地备份)以最大安全保障数据的完整性.</p>]]></content>
    
    
    <summary type="html">对学习Redis的一些知识笔记</summary>
    
    
    
    <category term="数据库" scheme="http://example.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
    <category term="Redis" scheme="http://example.com/tags/Redis/"/>
    
  </entry>
  
</feed>
