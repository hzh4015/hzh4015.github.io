<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hive优化</title>
      <link href="/2022/06/07/Hive%E4%BC%98%E5%8C%96/"/>
      <url>/2022/06/07/Hive%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h2 id="Hive优化"><a href="#Hive优化" class="headerlink" title="Hive优化"></a>Hive优化</h2><h2 id="1-1-hive的随机抓取策略"><a href="#1-1-hive的随机抓取策略" class="headerlink" title="1.1    hive的随机抓取策略"></a>1.1    <strong>hive的随机抓取策略</strong></h2><blockquote><p>理论上来说，Hive中的所有sql都需要进行mapreduce，但是hive的抓取策略帮我们<br>省略掉了这个过程，把切片split的过程提前帮我们做了。<br>set hive.fetch.task.conversion&#x3D;none;<br>(一旦进行这么设置，select字段名也是需要进行mapreduce的过程，默认是more)</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Fetch</span>抓取的模式</span><br><span class="line">可以通过 <span class="keyword">set</span> hive.fetch.task.conversion查看，有以下<span class="number">3</span>种模式：</span><br><span class="line"></span><br><span class="line"><span class="keyword">none</span>：所有涉及hdfs的读取查询都走mapreduce任务；</span><br><span class="line">mininal：在进行简单的<span class="keyword">select</span> <span class="operator">*</span>，简单的过滤或涉及分区字段的过滤时走mr；</span><br><span class="line">more:在mininal模式的基础上，增加了针对查询语句字段进行一些别名的计算操作。</span><br><span class="line">以下HQL，mininal模式与more模式下都不会走mr任务:</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">sale_ord_id,</span><br><span class="line">store_id</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">test_table</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">dt <span class="operator">=</span> <span class="string">&#x27;2021-01-01&#x27;</span></span><br><span class="line"> limit <span class="number">10</span>;</span><br><span class="line"> </span><br><span class="line">以下HQL,mininal模式会走mr任务，more模式不会：</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">sale_ord_id,</span><br><span class="line">store_id,</span><br><span class="line">if(store_id <span class="operator">&gt;</span> <span class="number">20</span>,<span class="number">1</span>,<span class="number">0</span>) <span class="keyword">as</span> store_id_new</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">test_table</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">dt <span class="operator">=</span> <span class="string">&#x27;2021-01-01&#x27;</span></span><br><span class="line"> limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure><blockquote><p>查看怎么将一个sql转化成一个MR任务的<br>explain sql语句<br>例如：<br>explain select count(*) from stu_dy1_1;<br>更加详细的查看，例如：<br><strong>explain extended select count(*) from stu_dy1_1;</strong><br>当你输入一个sql语句的时候，hive会将对其关键字进行截串，截完串之后，变成<br>都是一些TOK开头的一些东西，然后经过这样的抽象语法树，再转成具体的查询块，<br>最后变成逻辑查询计划</p></blockquote><h2 id="1-2-本地运行模式"><a href="#1-2-本地运行模式" class="headerlink" title="1.2    本地运行模式"></a>1.2    本地运行模式</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">大多数的 Hadoop Job 是需要 Hadoop 提供的完整的可扩展性来处理大数据集的。不过，</span><br><span class="line">有时 Hive 的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能</span><br><span class="line">会比实际 job 的执行时间要多的多。对于大多数这种情况， Hive 可以通过本地模式在单台机</span><br><span class="line">器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</span><br><span class="line">用户可以通过设置 hive.exec.mode.local.auto 的值为 true ，来让 Hive 在适当的时候自动</span><br><span class="line">启动这个优化。</span><br><span class="line"></span><br><span class="line">本地模式运行比集群模式块很多，33秒的任务降到2秒</span><br><span class="line">更改为本地模式：</span><br><span class="line">hive&gt; set hive.exec.mode.local.auto=true</span><br><span class="line">注意：</span><br><span class="line">hive&gt; set hive.exec.mode.local.auto.inputbytes.max=134217728     ---&gt; 128M</span><br><span class="line">（默认值就是128）</span><br><span class="line">表示加载文件的最大值，若大于该配置仍然会以集群的方式去运行。</span><br><span class="line">97万行数据，50MB</span><br><span class="line">当我们开发或者测试阶段，可以去使用本地模式进行运行，默认是集群模式</span><br><span class="line">但是，这里有个问题，当我们去更改为本地模式的时候，在8088的页面上就看不到</span><br><span class="line">任务的执行情况了。</span><br><span class="line"></span><br><span class="line">测试：select count(*) from emp group by deptno;</span><br></pre></td></tr></table></figure><h2 id="1-3-并行计算"><a href="#1-3-并行计算" class="headerlink" title="1.3    并行计算"></a>1.3    <strong>并行计算</strong></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">通过设置以下参数开启并行模式（默认是false）</span><br><span class="line">set hive.exec.parallel=true;</span><br><span class="line"></span><br><span class="line">注意：hive.exec.parallel.thread.number</span><br><span class="line">(一次SQl计算中允许并行执行的job个数最大值，默认是8个)</span><br><span class="line"></span><br><span class="line">举例：</span><br><span class="line">select t1.n1,t2.n2 from (select count(ename) as n1 from emp) t1,(select count(dname) as n2 from dept) t2;</span><br><span class="line">注意，有时候开启并行计算运行时间并没有不开启的快，那是因为，资源的问题。</span><br><span class="line">需要两套资源，资源申请会浪费点时间，最多可以并行8个，默认是8个。</span><br><span class="line">所以，并行的越多，不一定是越快，因为它涉及到一个资源申请的策略。</span><br></pre></td></tr></table></figure><h2 id="1-4-严格模式-理解为增加一些限制"><a href="#1-4-严格模式-理解为增加一些限制" class="headerlink" title="1.4    严格模式(理解为增加一些限制)"></a>1.4    <strong>严格模式(理解为增加一些限制)</strong></h2><p>​    <strong>1.什么是Hive的严格模式</strong><br>​        hive中的一种模式,在该模式下禁止一些不好SQL的执行。</p><p>​    <strong>2.Hive的严格模式不允许哪些SQL执行</strong><br>​        <strong>2.1 禁止分区表全表扫描</strong><br>               分区表往往数据量大,如果不加分区查询会带来巨大的资源消耗 。例如以下分区表<br>               SELECT DISTINCT(planner_id) FROM fracture_ins WHERE planner_id&#x3D;5;</p><p>​                报错如下:<br>​               FAILED: Error in semantic analysis: No Partition Predicate Found for Alias “fracture_ins” Table “fracture_ins</p><p>​               解决如下:<br>​              SELECT DISTINCT(planner_id) FROM fracture_ins WHERE planner_id&#x3D;5 AND hit_date&#x3D;20120101;</p><p>​      <strong>2.2 禁止排序不加limit</strong><br>​        排序最终是要都进到一个Reduce中操作,防止reducer额外执行很长一段时间<br>​        SELECT * FROM fracture_ins WHERE hit_date&gt;2012 ORDER BY planner_id;<br>​        出现如下错误<br>​               FAILED: Error in semantic analysis: line 1:56 In strict mode,limit must be specified if ORDER BY is present planner_id<br>​        解决方案就是增加一个limit关键字：<br>​               hive&gt; SELECT * FROM fracture_ins WHERE hit_date&gt;2012 ORDER BY planner_id LIMIT 100000;</p><p>​      <strong>2.3 禁止笛卡尔积</strong><br>​          笛卡尔积是什么: A&#x3D;{a,b}, B&#x3D;{0,1,2}，则 A×B&#x3D;{(a, 0), (a, 1), (a, 2), (b, 0), (b, 1), (b, 2)}</p><p>​          SELECT * FROM fracture_act JOIN fracture_ads;<br>​        解决方法<br>​        SELECT * FROM fracture_act JOIN fracture_ads WHERE fracture_act.planner_id &#x3D; fracture_ads.planner_id;</p><p><strong>3.Hive的严格模式怎样开启</strong></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> 查看当前严格模式的状态</span><br><span class="line"><span class="keyword">set</span> hive.mapred.mode;</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 设置为严格模式</span><br><span class="line"><span class="keyword">set</span> hive.mapred.mode<span class="operator">=</span>strict;</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 设置为非严格模式</span><br><span class="line"><span class="keyword">set</span> hive.mapred.mode<span class="operator">=</span>nostrict;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">注意，这里的严格模式和动态分区的那个严格模式半毛钱关系没有）</span><br><span class="line">通过设置以下参数开启严格模式：</span><br><span class="line">set hive.mapred.mode=strict;</span><br><span class="line">(默认为：nonstrict非严格模式)</span><br><span class="line"></span><br><span class="line">查询限制：</span><br><span class="line">1、对于分区表，必须添加where对于分区字段的条件过滤</span><br><span class="line">2、order by 语句必须包含limit输出限制</span><br><span class="line">3、限制执行笛卡尔积的查询</span><br><span class="line">这些限制是帮助我们提高查询效率的。</span><br></pre></td></tr></table></figure><h2 id="1-5-Hive排序"><a href="#1-5-Hive排序" class="headerlink" title="1.5    Hive排序"></a>1.5    <strong>Hive排序</strong></h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> 对于查询结果做全排序，只允许有一个reduce处理</span><br><span class="line">（注意：它会把我们所有的字段或者查询结果全部放在一个reduce里进行处理</span><br><span class="line">当数据量较大时候，有可能reduce执行不完，所以，我们以后把这个给弃用掉）</span><br><span class="line"></span><br><span class="line"><span class="operator">*</span><span class="operator">*</span>   sort <span class="keyword">by</span> 对于单个reduce进行排序 但是我们将每个reduce里面进行排序，没有考虑到</span><br><span class="line">每个reduce之间的排序。所以我们引出下一个</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span>   distribute <span class="keyword">by</span> 分区排序，通常结合sort <span class="keyword">by</span>一起使用</span><br><span class="line">（distribute <span class="keyword">by</span> <span class="keyword">column</span> sort <span class="keyword">by</span> <span class="keyword">column</span> <span class="keyword">asc</span><span class="operator">|</span><span class="keyword">desc</span>）</span><br><span class="line"></span><br><span class="line">cluster <span class="keyword">by</span> 相当于distribute <span class="keyword">by</span> <span class="operator">+</span> sort <span class="keyword">by</span>  (注意，虽然是两个结合，但是我们也不去用它</span><br><span class="line">原因很简单，cluster <span class="keyword">by</span>不能通过<span class="keyword">asc</span> <span class="keyword">desc</span>的方式指定排序方式规则)</span><br></pre></td></tr></table></figure><h2 id="1-6-Hive-join数据倾斜"><a href="#1-6-Hive-join数据倾斜" class="headerlink" title="1.6    Hive join数据倾斜"></a>1.6    Hive join数据倾斜</h2><p>1、小表join小表 不管他</p><p>2、小表join大表   map-join</p><p>3、大表join大表  map-side</p><p>考虑会不会发生reduce,并且考虑reduce压力是否大（是否会出现某个reduce数据量庞大的情况）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">join</span>计算的时候，将小表（驱动表）放在<span class="keyword">join</span>的左边</span><br><span class="line">Map <span class="keyword">join</span>：在Map端完成<span class="keyword">join</span></span><br><span class="line">两种实现方式：</span><br><span class="line"><span class="number">1</span>、<span class="keyword">sql</span>方式，在<span class="keyword">sql</span>语句中添加Mapjoin标记（mapjoin hint）</span><br><span class="line"><span class="operator">&gt;&gt;</span>语法：</span><br><span class="line"><span class="keyword">select</span> <span class="comment">/*+MAPJOIN(smallTable)*/</span> smallTable.key bigTable.value <span class="keyword">from</span> smallTable <span class="keyword">join</span> bigTable <span class="keyword">on</span> smallTable.key<span class="operator">=</span>bigTable.key;</span><br><span class="line"><span class="number">2</span>、自动开启mapjoin</span><br><span class="line">通过修改以下配置启用自动的mapjoin：</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line">(注意：该参数为<span class="literal">true</span>的时候，Hive自动对左边的表统计量，如果</span><br><span class="line">是小表，就加入到内存，即对小表使用Mapjoin)</span><br><span class="line"></span><br><span class="line">相关配置参数</span><br><span class="line">　　hive.mapjoin.smalltable.filesize;(默认<span class="number">25</span>M,大表小表判断的阈值，如果表的大小小于该值则会被加载到内存中运行。)</span><br><span class="line">　　hive.ignore,mapjoin.hint;(默认值：<span class="literal">true</span>;是否忽略mapjoin hint的标记)</span><br><span class="line">　　hive.auto.convert.join.noconditionaltask;(默认值：<span class="literal">true</span>；将普通的<span class="keyword">join</span>转换为mapjoin时，是否将多个mapjoin转化为一个mapjoin)</span><br><span class="line">　　hive.auto.convert.join.noconditionaltask.size;(将多个mapjoin转化为一个mapjoin时，这个表的最大值)</span><br><span class="line"><span class="number">3</span>、尽可能使用相同的连接键，如果不同，多一个<span class="keyword">join</span>就会多开启一个mapreduce，执行速度变得慢。</span><br><span class="line"><span class="number">4</span>、大表<span class="keyword">join</span>大表（当两个都是大表的时候，只能发生reduce了，但是这里有两个优化策略）（面试的时候说，加分）</span><br><span class="line">　　a: 空key过滤:</span><br><span class="line">　　　　有时<span class="keyword">join</span>超时是因为某些key对应的数据太多,而相同key对应的数据都会发送到相同的 reducer上,从而导致内存不够。</span><br><span class="line">　　　　此时我们应该仔细分析这些异常的key,很多情况下,这些key对应的数据是异常数据,我们需要在<span class="keyword">SQL</span>语句中进行过滤。</span><br><span class="line">　　　　但是这个的前提条件是异常数据，但是我们一般拿到的数据都是经过ETL数据清洗过后的，一般影响不大，面试的时候可以说。</span><br><span class="line">　　b: 空key转换:</span><br><span class="line">　　　　有时虽然某个key为空对应的数据很多,但是相应的数据不是异常数据,必须要包含在<span class="keyword">join</span>的结果中,</span><br><span class="line">　　　　此时我们可以表a中key为空的字段赋随机的值,使得数据随机均匀地分不到不同的 reducer上。</span><br><span class="line">　　　　但是我们一般拿到的数据都是经过ETL数据清洗过后的，规则数据，一般影响不大，面试的时候可以说。</span><br><span class="line"><span class="number">5</span>、Map<span class="operator">-</span>Side聚合</span><br><span class="line">通过设置以下参数开启在Map端的聚合</span><br><span class="line"><span class="keyword">set</span> hive.map.aggr<span class="operator">=</span><span class="literal">true</span>;（一定要进行开启，虽然进行了两个mapreduce，但是当数据倾斜发生的时候，很多时候会根本跑不出结果，卡死在<span class="number">99</span><span class="operator">%</span>或者<span class="number">100</span><span class="operator">%</span>，慢总比出不来结果要好）！！！！！！！</span><br><span class="line">相关配置参数</span><br><span class="line">　　hive. groupby mapaggr. checkinterval;</span><br><span class="line">　　map端 igroup <span class="keyword">by</span>执行聚合时处理的多少行数据(默认:<span class="number">10000</span></span><br><span class="line">　　hive.map.aggr.hash.min.reduction;比例(若聚合之后的数据<span class="number">100</span>大该<span class="number">0.5</span>,map端聚合使用的内存的最大值</span><br><span class="line">　　hive.mapaggr.hashforce.flush.memory.threshold;map端做聚合操作是has表的最大可用内容,大于该值则会触发fush</span><br><span class="line">　　hive.groupby.skewindata<span class="operator">-</span>是否对 GroupBy产生的数据倾斜做优化,默认为<span class="literal">false</span>(十分重要！！！)</span><br><span class="line"><span class="number">6</span>、数据倾斜，尽可能地让我们的数据散列到不同的reduce里面去,负载均衡</span><br></pre></td></tr></table></figure><h2 id="1-7-合并小文件"><a href="#1-7-合并小文件" class="headerlink" title="1.7    合并小文件"></a>1.7    <strong>合并小文件</strong></h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">Hive优化</span><br><span class="line">合并小文件</span><br><span class="line">文件数目小,容易在文件存储端造成压力,给hdfs造成压力,影响效率</span><br><span class="line">设置合并属性</span><br><span class="line">　　是否合并map输出文件: hive.merge.mapfiles<span class="operator">=</span><span class="literal">true</span></span><br><span class="line">　　是否合并reduce输出文件: hive.merge.mapredfiles<span class="operator">=</span><span class="literal">true</span></span><br><span class="line">　　合并文件的大小: hive.merge.size.per.task<span class="operator">=</span><span class="number">256</span><span class="operator">*</span><span class="number">1000</span><span class="operator">*</span><span class="number">1000</span></span><br><span class="line">去重统计</span><br><span class="line">数据量小的时候无所谓,数据量大的情况下,由于 COUNT <span class="keyword">DISTINCT</span>操作需要用一个 Reduce Task来完成,</span><br><span class="line">这一个 Reduce需要处理的数据量太大,就会导致整个JOb很难完成,一般 COUNT <span class="keyword">DISTINCT</span>使用先 <span class="keyword">GROUP</span> <span class="keyword">BY</span>再COUNT的方式替换</span><br></pre></td></tr></table></figure><h2 id="1-8-控制map和reduce的数量-一般情况下我们不去动它"><a href="#1-8-控制map和reduce的数量-一般情况下我们不去动它" class="headerlink" title="1.8    控制map和reduce的数量(一般情况下我们不去动它)"></a>1.8    <strong>控制map和reduce的数量(一般情况下我们不去动它)</strong></h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">控制Hive中Map以及 Reduce的数量</span><br><span class="line">Map数量相关的参数</span><br><span class="line">mapred.max.split.size;一个split的最大值,即每个map处理文件的最大值</span><br><span class="line">mapred.min.split.size.per.node个节点上split的最小值</span><br><span class="line">mapred.min.split.size.per.rack一个机架上spit的最小值</span><br><span class="line">Reduce数量相关的参数</span><br><span class="line">mapred.reduce.tasks;强制指定reduce任务的数量</span><br><span class="line">hive.exec.reducers.bytes.per.reducer每个reduce任务处理的数据量</span><br><span class="line">hive.exec.reducers.max每个任务最大的reduce数</span><br></pre></td></tr></table></figure><h2 id="1-9-JVM重用"><a href="#1-9-JVM重用" class="headerlink" title="1.9    JVM重用"></a>1.9    <strong>JVM重用</strong></h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">当我们的小文件个数过多，task个数过多，需要申请的资源过多的时候，我们可以先申请一部分资源，全部执行完毕后再释放，</span><br><span class="line">比我们申请一个释放一个要快。</span><br><span class="line">通过 <span class="keyword">set</span> mapred.job.reuse.jvm.num.tasks<span class="operator">=</span>n;来设置</span><br><span class="line">（n为task插槽个数）</span><br><span class="line">缺点：</span><br><span class="line">设置开启后，task插槽会一直占用资源，无论是否有task进行，直到所有的task,</span><br><span class="line">即整个job全部执行完毕后，才会释放所有的task插槽，所以我们要合理地设置这个n</span><br><span class="line">(比如，我们设置申请了<span class="number">10</span>个，但是现在来了<span class="number">6</span>个，剩下<span class="number">4</span>个插槽会在job全部执行完毕之前一直占用资源)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>连续登陆问题</title>
      <link href="/2022/06/07/%E8%BF%9E%E7%BB%AD%E7%99%BB%E5%BD%95%E9%97%AE%E9%A2%98/"/>
      <url>/2022/06/07/%E8%BF%9E%E7%BB%AD%E7%99%BB%E5%BD%95%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h4 id="连续登陆问题"><a href="#连续登陆问题" class="headerlink" title="连续登陆问题"></a>连续登陆问题</h4><blockquote><p>在电商、物流和银行可能经常会遇到这样的需求：统计用户连续交易的总额、连续登陆天数、连续登陆开始和结束时间、间隔天数等</p></blockquote><h5 id="数据："><a href="#数据：" class="headerlink" title="数据："></a>数据：</h5><blockquote><p>注意：每个用户每天可能会有多条记录</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iddatestr  amount</span><br><span class="line">1,2019-02-08,6214.23 </span><br><span class="line">1,2019-02-08,6247.32 </span><br><span class="line">1,2019-02-09,85.63 </span><br><span class="line">1,2019-02-09,967.36 </span><br><span class="line">1,2019-02-10,85.69 </span><br><span class="line">1,2019-02-12,769.85 </span><br><span class="line">1,2019-02-13,943.86 </span><br><span class="line">1,2019-02-14,538.42</span><br><span class="line">1,2019-02-15,369.76</span><br><span class="line">1,2019-02-16,369.76</span><br><span class="line">1,2019-02-18,795.15</span><br><span class="line">1,2019-02-19,715.65</span><br><span class="line">1,2019-02-21,537.71</span><br><span class="line">2,2019-02-08,6214.23 </span><br><span class="line">2,2019-02-08,6247.32 </span><br><span class="line">2,2019-02-09,85.63 </span><br><span class="line">2,2019-02-09,967.36 </span><br><span class="line">2,2019-02-10,85.69 </span><br><span class="line">2,2019-02-12,769.85 </span><br><span class="line">2,2019-02-13,943.86 </span><br><span class="line">2,2019-02-14,943.18</span><br><span class="line">2,2019-02-15,369.76</span><br><span class="line">2,2019-02-18,795.15</span><br><span class="line">2,2019-02-19,715.65</span><br><span class="line">2,2019-02-21,537.71</span><br><span class="line">3,2019-02-08,6214.23 </span><br><span class="line">3,2019-02-08,6247.32 </span><br><span class="line">3,2019-02-09,85.63 </span><br><span class="line">3,2019-02-09,967.36 </span><br><span class="line">3,2019-02-10,85.69 </span><br><span class="line">3,2019-02-12,769.85 </span><br><span class="line">3,2019-02-13,943.86 </span><br><span class="line">3,2019-02-14,276.81</span><br><span class="line">3,2019-02-15,369.76</span><br><span class="line">3,2019-02-16,369.76</span><br><span class="line">3,2019-02-18,795.15</span><br><span class="line">3,2019-02-19,715.65</span><br><span class="line">3,2019-02-21,537.71</span><br></pre></td></tr></table></figure><h5 id="建表语句"><a href="#建表语句" class="headerlink" title="建表语句"></a>建表语句</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> deal_tb(</span><br><span class="line">    id string</span><br><span class="line">    ,datestr string</span><br><span class="line">    ,amount string</span><br><span class="line">)<span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br></pre></td></tr></table></figure><h5 id="计算逻辑"><a href="#计算逻辑" class="headerlink" title="计算逻辑"></a>计算逻辑</h5><ul><li>先按用户和日期分组求和，使每个用户每天只有一条数据</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> id,datestr,<span class="built_in">sum</span>(amount) <span class="keyword">as</span> sum_amount <span class="keyword">from</span> deal_tb <span class="keyword">group</span> <span class="keyword">by</span> id,datestr;</span><br></pre></td></tr></table></figure><ul><li>根据用户ID分组按日期排序，将日期和分组序号相减得到连续登陆的开始日期，如果开始日期相同说明连续登陆</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> tt1.id,tt1.datestr,tt1.sum_amount,date_sub(tt1.datestr,tt1.rn) <span class="keyword">as</span> grp <span class="keyword">from</span> (<span class="keyword">select</span> t1.id <span class="keyword">as</span> id,t1.datestr <span class="keyword">as</span> datestr,t1.sum_amount <span class="keyword">as</span> sum_amount,<span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> t1.id <span class="keyword">order</span> <span class="keyword">by</span> t1.datestr) <span class="keyword">as</span> rn <span class="keyword">from</span> (<span class="keyword">select</span> id,datestr,<span class="built_in">sum</span>(amount) <span class="keyword">as</span> sum_amount <span class="keyword">from</span> deal_tb <span class="keyword">group</span> <span class="keyword">by</span> id,datestr) t1) tt1;</span><br></pre></td></tr></table></figure><ul><li><em>datediff(string end_date,string start_date);</em> 等于0说明连续登录</li><li>统计用户连续交易的总额、连续登陆天数、连续登陆开始和结束时间、间隔天数</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ttt1.id,ttt1.grp,round(<span class="built_in">sum</span>(ttt1.sum_amount),<span class="number">2</span>) <span class="keyword">as</span> user_sum_amount,<span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">as</span> user_days,<span class="built_in">min</span>(ttt1.datestr) <span class="keyword">as</span> user_start_date,<span class="built_in">max</span>(ttt1.datestr) <span class="keyword">as</span> user_end_date,datediff(ttt1.grp,<span class="built_in">lag</span>(ttt1.grp,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> ttt1.id <span class="keyword">order</span> <span class="keyword">by</span> ttt1.grp)) <span class="keyword">as</span> interval_days <span class="keyword">from</span> (<span class="keyword">select</span> tt1.id <span class="keyword">as</span> id,tt1.datestr <span class="keyword">as</span> datestr,tt1.sum_amount <span class="keyword">as</span> sum_amount,date_sub(tt1.datestr,tt1.rn) <span class="keyword">as</span> grp <span class="keyword">from</span> (<span class="keyword">select</span> t1.id <span class="keyword">as</span> id,t1.datestr <span class="keyword">as</span> datestr,t1.sum_amount <span class="keyword">as</span> sum_amount,<span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> t1.id <span class="keyword">order</span> <span class="keyword">by</span> t1.datestr) <span class="keyword">as</span> rn <span class="keyword">from</span> (<span class="keyword">select</span> id,datestr,<span class="built_in">sum</span>(amount) <span class="keyword">as</span> sum_amount <span class="keyword">from</span> deal_tb <span class="keyword">group</span> <span class="keyword">by</span> id,datestr) t1) tt1) ttt1 <span class="keyword">group</span> <span class="keyword">by</span> ttt1.id,ttt1.grp;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> ttt1.id, ttt1.grp</span><br><span class="line">, round(<span class="built_in">sum</span>(ttt1.sum_amount), <span class="number">2</span>) <span class="keyword">AS</span> user_sum_amount</span><br><span class="line">, <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">AS</span> user_days, <span class="built_in">min</span>(ttt1.datestr) <span class="keyword">AS</span> user_start_date</span><br><span class="line">, <span class="built_in">max</span>(ttt1.datestr) <span class="keyword">AS</span> user_end_date</span><br><span class="line">, datediff(ttt1.grp, <span class="built_in">lag</span>(ttt1.grp, <span class="number">1</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> ttt1.id <span class="keyword">ORDER</span> <span class="keyword">BY</span> ttt1.grp)) <span class="keyword">AS</span> interval_days</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> tt1.id <span class="keyword">AS</span> id, tt1.datestr <span class="keyword">AS</span> datestr, tt1.sum_amount <span class="keyword">AS</span> sum_amount</span><br><span class="line">, date_sub(tt1.datestr, tt1.rn) <span class="keyword">AS</span> grp</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> t1.id <span class="keyword">AS</span> id, t1.datestr <span class="keyword">AS</span> datestr, t1.sum_amount <span class="keyword">AS</span> sum_amount, <span class="built_in">row_number</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> t1.id <span class="keyword">ORDER</span> <span class="keyword">BY</span> t1.datestr) <span class="keyword">AS</span> rn</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> id, datestr, <span class="built_in">sum</span>(amount) <span class="keyword">AS</span> sum_amount</span><br><span class="line"><span class="keyword">FROM</span> deal_tb</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> id, datestr</span><br><span class="line">) t1</span><br><span class="line">) tt1</span><br><span class="line">) ttt1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> ttt1.id, ttt1.grp;</span><br></pre></td></tr></table></figure><ul><li>结果</li></ul><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">12019-02-0713600.2332019-02-082019-02-10 NULL</span><br><span class="line">12019-02-082991.65052019-02-122019-02-161</span><br><span class="line">12019-02-091510.822019-02-182019-02-191</span><br><span class="line">12019-02-10537.7112019-02-212019-02-211</span><br><span class="line">22019-02-0713600.2332019-02-082019-02-10 NULL</span><br><span class="line">22019-02-083026.64942019-02-122019-02-151</span><br><span class="line">22019-02-101510.822019-02-182019-02-192</span><br><span class="line">22019-02-11537.7112019-02-212019-02-211</span><br><span class="line">32019-02-0713600.2332019-02-082019-02-10 NULL</span><br><span class="line">32019-02-082730.0452019-02-122019-02-161</span><br><span class="line">32019-02-091510.822019-02-182019-02-191</span><br><span class="line">32019-02-10537.7112019-02-212019-02-211</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的基本操作-2</title>
      <link href="/2022/06/02/Hive%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-2/"/>
      <url>/2022/06/02/Hive%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-2/</url>
      
        <content type="html"><![CDATA[<h2 id="1、Hive分区"><a href="#1、Hive分区" class="headerlink" title="1、Hive分区"></a>1、Hive分区</h2><blockquote><p>在大数据中，最常见的一种思想就是<strong>分治</strong>，我们可以<strong>把大的文件切割划分成一个个的小的文件</strong>，这样每次操作一个个小的文件就会很容易了，同样的道理，在hive当中也是支持这种思想的，就是我们可以把大的数据，按照每天或者每小时切分成一个个小的文件，这样去操作小的文件就会容易很多了。</p><p>假如现在我们公司一天产生3亿的数据量，那么为了方便管理和查询，就做以下的事情。</p><p>​        1）建立分区（可按照日期，部门等等具体业务分区）</p><p>​        2）分门别类的管理</p></blockquote><p><img src="https://s2.loli.net/2022/06/03/QdRsgvWS7yYnN13.png" alt="hive分区的种类.png"></p><h3 id="1-2-静态分区（SP）"><a href="#1-2-静态分区（SP）" class="headerlink" title="1.2    静态分区（SP）"></a>1.2    静态分区（SP）</h3><blockquote><p>静态分区（SP）static partition–partition by (字段 类型)</p><p>​        <strong>借助于物理的文件夹分区，实现快速检索的目的。</strong></p><p>​        <strong>一般对于查询比较频繁的列设置为分区列。</strong></p><p>​        <strong>分区查询的时候直接把对应分区中所有数据放到对应的文件夹中</strong>。</p></blockquote><blockquote><p>创建单分区表语法：</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> t_student (</span><br><span class="line">sno <span class="type">int</span>,</span><br><span class="line">sname string</span><br><span class="line">) partitioned <span class="keyword">by</span>(grade <span class="type">int</span>)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line"><span class="comment">--  分区的字段不要和表的字段相同。相同会报错error10035</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">1</span>,xiaohu01,<span class="number">1</span></span><br><span class="line"><span class="number">2</span>,xiaohu02,<span class="number">1</span></span><br><span class="line"><span class="number">3</span>,xiaohu03,<span class="number">1</span></span><br><span class="line"><span class="number">4</span>,xiaohu04,<span class="number">1</span></span><br><span class="line"><span class="number">5</span>,xiaohu05,<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">6</span>,xiaohu06,<span class="number">2</span></span><br><span class="line"><span class="number">7</span>,xiaohu07,<span class="number">2</span></span><br><span class="line"><span class="number">8</span>,xiaohu08,<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">9</span>,xiaohu09,<span class="number">3</span></span><br><span class="line"><span class="number">10</span>,xiaohu10,<span class="number">3</span></span><br><span class="line"><span class="number">11</span>,xiaohu11,<span class="number">3</span></span><br><span class="line"><span class="number">12</span>,xiaohu12,<span class="number">3</span></span><br><span class="line"><span class="number">13</span>,xiaohu13,<span class="number">3</span></span><br><span class="line"><span class="number">14</span>,xiaohu14,<span class="number">3</span></span><br><span class="line"><span class="number">15</span>,xiaohu15,<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">16</span>,xiaohu16,<span class="number">4</span></span><br><span class="line"><span class="number">17</span>,xiaohu17,<span class="number">4</span></span><br><span class="line"><span class="number">18</span>,xiaohu18,<span class="number">4</span></span><br><span class="line"><span class="number">19</span>,xiaohu19,<span class="number">4</span></span><br><span class="line"><span class="number">20</span>,xiaohu20,<span class="number">4</span></span><br><span class="line"><span class="number">21</span>,xiaohu21,<span class="number">4</span></span><br><span class="line"><span class="comment">-- 载入数据</span></span><br><span class="line"><span class="comment">-- 将相应年级一次导入</span></span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/usr/local/soft/shujia/student1.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> t_student <span class="keyword">partition</span>(grade<span class="operator">=</span><span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 演示多拷贝一行上传，分区的列的值是分区的值，不是原来的值</span></span><br></pre></td></tr></table></figure><blockquote><p>静态多分区表语法：</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> t_teacher (</span><br><span class="line">tno <span class="type">int</span>,</span><br><span class="line">tname string</span><br><span class="line">) partitioned <span class="keyword">by</span>(grade <span class="type">int</span>,clazz <span class="type">int</span>)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--注意：前后两个分区的关系为父子关系，也就是grade文件夹下面有多个clazz子文件夹。</span></span><br><span class="line"><span class="number">1</span>,xiaoge01,<span class="number">1</span>,<span class="number">1</span></span><br><span class="line"><span class="number">2</span>,xiaoge02,<span class="number">1</span>,<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">3</span>,xiaoge03,<span class="number">1</span>,<span class="number">2</span></span><br><span class="line"><span class="number">4</span>,xiaoge04,<span class="number">1</span>,<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">5</span>,xiaoge05,<span class="number">1</span>,<span class="number">3</span></span><br><span class="line"><span class="number">6</span>,xiaoge06,<span class="number">1</span>,<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">7</span>,xiaoge07,<span class="number">2</span>,<span class="number">1</span></span><br><span class="line"><span class="number">8</span>,xiaoge08,<span class="number">2</span>,<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">9</span>,xiaoge09,<span class="number">2</span>,<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--载入数据</span></span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/usr/local/soft/shujia/teacher11.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> t_teacher <span class="keyword">partition</span>(grade<span class="operator">=</span><span class="number">1</span>,clazz<span class="operator">=</span><span class="number">1</span>);</span><br></pre></td></tr></table></figure><blockquote><p>分区表查询</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_student <span class="keyword">where</span> grade <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 全表扫描，不推荐，效率低</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> students_pt1;</span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 使用<span class="keyword">where</span>条件进行分区裁剪，避免了全表扫描，效率高</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> students_pt1 <span class="keyword">where</span> grade <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 也可以在<span class="keyword">where</span>条件中使用非等值判断</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> students_pt1 <span class="keyword">where</span> grade<span class="operator">&lt;</span><span class="number">3</span> <span class="number">1</span> <span class="keyword">and</span> grade<span class="operator">&gt;=</span><span class="number">1</span>;</span><br></pre></td></tr></table></figure><blockquote><p>查看分区</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> partitions t_student;</span><br></pre></td></tr></table></figure><blockquote><p>添加分区</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t_student <span class="keyword">add</span> <span class="keyword">partition</span> (grade<span class="operator">=</span><span class="number">5</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t_student <span class="keyword">add</span> <span class="keyword">partition</span> (grade<span class="operator">=</span><span class="number">5</span>) location <span class="string">&#x27;指定数据文件的路径&#x27;</span>;</span><br></pre></td></tr></table></figure><blockquote><p>删除分区</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t_student <span class="keyword">drop</span> <span class="keyword">partition</span> (grade<span class="operator">=</span><span class="number">5</span>);</span><br></pre></td></tr></table></figure><h3 id="1-3-动态分区（DP）"><a href="#1-3-动态分区（DP）" class="headerlink" title="1.3    动态分区（DP）"></a>1.3    动态分区（DP）</h3><ul><li>动态分区（DP）dynamic partition</li><li>静态分区与动态分区的<strong>主要区别在于静态分区是手动指定，而动态分区是通过数据来进行判断。</strong></li><li>详细来说，静态分区的列是在编译时期通过用户传递来决定的；<strong>动态分区只有在SQL执行时才能决定</strong>。</li></ul><blockquote><p>开启动态分区首先要在hive会话中设置如下的参数</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 表示开启动态分区</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">set</span> hive.exec.dynamic.partition<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"># 表示动态分区模式：strict（需要配合静态分区一起使用）、nostrict</span><br><span class="line"># strict： <span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> students_pt <span class="keyword">partition</span>(dt<span class="operator">=</span><span class="string">&#x27;anhui&#x27;</span>,pt) <span class="keyword">select</span> ......,pt <span class="keyword">from</span> students;</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">set</span> hive.exec.dynamic.partition.mode<span class="operator">=</span>nonstrict;</span><br><span class="line"></span><br><span class="line"># 表示支持的最大的分区数量为<span class="number">1000</span>，可以根据业务自己调整</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">set</span> hive.exec.max.dynamic.partitions.pernode<span class="operator">=</span><span class="number">1000</span>;</span><br></pre></td></tr></table></figure><blockquote><p>其余的参数详细配置如下</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">设置为<span class="literal">true</span>表示开启动态分区的功能（默认为<span class="literal">false</span>）</span><br><span class="line"><span class="comment">--hive.exec.dynamic.partition=true;</span></span><br><span class="line"></span><br><span class="line">设置为nonstrict，表示允许所有分区都是动态的（默认为strict）</span><br><span class="line"><span class="comment">-- hive.exec.dynamic.partition.mode=nonstrict; </span></span><br><span class="line"></span><br><span class="line">每个mapper或reducer可以创建的最大动态分区个数(默认为<span class="number">100</span>) </span><br><span class="line">比如：源数据中包含了一年的数据，即<span class="keyword">day</span>字段有<span class="number">365</span>个值，那么该参数就需要设置成大于<span class="number">365</span>，如果使用默认值<span class="number">100</span>，则会报错</span><br><span class="line"><span class="comment">--hive.exec.max.dynamic.partition.pernode=100; </span></span><br><span class="line"></span><br><span class="line">一个动态分区创建可以创建的最大动态分区个数（默认值<span class="number">1000</span>）</span><br><span class="line"><span class="comment">--hive.exec.max.dynamic.partitions=1000;</span></span><br><span class="line"></span><br><span class="line">全局可以创建的最大文件个数（默认值<span class="number">100000</span>）</span><br><span class="line"><span class="comment">--hive.exec.max.created.files=100000; </span></span><br><span class="line"></span><br><span class="line">当有空分区产生时，是否抛出异常（默认<span class="literal">false</span>） </span><br><span class="line"><span class="comment">-- hive.error.on.empty.partition=false;  </span></span><br></pre></td></tr></table></figure><ul><li>案例1： 动态插入学生年级班级信息</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--创建分区表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> t_student_d (</span><br><span class="line">sno <span class="type">int</span>,</span><br><span class="line">sname string</span><br><span class="line">) partitioned <span class="keyword">by</span> (grade <span class="type">int</span>,clazz <span class="type">int</span>)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--创建外部表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> t_student_e (</span><br><span class="line">sno <span class="type">int</span>,</span><br><span class="line">sname string,</span><br><span class="line">grade <span class="type">int</span>,</span><br><span class="line">clazz <span class="type">int</span></span><br><span class="line">) </span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">location &quot;/shujia/student&quot;;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">数据：</span><br><span class="line"></span><br><span class="line">1,xiaohu01,1,1</span><br><span class="line">2,xiaohu02,1,1</span><br><span class="line">3,xiaohu03,1,1</span><br><span class="line">4,xiaohu04,1,2</span><br><span class="line">5,xiaohu05,1,2</span><br><span class="line">6,xiaohu06,2,3</span><br><span class="line">7,xiaohu07,2,3</span><br><span class="line">8,xiaohu08,2,3</span><br><span class="line">9,xiaohu09,3,3</span><br><span class="line">10,xiaohu10,3,3</span><br><span class="line">11,xiaohu11,3,3</span><br><span class="line">12,xiaohu12,3,4</span><br><span class="line">13,xiaohu13,3,4</span><br><span class="line">14,xiaohu14,3,4</span><br><span class="line">15,xiaohu15,3,4</span><br><span class="line">16,xiaohu16,4,4</span><br><span class="line">17,xiaohu17,4,4</span><br><span class="line">18,xiaohu18,4,5</span><br><span class="line">19,xiaohu19,4,5</span><br><span class="line">20,xiaohu20,4,5</span><br><span class="line">21,xiaohu21,4,5</span><br></pre></td></tr></table></figure><blockquote><p>如果静态分区的话，我们插入数据必须指定分区的值。</p><p>如果想要插入多个班级的数据，我要写很多SQL并且执行24次很麻烦。</p><p>而且静态分区有可能会产生数据错误问题</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 会报错 </span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t_student_d <span class="keyword">partition</span> (grade<span class="operator">=</span><span class="number">1</span>) <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_student_e <span class="keyword">where</span> grade<span class="operator">=</span><span class="number">1</span>;</span><br></pre></td></tr></table></figure><blockquote><p>如果使用动态分区，动态分区会根据select的结果自动判断数据应该load到哪儿分区去。</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t_student_d <span class="keyword">partition</span> (grade,clazz) <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_student_e;</span><br></pre></td></tr></table></figure><blockquote><p>优点：不用手动指定了，自动会对数据进行分区</p><p>缺点：可能会出现数据倾斜</p></blockquote><h2 id="2、Hive分桶"><a href="#2、Hive分桶" class="headerlink" title="2、Hive分桶"></a>2、Hive分桶</h2><h3 id="2-1-业务场景"><a href="#2-1-业务场景" class="headerlink" title="2.1    业务场景"></a>2.1    业务场景</h3><blockquote><p>数据分桶的适用场景：<br>        分区提供了一个隔离数据和优化查询的便利方式，不过并非所有的数据都可形成合理的分区，尤其是需要确定合适大小的分区划分方式<br>        不合理的数据分区划分方式可能导致有的分区数据过多，而某些分区没有什么数据的尴尬情况<br>        分桶是将数据集分解为更容易管理的若干部分的另一种技术。<br>        分桶就是将数据按照字段进行划分，可以将数据按照字段划分到多个文件当中去。</p></blockquote><h3 id="2-2-数据分桶原理"><a href="#2-2-数据分桶原理" class="headerlink" title="2.2    数据分桶原理"></a>2.2    数据分桶原理</h3><ul><li>Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。<ul><li>bucket num &#x3D; hash_function(bucketing_column) mod num_buckets</li><li>列的值做哈希取余 决定数据应该存储到哪个桶</li></ul></li></ul><h3 id="2-3-数据分桶优势"><a href="#2-3-数据分桶优势" class="headerlink" title="2.3    数据分桶优势"></a>2.3    数据分桶优势</h3><blockquote><p><strong>方便抽样</strong></p><p>​        使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便</p><p><strong>提高join查询效率</strong></p><p>​        获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。</p></blockquote><h3 id="2-4-分桶实战"><a href="#2-4-分桶实战" class="headerlink" title="2.4    分桶实战"></a>2.4    分桶实战</h3><blockquote><p>​    首先，分区和分桶是两个不同的概念，很多资料上说需要先分区在分桶，其实不然，分区是对数据进行划分，而分桶是对文件进行划分。</p><p>​    当我们的分区之后，最后的文件还是很大怎么办，就引入了分桶的概念。</p><p>将这个比较大的文件再分成若干个小文件进行存储，我们再去查询的时候，在这个小范围的文件中查询就会快很多。</p><p>​        对于hive中的每一张表、分区都可以进一步的进行分桶。</p><p>​        当然，分桶不是说将文件随机进行切分存储，而是有规律的进行存储。在看完下面的例子后进行解释，现在干巴巴的解释也不太好理解。它是由列的哈希值除以桶的个数来决定每条数据划分在哪个桶中。</p><p>创建顺序和分区一样，创建的方式不一样。</p></blockquote><blockquote><p><strong>首先我们需要开启分桶的支持</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">（依然十分重要，不然无法进行分桶操作！！！！）</span><br><span class="line"><span class="keyword">set</span> hive.enforce.bucketing<span class="operator">=</span><span class="literal">true</span>; </span><br></pre></td></tr></table></figure><blockquote><p><strong>数据准备（id,name,age）</strong></p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1,tom,11</span><br><span class="line">2,cat,22</span><br><span class="line">3,dog,33</span><br><span class="line">4,hive,44</span><br><span class="line">5,hbase,55</span><br><span class="line">6,mr,66</span><br><span class="line">7,alice,77</span><br><span class="line">8,scala,88</span><br></pre></td></tr></table></figure><blockquote><p><strong>创建一个普通的表</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> psn31</span><br><span class="line">(</span><br><span class="line">id <span class="type">int</span>,</span><br><span class="line">name string,</span><br><span class="line">age <span class="type">int</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br></pre></td></tr></table></figure><blockquote><p><strong>将数据load到这张表中</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;文件在Linux上的绝对路径&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> psn31;</span><br></pre></td></tr></table></figure><blockquote><p><strong>创建分桶表</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> psn_bucket</span><br><span class="line">(</span><br><span class="line">id <span class="type">int</span>,</span><br><span class="line">name string,</span><br><span class="line">age <span class="type">int</span></span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span>(age) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br></pre></td></tr></table></figure><blockquote><p><strong>将数据insert到表psn_bucket中</strong></p><p><strong>(注意：这里和分区表插入数据有所区别，分区表需要select 和指定分区，而分桶则不需要)</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> psn_bucket <span class="keyword">select</span> id,name,age <span class="keyword">from</span> psn31;</span><br></pre></td></tr></table></figure><blockquote><p><strong>在HDFS上查看数据</strong></p></blockquote><p><img src="https://s2.loli.net/2022/06/08/xOIdzYoEqhRLe4g.png" alt="image-20220601223434297.png"></p><blockquote><p><strong>查询数据</strong></p><p><strong>我们在linux中使用Hadoop的命令查看一下（与我们猜想的顺序一致）</strong></p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop fs -cat /user/hive/warehouse/bigdata17.db/psn_bucket/*</span><br></pre></td></tr></table></figure><blockquote><p>这里设置的桶的个数是4 数据按照 年龄%4 进行放桶(文件)<br>11%4 &#x3D;&#x3D; 3 —–&gt; 000003_0<br>22%4 &#x3D;&#x3D; 2 —–&gt; 000002_0<br>33%4 &#x3D;&#x3D; 1 —–&gt; 000001_0<br>44%4 &#x3D;&#x3D; 0 —–&gt; 000000_0<br>…以此类推</p></blockquote><p><img src="https://s2.loli.net/2022/06/08/smJGetELUTlaIuo.png" alt="分桶逻辑发逻辑.png"></p><blockquote><p>在Hive进行查询</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y)</span></span><br><span class="line"><span class="comment">-- 分桶语句中的分母表示的是数据将会被散列的桶的个数，分子表示将会选择的桶的个数。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- x表示从哪个bucket开始抽取。</span></span><br><span class="line"><span class="comment">-- 例如，table总bucket数为32，tablesample(bucket 2 out of 2)</span></span><br><span class="line"><span class="comment">-- 表示总共抽取（2/2=）1个bucket的数据，分别为第2个bucket和第（2+2=）4个bucket的数据</span></span><br><span class="line"><span class="comment">-- y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。</span></span><br><span class="line"><span class="comment">-- 例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> psn_bucket <span class="keyword">tablesample</span>(bucket <span class="number">2</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">2</span>);</span><br><span class="line">随机取值（设置因子，桶的个数<span class="operator">/</span>因子）</span><br><span class="line">这里就是取<span class="number">2</span>号桶和<span class="number">4</span>号桶，取<span class="number">2</span>个</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> psn_bucket <span class="keyword">tablesample</span>(bucket <span class="number">2</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span>);</span><br><span class="line">随机取值（设置因子，桶的个数<span class="operator">/</span>因子）</span><br><span class="line">这里就是取<span class="number">2</span>号桶，取一个</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> psn_bucket <span class="keyword">tablesample</span>(bucket <span class="number">2</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">8</span>);</span><br><span class="line">随机取值（设置倍数，倍数<span class="operator">/</span>桶的个数）</span><br><span class="line">这里就是取<span class="number">2</span>号桶 <span class="number">1</span><span class="operator">/</span><span class="number">2</span>个数据</span><br><span class="line">取出来是一条数据</span><br></pre></td></tr></table></figure><h2 id="3、Hive-JDBC"><a href="#3、Hive-JDBC" class="headerlink" title="3、Hive JDBC"></a>3、Hive JDBC</h2><h5 id="启动hiveserver2"><a href="#启动hiveserver2" class="headerlink" title="启动hiveserver2"></a>启动hiveserver2</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive --service hiveserver2 &amp;</span><br><span class="line">或者</span><br><span class="line">hiveserver2 &amp;</span><br></pre></td></tr></table></figure><h5 id="新建maven项目并添加两个依赖"><a href="#新建maven项目并添加两个依赖" class="headerlink" title="新建maven项目并添加两个依赖"></a>新建maven项目并添加两个依赖</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.7.6&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-jdbc --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.2.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h5 id="编写JDBC代码"><a href="#编写JDBC代码" class="headerlink" title="编写JDBC代码"></a>编写JDBC代码</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HiveJDBC</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ClassNotFoundException, SQLException &#123;</span><br><span class="line">        Class.forName(<span class="string">&quot;org.apache.hive.jdbc.HiveDriver&quot;</span>);</span><br><span class="line">        <span class="type">Connection</span> <span class="variable">conn</span> <span class="operator">=</span> DriverManager.getConnection(<span class="string">&quot;jdbc:hive2://master:10000/bigdata17&quot;</span>);</span><br><span class="line">        <span class="type">Statement</span> <span class="variable">stat</span> <span class="operator">=</span> conn.createStatement();</span><br><span class="line">        <span class="type">ResultSet</span> <span class="variable">rs</span> <span class="operator">=</span> stat.executeQuery(<span class="string">&quot;select * from students limit 10&quot;</span>);</span><br><span class="line">        <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">id</span> <span class="operator">=</span> rs.getInt(<span class="number">1</span>);</span><br><span class="line">            <span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span> rs.getString(<span class="number">2</span>);</span><br><span class="line">            <span class="type">int</span> <span class="variable">age</span> <span class="operator">=</span> rs.getInt(<span class="number">3</span>);</span><br><span class="line">            <span class="type">String</span> <span class="variable">gender</span> <span class="operator">=</span> rs.getString(<span class="number">4</span>);</span><br><span class="line">            <span class="type">String</span> <span class="variable">clazz</span> <span class="operator">=</span> rs.getString(<span class="number">5</span>);</span><br><span class="line">            System.out.println(id + <span class="string">&quot;,&quot;</span> + name + <span class="string">&quot;,&quot;</span> + age + <span class="string">&quot;,&quot;</span> + gender + <span class="string">&quot;,&quot;</span> + clazz);</span><br><span class="line">        &#125;</span><br><span class="line">        rs.close();</span><br><span class="line">        stat.close();</span><br><span class="line">        conn.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、Hive查询语法-DQL"><a href="#4、Hive查询语法-DQL" class="headerlink" title="4、Hive查询语法(DQL)"></a>4、Hive查询语法(DQL)</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> <span class="operator">|</span> <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line"><span class="keyword">FROM</span> table_reference</span><br><span class="line">[<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">[<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">[<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">[CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line"><span class="operator">|</span> [DISTRIBUTE <span class="keyword">BY</span> col_list] [SORT <span class="keyword">BY</span> col_list]</span><br><span class="line">]</span><br><span class="line">[LIMIT [<span class="keyword">offset</span>,] <span class="keyword">rows</span>]</span><br></pre></td></tr></table></figure><h3 id="4-1-全局排序"><a href="#4-1-全局排序" class="headerlink" title="4.1    全局排序"></a>4.1    全局排序</h3><ul><li><strong>order by 会对输入做全局排序，因此只有一个reducer</strong>，会导致当输入规模较大时，需要较长的计算时间</li><li>使用 order by子句排序 :ASC（ascend）升序（默认）| DESC（descend）降序</li><li><strong>order by放在select语句的结尾</strong></li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> 表名 <span class="keyword">order</span> <span class="keyword">by</span> 字段名<span class="number">1</span>[，别名<span class="number">2.</span>..];</span><br></pre></td></tr></table></figure><h3 id="4-2-局部排序"><a href="#4-2-局部排序" class="headerlink" title="4.2    局部排序"></a>4.2    局部排序</h3><ul><li><strong>sort by 不是全局排序,其在数据进入reducer前完成排序</strong>。</li><li>如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1,则sort by 只保证每个reducer的输出有序，<strong>不保证全局有序</strong>。asc,desc</li><li>设置reduce个数</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduce<span class="operator">=</span><span class="number">3</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.reduce.tasks<span class="operator">=</span><span class="number">3</span>;</span><br></pre></td></tr></table></figure><ul><li>查看reduce个数</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduce;</span><br></pre></td></tr></table></figure><ul><li>排序</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> 表名 sort <span class="keyword">by</span> 字段名[,字段名...];</span><br></pre></td></tr></table></figure><h3 id="4-3-分区排序"><a href="#4-3-分区排序" class="headerlink" title="4.3    分区排序"></a>4.3    分区排序</h3><blockquote><p><strong>distribute by（字段）根据指定的字段将数据</strong>分到不同的reducer，且分发算法是hash散列。</p><p><strong>类似MR中partition,进行分区，结合sort by使用。</strong>（注意：distribute by 要在sort by之前）</p><p>对于distrbute by 进行测试，一定要多分配reduce进行处理，否则无法看到distribute by的效果。</p><p>设置reduce个数</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduce<span class="operator">=</span><span class="number">7</span>;</span><br></pre></td></tr></table></figure><ul><li>排序</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> 表名 distribute <span class="keyword">by</span> 字段名[,字段名...];</span><br></pre></td></tr></table></figure><h3 id="4-3-分区并排序"><a href="#4-3-分区并排序" class="headerlink" title="4.3    分区并排序"></a>4.3    分区并排序</h3><ul><li>cluster by（字段）除了具有Distribute by的功能外，还会对该字段进行排序</li><li>cluster by &#x3D; distribute by + sort by 只能默认升序，不能使用倒序</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> 表名 sort cluster <span class="keyword">by</span> 字段名[,字段名...];</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> 表名 distribute <span class="keyword">by</span> 字段名[,字段名...] sort <span class="keyword">by</span> 字段名[,字段名...];</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/08/ZVozuJNtE9wXm2a.png" alt="hive几种分区的区别.png"></p><h2 id="5、Hive内置函数"><a href="#5、Hive内置函数" class="headerlink" title="5、Hive内置函数"></a>5、Hive内置函数</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">https:<span class="operator">/</span><span class="operator">/</span>cwiki.apache.org<span class="operator">/</span>confluence<span class="operator">/</span>display<span class="operator">/</span>Hive<span class="operator">/</span>LanguageManual<span class="operator">+</span>UDF</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 1.查看系统自带函数</span></span><br><span class="line"><span class="keyword">show</span> functions;</span><br><span class="line"><span class="comment">-- 2.显示自带的函数的用法</span></span><br><span class="line"><span class="keyword">desc</span> <span class="keyword">function</span> upper;</span><br><span class="line"><span class="comment">-- 3.详细显示自带的函数的用法</span></span><br><span class="line"><span class="keyword">desc</span> <span class="keyword">function</span> extended upper;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/08/gWa3pOnDHT89EtI.png" alt="内置函数的分类.png"></p><h3 id="5-1-内置函数分类"><a href="#5-1-内置函数分类" class="headerlink" title="5.1    内置函数分类"></a>5.1    内置函数分类</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">关系操作符：包括 <span class="operator">=</span> 、 <span class="operator">&lt;&gt;</span> 、 <span class="operator">&lt;=</span> 、<span class="operator">&gt;=</span>等</span><br><span class="line"></span><br><span class="line">算数操作符：包括 <span class="operator">+</span> 、 <span class="operator">-</span> 、 <span class="operator">*</span>、／等</span><br><span class="line"></span><br><span class="line">逻辑操作符：包括<span class="keyword">AND</span> 、 <span class="operator">&amp;&amp;</span> 、 <span class="keyword">OR</span> 、 <span class="operator">||</span> 等</span><br><span class="line"></span><br><span class="line">复杂类型构造函数：包括map、struct、create_union等</span><br><span class="line"></span><br><span class="line">复杂类型操作符：包括A[n]、Map[key]、S.x</span><br><span class="line"></span><br><span class="line">数学操作符：包括<span class="built_in">ln</span>(<span class="keyword">double</span> a)、<span class="built_in">sqrt</span>(<span class="keyword">double</span> a)等</span><br><span class="line"></span><br><span class="line">集合操作符：包括size(<span class="keyword">Array</span>)、sort_array(<span class="keyword">Array</span>)等</span><br><span class="line"></span><br><span class="line">类型转换函数： <span class="type">binary</span>(string<span class="operator">|</span><span class="type">binary</span>)、<span class="built_in">cast</span>(expr <span class="keyword">as</span> )</span><br><span class="line"></span><br><span class="line">日期函数：包括from_unixtime(<span class="type">bigint</span> unixtime[, string format])、unix_timestamp()等</span><br><span class="line"></span><br><span class="line">条件函数：包括if(<span class="type">boolean</span> testCondition, T valueTrue, T valueFalseOrNull)等</span><br><span class="line"></span><br><span class="line">字符串函数：包括acat(string<span class="operator">|</span><span class="type">binary</span> A, string<span class="operator">|</span><span class="type">binary</span> B…)等</span><br><span class="line"></span><br><span class="line">其他：xpath、get_json_objectscii(string str)、con</span><br></pre></td></tr></table></figure><h3 id="5-2-UDTF-hive中特殊的一个功能（进一出多）"><a href="#5-2-UDTF-hive中特殊的一个功能（进一出多）" class="headerlink" title="5.2    UDTF hive中特殊的一个功能（进一出多）"></a>5.2    UDTF hive中特殊的一个功能（进一出多）</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- UDF 进一出一</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- UDAF 进多出一</span></span><br><span class="line"><span class="comment">-- collect_set()和collect_list()都是对多列转成一行，区别就是list里面可重复而set里面是去重的</span></span><br><span class="line"><span class="comment">-- concat_ws(&#x27;:&#x27;,collect_set(type))   &#x27;:&#x27; 表示你合并后用什么分隔，collect_set(stage)表示要合并表中的那一列数据</span></span><br><span class="line"><span class="keyword">select</span> 字段名,concat_ws(<span class="string">&#x27;:&#x27;</span>,collect_set(列名)) <span class="keyword">as</span> 别名 <span class="keyword">from</span> 表名 <span class="keyword">group</span> <span class="keyword">by</span> id;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- UDTF 进一出多</span></span><br><span class="line"><span class="comment">-- explode  可以将一组数组的数据变成一列表</span></span><br><span class="line"><span class="keyword">select</span>  explode(split(列名,&quot;数据的分隔符&quot;)) <span class="keyword">from</span> 表名;</span><br><span class="line"><span class="comment">-- lateral view 表生成函数，可以将explode的数据生成一个列表</span></span><br><span class="line"><span class="keyword">select</span> id,name,列名 <span class="keyword">from</span> 表<span class="number">1</span>,<span class="keyword">lateral</span> <span class="keyword">view</span> explode(split(表<span class="number">1.</span>列名,&quot;数据的分隔符&quot;))新列名 <span class="keyword">as</span> 别列名;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建数据库表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_movie1(</span><br><span class="line">id <span class="type">int</span>,</span><br><span class="line">name string,</span><br><span class="line">types string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 电影数据  movie1.txt</span></span><br><span class="line"><span class="comment">-- 加载数据到数据库 load data inpath &#x27;/shujia/movie1.txt&#x27; into table t_movie1;</span></span><br><span class="line"><span class="number">1</span>,这个杀手不太冷,剧情<span class="operator">-</span>动作<span class="operator">-</span>犯罪</span><br><span class="line"><span class="number">2</span>,七武士,动作<span class="operator">-</span>冒险<span class="operator">-</span>剧情</span><br><span class="line"><span class="number">3</span>,勇敢的心,动作<span class="operator">-</span>传记<span class="operator">-</span>剧情<span class="operator">-</span>历史<span class="operator">-</span>战争</span><br><span class="line"><span class="number">4</span>,东邪西毒,剧情<span class="operator">-</span>动作<span class="operator">-</span>爱情<span class="operator">-</span>武侠<span class="operator">-</span>古装</span><br><span class="line"><span class="number">5</span>,霍比特人,动作<span class="operator">-</span>奇幻<span class="operator">-</span>冒险</span><br><span class="line"></span><br><span class="line"><span class="comment">-- explode  可以将一组数组的数据变成一列表</span></span><br><span class="line"><span class="keyword">select</span>  explode(split(types,&quot;-&quot;)) <span class="keyword">from</span> t_movie1;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- lateral view 表生成函数，可以将explode的数据生成一个列表</span></span><br><span class="line"><span class="keyword">select</span> id,name,type <span class="keyword">from</span> t_movie1 <span class="keyword">lateral</span> <span class="keyword">view</span> explode(split(types,&quot;-&quot;)) typetable <span class="keyword">as</span> type;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建数据库表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_movie2(</span><br><span class="line">id <span class="type">int</span>,</span><br><span class="line">name string,</span><br><span class="line">type string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 电影数据 movie2.txt</span></span><br><span class="line"><span class="comment">-- 加载数据到数据库 load data inpath &#x27;/shujia/movie2.txt&#x27; into table t_movie2;</span></span><br><span class="line"><span class="number">1</span>,这个杀手不太冷,剧情</span><br><span class="line"><span class="number">1</span>,这个杀手不太冷,动作</span><br><span class="line"><span class="number">1</span>,这个杀手不太冷,犯罪</span><br><span class="line"><span class="number">2</span>,七武士,动作</span><br><span class="line"><span class="number">2</span>,七武士,冒险</span><br><span class="line"><span class="number">2</span>,七武士,剧情</span><br><span class="line"><span class="number">3</span>,勇敢的心,动作</span><br><span class="line"><span class="number">3</span>,勇敢的心,传记</span><br><span class="line"><span class="number">3</span>,勇敢的心,剧情</span><br><span class="line"><span class="number">3</span>,勇敢的心,历史</span><br><span class="line"><span class="number">3</span>,勇敢的心,战争</span><br><span class="line"><span class="number">4</span>,东邪西毒,剧情</span><br><span class="line"><span class="number">4</span>,东邪西毒,动作</span><br><span class="line"><span class="number">4</span>,东邪西毒,爱情</span><br><span class="line"><span class="number">4</span>,东邪西毒,武侠</span><br><span class="line"><span class="number">4</span>,东邪西毒,古装</span><br><span class="line"><span class="number">5</span>,霍比特人,动作</span><br><span class="line"><span class="number">5</span>,霍比特人,奇幻</span><br><span class="line"><span class="number">5</span>,霍比特人,冒险</span><br><span class="line"></span><br><span class="line"><span class="comment">-- collect_set()和collect_list()都是对列转成行，区别就是list里面可重复而set里面是去重的</span></span><br><span class="line"><span class="comment">-- concat_ws(&#x27;:&#x27;,collect_set(type))   &#x27;:&#x27; 表示你合并后用什么分隔，collect_set(stage)表示要合并表中的那一列数据</span></span><br><span class="line"><span class="keyword">select</span> id,concat_ws(<span class="string">&#x27;:&#x27;</span>,collect_set(type)) <span class="keyword">as</span> types <span class="keyword">from</span> t_movie2 <span class="keyword">group</span> <span class="keyword">by</span> id;</span><br></pre></td></tr></table></figure><h3 id="5-3-WordCount案例"><a href="#5-3-WordCount案例" class="headerlink" title="5.3    WordCount案例"></a>5.3    WordCount案例</h3><blockquote><p>数据准备</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hello,world</span><br><span class="line">hello,bigdata</span><br><span class="line">like,life</span><br><span class="line">bigdata,good</span><br></pre></td></tr></table></figure><blockquote><p>建表</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> wc</span><br><span class="line">(</span><br><span class="line">line string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>导入数据</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/usr/local/soft/data/wc1.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> wc;</span><br></pre></td></tr></table></figure><blockquote><p><strong>步骤1：先对一行数据进行切分</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> split(line,<span class="string">&#x27;,&#x27;</span>) <span class="keyword">from</span> wc;</span><br></pre></td></tr></table></figure><blockquote><p><strong>步骤2：将行转列</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> explode(split(line,<span class="string">&#x27;,&#x27;</span>)) <span class="keyword">from</span> wc; </span><br></pre></td></tr></table></figure><blockquote><p><strong>步骤3：将相同的进行分组统计</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> w.word,<span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> (<span class="keyword">select</span> explode(split(line,<span class="string">&#x27;,&#x27;</span>)) <span class="keyword">as</span> word <span class="keyword">from</span> wc) w <span class="keyword">group</span> <span class="keyword">by</span> w.word;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive概述及安装</title>
      <link href="/2022/05/31/Hive%E6%A6%82%E8%BF%B0%E5%8F%8A%E5%AE%89%E8%A3%85/"/>
      <url>/2022/05/31/Hive%E6%A6%82%E8%BF%B0%E5%8F%8A%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h2 id="一、Hive基本概念"><a href="#一、Hive基本概念" class="headerlink" title="一、Hive基本概念"></a>一、Hive基本概念</h2><h3 id="1-1-Hive简介"><a href="#1-1-Hive简介" class="headerlink" title="1.1    Hive简介"></a>1.1    Hive简介</h3><p>Hive本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据存储，说白了hive可以理解为一个将SQL转换为MapReduce的任务的工具，甚至更近一步说hive就是一个MapReduce客户端。</p><p><img src="https://s2.loli.net/2022/06/01/EXbKAUlr4ao5e83.png" alt="image-20220531221835408"></p><h4 id="1-1-1-为什么使用Hive"><a href="#1-1-1-为什么使用Hive" class="headerlink" title="1.1.1    为什么使用Hive?"></a>1.1.1    为什么使用Hive?</h4><blockquote><p>如果直接使用hadoop的话，人员学习成本太高，项目要求周期太短，MapReduce实现复杂查询逻辑开发难度太大。如果使用hive的话，可以操作接口采用类SQL语法，提高开发能力，免去了写MapReduce，减少开发人员学习成本，功能扩展很方便（比如：开窗函数）。</p></blockquote><h4 id="1-1-2-Hive的特点："><a href="#1-1-2-Hive的特点：" class="headerlink" title="1.1.2    Hive的特点："></a>1.1.2    Hive的特点：</h4><blockquote><p>1、可扩展性</p><p>​    Hive可以自由的扩展集群的规模，一般情况下不需要重启服务</p><p>2、延申性</p><p>​    Hive支持自定义函数，用户可以根据自己的需求来实现自己的函数</p><p>3、容错</p><p>​    即使节点出现错误，SQL仍然可以完成执行</p></blockquote><h4 id="1-1-3-Hive的优缺点："><a href="#1-1-3-Hive的优缺点：" class="headerlink" title="1.1.3    Hive的优缺点："></a>1.1.3    Hive的优缺点：</h4><blockquote><p><strong>优点：</strong></p><p>​    1、操作接口采用类sql语法，提供快速开发的能力（简单、容易上手）</p><p>​    2、避免了去写MapReduce,减少开发人员的学习成本</p><p>​    3、Hive的延迟性比较高，因此Hive常用于数据分析，适用于对实时性要求不高的场合</p><p>​    4、Hive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。（不断地开关JVM虚拟机）</p><p>​    5、Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</p><p>​    6、集群可自由扩展并且具有良好的容错性，节点出现问题SQL仍可以完成执行</p><p><strong>缺点：</strong></p><p>​    1、Hive的HQL表达能力有限</p><p>​            （1）迭代式算法无法表达    （反复调用，mr之间独立，只有一个map一个reduce，反复开关）                </p><p>​            （2）数据挖掘方面不擅长</p><p>​    2、Hive 的效率比较低</p><p>​                （1）Hive 自动生成的 MapReduce 作业，通常情况下不够智能化</p><p>​                （2）Hive 调优比较困难，粒度较粗   （hql根据模板转成mapreduce，不能像自己编写mapreduce一样精细，无法控制在map处理数据还是在reduce处理数据）</p></blockquote><h4 id="1-1-4-Hive和传统数据库对比"><a href="#1-1-4-Hive和传统数据库对比" class="headerlink" title="1.1.4    Hive和传统数据库对比"></a>1.1.4    Hive和传统数据库对比</h4><p><img src="https://s2.loli.net/2022/06/01/JlRsONUB1oQMrxy.png" alt="image-20220531213145918"></p><h4 id="1-1-5-Hive应用场景"><a href="#1-1-5-Hive应用场景" class="headerlink" title="1.1.5    Hive应用场景"></a>1.1.5    Hive应用场景</h4><blockquote><p>日志分析：大部分互联网公司使用hive进行日志分析，如百度、淘宝等。</p><p>​    统计一个网站一个时间段内的<strong>pv,uv，SKU,SPU</strong></p><p>​    多维度数据分析</p><p>海量结构化数据离线分析</p><p><strong>构建数据仓库</strong></p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PV（Page View）访问量, 即页面浏览量或点击量，衡量网站用户访问的网页数量；在一定统计周期内用户每打开或刷新一个页面就记录1次，多次打开或刷新同一页面则浏览量累计。</span><br><span class="line"></span><br><span class="line">UV（Unique Visitor）独立访客，统计1天内访问某站点的用户数(以cookie为依据);访问网站的一台电脑客户端为一个访客。可以理解成访问某网站的电脑的数量。网站判断来访电脑的身份是通过来访电脑的cookies实现的。如果更换了IP后但不清除cookies，再访问相同网站，该网站的统计中UV数是不变的。如果用户不保存cookies访问、清除了cookies或者更换设备访问，计数会加1。00:00-24:00内相同的客户端多次访问只计为1个访客。</span><br></pre></td></tr></table></figure><h3 id="1-2-Hive架构"><a href="#1-2-Hive架构" class="headerlink" title="1.2    Hive架构"></a>1.2    Hive架构</h3><p><img src="https://s2.loli.net/2022/06/01/vRqfEbaU8TzhVQs.png" alt="image-20220531214038409"></p><h4 id="1-2-1-Client"><a href="#1-2-1-Client" class="headerlink" title="1.2.1    Client"></a>1.2.1    Client</h4><blockquote><p>Hive允许client连接的方式有三个CLI（hive shell）、JDBC&#x2F;ODBC(java访问hive)、WEBUI（浏览器访问 hive）。JDBC访问时中间件Thrift软件框架，跨语言服务开发。DDL DQL DML,整体仿写一套SQL语句。</p><p>​        1）client–需要下载安装包</p><p>​        2）JDBC&#x2F;ODBC 也可以连接到Hive<br>​                现在主流都在倡导第二种 HiveServer2&#x2F;beeline<br>​                做基于用户名和密码安全的一个校验</p><p>​        3）Web Gui<br>​                hive给我们提供了一套简单的web页面<br>​                我们可以通过这套web页面访问hive 做的太简陋了</p></blockquote><h4 id="1-2-2-Metastore"><a href="#1-2-2-Metastore" class="headerlink" title="1.2.2    Metastore"></a>1.2.2    Metastore</h4><blockquote><p><strong>元数据</strong>包括表名、表所属的数据库（默认是default）、表的拥有者、列&#x2F;分区字段、表的类型（是否是 外部表）、表的数据所在目录等。</p><p>​        一般需要借助于其他的数据载体（数据库）</p><p>​        主要用于存放数据库的建表语句等信息</p><p>​        推荐使用Mysql数据库存放数据</p><p>​        连接数据库需要提供：uri username password driver</p></blockquote><h4 id="1-2-3-Driver"><a href="#1-2-3-Driver" class="headerlink" title="1.2.3    Driver"></a>1.2.3    Driver</h4><blockquote><p>元数据存储在数据库中，默认存在自带的derby数据库（单用户局限性）中，推荐使用Mysql进行存储。</p><p>​            1） 解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完 成，比如ANTLR；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</p><p>​            2） 编译器（Physical Plan）：将AST编译生成逻辑执行计划。</p><p>​            3） 优化器（Query Optimizer）：对逻辑执行计划进行优化。</p><p>​            4） 执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是 MR&#x2F;Spark。</p></blockquote><p><img src="https://s2.loli.net/2022/06/01/Ubx7kIWChHfcLmv.png" alt="image-20220531000823975.png"></p><h4 id="1-2-4-数据处理"><a href="#1-2-4-数据处理" class="headerlink" title="1.2.4    数据处理"></a>1.2.4    数据处理</h4><blockquote><p>Hive的数据存储在HDFS中，计算由MapReduce完成。HDFS和MapReduce是源码级别上的整合，两者结合最佳。解释器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。</p></blockquote><h3 id="二、Hive的安装和使用"><a href="#二、Hive的安装和使用" class="headerlink" title="二、Hive的安装和使用"></a>二、Hive的安装和使用</h3><h4 id="2-1-离线安装MySQL（已经安装过MySQL可以跳过此步骤）"><a href="#2-1-离线安装MySQL（已经安装过MySQL可以跳过此步骤）" class="headerlink" title="2.1    离线安装MySQL（已经安装过MySQL可以跳过此步骤）"></a>2.1    离线安装MySQL（已经安装过MySQL可以跳过此步骤）</h4><p>注：</p><p>如果在&#x2F;usr&#x2F;bin&#x2F;mysql_secure_installation 一直是下面报错后</p><p><strong>解决办法：</strong></p><p>ps aux | grep mysql<br> 然后KILLmysql相关全部进程 Pid是进程号<br> kill -9 pid1 pid2 …</p><p>比如 kill -9 8301 8302<br> 然后再从第4步重新操作。</p><h4 id="2-2-修改MySQL编码"><a href="#2-2-修改MySQL编码" class="headerlink" title="2.2    修改MySQL编码"></a>2.2    修改MySQL编码</h4><p>1、修改mysql编码为UTF-8</p><p>1.1 编辑配置文件</p><pre><code>vim /etc/my.cnf</code></pre><p>1.2 加入以下内容：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client]</span><br><span class="line"></span><br><span class="line">default-character-set = utf8mb4</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line"></span><br><span class="line">character-set-server = utf8mb4</span><br><span class="line"></span><br><span class="line">collation-server = utf8mb4_general_ci</span><br></pre></td></tr></table></figure><p>1.3 重启mysql</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart mysqld</span><br></pre></td></tr></table></figure><p>1.4 登录mysql</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql -uroot -p123456</span><br></pre></td></tr></table></figure><p>1.5 查看mysql当前字符集</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">show variables like <span class="string">&#x27;%char%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>1.6 修改mysql元数据库hive，让其hive支持utf-8编码以支持中文</p><p>登录mysql：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql -u root -p123456</span><br></pre></td></tr></table></figure><p>切换到hive数据库:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">use hive;</span><br></pre></td></tr></table></figure><p>1).修改字段注释字符集</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> COLUMNS_V2 modify <span class="keyword">column</span> COMMENT <span class="type">varchar</span>(<span class="number">256</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure><p>2).修改表注释字符集</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> TABLE_PARAMS modify <span class="keyword">column</span> PARAM_VALUE <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure><p>3).修改分区表参数，以支持分区键能够用中文表示</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> PARTITION_PARAMS modify <span class="keyword">column</span> PARAM_VALUE <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> PARTITION_KEYS modify <span class="keyword">column</span> PKEY_COMMENT <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure><p>4).修改索引注解(可选)</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> INDEX_PARAMS modify <span class="keyword">column</span> PARAM_VALUE <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure><h4 id="2-3-安装Hive"><a href="#2-3-安装Hive" class="headerlink" title="2.3    安装Hive"></a>2.3    安装Hive</h4><p>前提是：mysql和hadoop必须已经成功启动了</p><h5 id="1、解压hive的安装包："><a href="#1、解压hive的安装包：" class="headerlink" title="1、解压hive的安装包："></a>1、解压hive的安装包：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf apache-hive-1.2.1-bin.tar.gz </span><br><span class="line"></span><br><span class="line">修改目录名称：</span><br><span class="line"><span class="built_in">mv</span> apache-hive-1.2.1-bin hive-1.2.1</span><br></pre></td></tr></table></figure><h5 id="2、进入hive-1-2-1-x2F-conf目录，复制备份文件并重命名"><a href="#2、进入hive-1-2-1-x2F-conf目录，复制备份文件并重命名" class="headerlink" title="2、进入hive-1.2.1&#x2F;conf目录，复制备份文件并重命名"></a>2、进入hive-1.2.1&#x2F;conf目录，复制备份文件并重命名</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cp</span> hive-env.sh.template hive-env.sh</span><br><span class="line"><span class="built_in">cp</span> hive-default.xml.template hive-site.xml</span><br></pre></td></tr></table></figure><h5 id="3、配置hive的配置文件"><a href="#3、配置hive的配置文件" class="headerlink" title="3、配置hive的配置文件"></a>3、配置hive的配置文件</h5><p>可以在 vim非编辑模式输入**&#x2F;想要查找的具体配置**，这样可以定位并以高亮形式标出</p><h6 id="3-1、修改hive-env-sh"><a href="#3-1、修改hive-env-sh" class="headerlink" title="3.1、修改hive-env.sh"></a>3.1、修改hive-env.sh</h6><p>加入三行内容（根据自己的目录和实际情况来添加）</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">HADOOP_HOME=/usr/local/soft/hadoop-2.7.6</span><br><span class="line">JAVA_HOME=/usr/local/soft/jdk1.8.0_171</span><br><span class="line">HIVE_HOME=/usr/local/soft/hive-1.2.1</span><br></pre></td></tr></table></figure><h6 id="3-2、修改hive-site-xml"><a href="#3-2、修改hive-site-xml" class="headerlink" title="3.2、修改hive-site.xml"></a>3.2、修改hive-site.xml</h6><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">&lt;！--数据存储位置就是我们在HDFS上看的目录--&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">(注意：修改自己安装mysql的主机地址）</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://192.168.40.110:3306/hive?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useUnicode=true<span class="symbol">&amp;amp;</span>characterEncoding=utf8<span class="symbol">&amp;amp;</span>useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">(固定写法，mysql驱动类的位置)</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">（mysql的用户名）</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">（mysql的用户密码）</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">（你的hive安装目录的tmp目录）</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.querylog.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/soft/hive-1.2.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">（同上）</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.local.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/soft/hive-1.2.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">（同上）</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.downloaded.resources.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/soft/hive-1.2.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--指定这个的时候，为了启动metastore服务的时候不用指定端口--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--hive --service metastore -p 9083 &amp; | hive --service metastore--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>thrift://master:9083<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>修改hadoop中<strong>core-site.xml</strong>直接改，改完重启就行，为后面beeline连接做准备</p><p><strong>注意：三个节点上的都要改。</strong></p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--该参数表示可以通过httpfs接口hdfs的ip地址限制--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hadoop.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--通过httpfs接口访问的用户获得的群组身份--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hadoop.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h5 id="4、拷贝mysql驱动到-HIVE-HOME-x2F-lib目录下"><a href="#4、拷贝mysql驱动到-HIVE-HOME-x2F-lib目录下" class="headerlink" title="4、拷贝mysql驱动到$HIVE_HOME&#x2F;lib目录下"></a>4、拷贝mysql驱动到$HIVE_HOME&#x2F;lib目录下</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cp</span> /usr/local/soft/mysql-connector-java-5.1.49.jar ../lib/</span><br></pre></td></tr></table></figure><h5 id="5、将hive的jline-2-12-jar拷贝到hadoop对应目录下，hive的-jline-2-12-jar-位置在-："><a href="#5、将hive的jline-2-12-jar拷贝到hadoop对应目录下，hive的-jline-2-12-jar-位置在-：" class="headerlink" title="5、将hive的jline-2.12.jar拷贝到hadoop对应目录下，hive的 jline-2.12.jar 位置在 ："></a>5、将hive的jline-2.12.jar拷贝到hadoop对应目录下，hive的 jline-2.12.jar 位置在 ：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/local/soft/hive-1.2.1/lib/jline-2.12.jar</span><br></pre></td></tr></table></figure><h5 id="6、将hive的jar拷过去hadoop下："><a href="#6、将hive的jar拷过去hadoop下：" class="headerlink" title="6、将hive的jar拷过去hadoop下："></a>6、将hive的jar拷过去hadoop下：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cp</span> /usr/local/soft/hive-1.2.1/lib/jline-2.12.jar /usr/local/soft/hadoop-2.7.6/share/hadoop/yarn/lib/</span><br></pre></td></tr></table></figure><h5 id="7、配置环境变量"><a href="#7、配置环境变量" class="headerlink" title="7、配置环境变量"></a>7、配置环境变量</h5><p> vim &#x2F;etc&#x2F;profile</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">HIVE_HOME=/usr/local/soft/hive-1.2.1</span><br><span class="line"><span class="built_in">export</span> PATH=.:<span class="variable">$HIVE_HOME</span>/bin</span><br><span class="line"></span><br><span class="line">重新加载环境变量</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><h5 id="8、分发Hive"><a href="#8、分发Hive" class="headerlink" title="8、分发Hive"></a>8、分发Hive</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">拷贝到其他两个节点中去，因为可能我们会在其他的节点上当作客户端访问hive，注意，也需要配置环境变量，增加驱动jar包，将hadoop的jline-0.9.94.jar的jar替换成hive的版本</span><br></pre></td></tr></table></figure><h5 id="9、启动hive："><a href="#9、启动hive：" class="headerlink" title="9、启动hive："></a>9、启动hive：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">启动hadoop</span><br><span class="line"></span><br><span class="line">start-all.sh</span><br><span class="line"></span><br><span class="line">启动hive</span><br><span class="line"></span><br><span class="line">​hive --service metastore</span><br><span class="line"></span><br><span class="line">​<span class="built_in">nohup</span> hive --service metastore &gt;/dev/null &amp;</span><br><span class="line"></span><br><span class="line">​hive</span><br><span class="line"></span><br><span class="line">启动HiveServer2</span><br><span class="line"></span><br><span class="line">​hiveserver2</span><br><span class="line"></span><br><span class="line">​<span class="built_in">nohup</span> hiveserver2 &gt;/dev/null &amp;</span><br><span class="line"></span><br><span class="line">​beeline -u jdbc:hive2://master:10000 -n root</span><br></pre></td></tr></table></figure><h6 id="9-1Hive的三种交互方式"><a href="#9-1Hive的三种交互方式" class="headerlink" title="9.1Hive的三种交互方式"></a>9.1Hive的三种交互方式</h6><h6 id="1）第一种交互方式"><a href="#1）第一种交互方式" class="headerlink" title="1）第一种交互方式"></a><strong>1）第一种交互方式</strong></h6><blockquote><p>shell交互Hive，用命令hive启动一个hive的shell命令行，在命令行中输入sql或者命令来和Hive交互。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">服务端启动metastore服务（后台启动）：nohup hive --service metastore &gt; /usr/local/soft/mylogs 2&gt;&amp;1 &amp;</span><br><span class="line">进入命令:hive</span><br><span class="line">退出命令行：quit;</span><br></pre></td></tr></table></figure><h6 id="2）第二种交互方式"><a href="#2）第二种交互方式" class="headerlink" title="2）第二种交互方式"></a><strong>2）第二种交互方式</strong></h6><blockquote><p><strong>Hive启动为一个服务器，对外提供服务</strong>，其他机器可以通过客户端通过协议连接到服务器，来完成访问操作，这是生产环境用法最多的</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">服务端启动hiveserver2服务：</span><br><span class="line">nohup hive --service metastore &gt;/dev/null &amp;</span><br><span class="line">nohup hiveserver2 &gt;/dev/null &amp;</span><br><span class="line"></span><br><span class="line">需要稍等一下，启动服务需要时间：</span><br><span class="line">进入命令:1)先执行： beeline ，再执行： !connect jdbc:hive2://master:10000 </span><br><span class="line">        2)或者直接执行：  beeline -u jdbc:hive2://master:10000 -n root</span><br><span class="line">退出命令行：！exit</span><br></pre></td></tr></table></figure><h6 id="3）第三种交互方式"><a href="#3）第三种交互方式" class="headerlink" title="3）第三种交互方式"></a><strong>3）第三种交互方式</strong></h6><blockquote><p>使用 –e 参数来直接执行hql的语句</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hive -e &quot;show databases;&quot;</span><br></pre></td></tr></table></figure><blockquote><p>使用 –f 参数通过指定文本文件来执行hql的语句</p><p>特点：执行完sql后，回到linux命令行。</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim hive.sql</span><br><span class="line"></span><br><span class="line">use myhive;</span><br><span class="line">select * from test;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive -f hive.sql</span><br></pre></td></tr></table></figure><h6 id="4）hive-cli和beeline-cli的区别"><a href="#4）hive-cli和beeline-cli的区别" class="headerlink" title="4）hive cli和beeline cli的区别"></a>4）hive cli和beeline cli的区别</h6><p><img src="https://s2.loli.net/2022/06/01/CaqXyTQ47VhWeZ3.png" alt="image-20220531230402802.png"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的基本操作</title>
      <link href="/2022/05/31/Hive%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-1/"/>
      <url>/2022/05/31/Hive%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-1/</url>
      
        <content type="html"><![CDATA[<h3 id="1、Hive元数据"><a href="#1、Hive元数据" class="headerlink" title="1、Hive元数据"></a>1、Hive元数据</h3><p><strong>Hive元数据库中一些重要的表结构及用途</strong>，方便Impala、SparkSQL、Hive等组件访问元数据库的理解。</p><p>1、存储Hive版本的**元数据表(VERSION)**，该表比较简单，但很重要,如果这个表出现问题，根本进不来Hive-Cli。比如该表不存在，当启动Hive-Cli的时候，就会报错“Table ‘hive.version’ doesn’t exist”</p><p>2、Hive数据库相关的元数据表(DBS、DATABASE_PARAMS)</p><p>​        DBS：该表存储Hive中所有数据库的基本信息。</p><p>​        DATABASE_PARAMS：该表存储数据库的相关参数。</p><p>3、Hive表和视图相关的元数据表</p><p>​        主要有TBLS、TABLE_PARAMS、TBL_PRIVS，这三张表通过TBL_ID关联。<br>​        TBLS:该表中存储Hive表，视图，索引表的基本信息。<br>​        TABLE_PARAMS:该表存储表&#x2F;视图的属性信息。<br>​        TBL_PRIVS：该表存储表&#x2F;视图的授权信息。<br>4、Hive文件存储信息相关的元数据表</p><p>​        主要涉及SDS、SD_PARAMS、SERDES、SERDE_PARAMS，由于HDFS支持的文件格式很多，而建Hive表时候也可以指定各种文件格式，Hive在将HQL解析成MapReduce时候，需要知道去哪里，使用哪种格式去读写HDFS文件，而这些信息就保存在这几张表中。<br>​        SDS：该表保存文件存储的基本信息，如INPUT_FORMAT、OUTPUT_FORMAT、是否压缩等。TBLS表中的SD_ID与该表关联，可以获取Hive表的存储信息。<br>​        SD_PARAMS: 该表存储Hive存储的属性信息。<br>​        SERDES:该表存储序列化使用的类信息。<br>​        SERDE_PARAMS:该表存储序列化的一些属性、格式信息，比如:行、列分隔符。<br>5、Hive表字段相关的元数据表</p><p>​        主要涉及COLUMNS_V2：该表存储表对应的字段信息。</p><h3 id="2、Hive的基本操作"><a href="#2、Hive的基本操作" class="headerlink" title="2、Hive的基本操作"></a>2、Hive的基本操作</h3><h4 id="2-1-Hive库操作"><a href="#2-1-Hive库操作" class="headerlink" title="2.1    Hive库操作"></a>2.1    Hive库操作</h4><h5 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h5><blockquote><p>1）创建一个数据库，数据库在<strong>HDFS上的默认存储路径是&#x2F;hive&#x2F;warehouse&#x2F;*.db</strong>。</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> database testdb;</span><br></pre></td></tr></table></figure><blockquote><p>2）避免要创建的数据库已经存在错误，增加if not exists判断。<strong>（标准写法）</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> testdb; </span><br></pre></td></tr></table></figure><h5 id="创建数据库和位置"><a href="#创建数据库和位置" class="headerlink" title="创建数据库和位置"></a>创建数据库和位置</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> dept location <span class="string">&#x27;/testdb.db&#x27;</span>;</span><br></pre></td></tr></table></figure><h5 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h5><blockquote><p><strong>数据库的其他元数据信息都是不可更改的</strong>，包括数据库名和数据库所在的目录位置。</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> database dept <span class="keyword">set</span> dbproperties(<span class="string">&#x27;createtime&#x27;</span><span class="operator">=</span><span class="string">&#x27;20220531&#x27;</span>);</span><br></pre></td></tr></table></figure><h5 id="数据库详细信息"><a href="#数据库详细信息" class="headerlink" title="数据库详细信息"></a>数据库详细信息</h5><blockquote><p>1）显示数据库（show）</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> databases;</span><br></pre></td></tr></table></figure><blockquote><p>2）可以通过like进行过滤</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> databases <span class="keyword">like</span> <span class="string">&#x27;t*&#x27;</span>;</span><br></pre></td></tr></table></figure><blockquote><p>3）查看详情（desc）</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">desc</span> database testdb;</span><br></pre></td></tr></table></figure><blockquote><p>4）切换数据库（use）</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">use testdb;</span><br></pre></td></tr></table></figure><h5 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h5><blockquote><p>1）最简写法</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> database testdb;</span><br></pre></td></tr></table></figure><blockquote><p>2）如果删除的数据库不存在，最好使用if exists判断数据库是否存在。否则会报错：FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> database if <span class="keyword">exists</span> testdb;</span><br></pre></td></tr></table></figure><blockquote><p>3)如果数据库不为空，使用cascade命令进行强制删除。报错信息如下FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> database if <span class="keyword">exists</span> testdb cascade;</span><br></pre></td></tr></table></figure><h3 id="2-2-Hive数据类型"><a href="#2-2-Hive数据类型" class="headerlink" title="2.2    Hive数据类型"></a>2.2    Hive数据类型</h3><h4 id="基础数据类型"><a href="#基础数据类型" class="headerlink" title="基础数据类型"></a>基础数据类型</h4><table><thead><tr><th>类型</th><th>Java数据类型</th><th>描述</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>8位有符号整型。取值范围：-128~127。</td></tr><tr><td>SMALLINT</td><td>short</td><td>16位有符号整型。取值范围：-32768~32767。</td></tr><tr><td>INT</td><td>int</td><td>32位有符号整型。取值范围：-2 31 ~2 31 -1。</td></tr><tr><td><strong>BIGINT</strong></td><td>long</td><td>64位有符号整型。取值范围：-2 63 +1~2 63 -1。</td></tr><tr><td>BINARY</td><td></td><td>二进制数据类型，目前长度限制为8MB。</td></tr><tr><td>FLOAT</td><td>float</td><td>32位二进制浮点型。</td></tr><tr><td>DOUBLE</td><td>double</td><td>64位二进制浮点型。</td></tr><tr><td><strong>DECIMAL(precision,scale)</strong></td><td></td><td>10进制精确数字类型。precision：表示最多可以表示多少位的数字。取值范围：1 &lt;&#x3D; precision &lt;&#x3D; 38。scale：表示小数部分的位数。取值范围： 0 &lt;&#x3D; scale &lt;&#x3D; 38。如果不指定以上两个参数，则默认为decimal(10,0)。</td></tr><tr><td>VARCHAR(n)</td><td></td><td>变长字符类型，n为长度。取值范围：1~65535。</td></tr><tr><td>CHAR(n)</td><td></td><td>固定长度字符类型，n为长度。最大取值255。长度不足则会填充空格，但空格不参与比较。</td></tr><tr><td><strong>STRING</strong></td><td>string</td><td>字符串类型，目前长度限制为8MB。</td></tr><tr><td>DATE</td><td></td><td>日期类型，格式为<code>yyyy-mm-dd</code>。取值范围：0000-01-01~9999-12-31。</td></tr><tr><td>DATETIME</td><td></td><td>日期时间类型。取值范围：0000-01-01 00:00:00.000~9999-12-31 23.59:59.999，精确到毫秒。</td></tr><tr><td><strong>TIMESTAMP</strong></td><td></td><td>与时区无关的时间戳类型。取值范围：0000-01-01 00:00:00.000000000~9999-12-31 23.59:59.999999999，精确到纳秒。说明 对于部分时区相关的函数，例如cast(<a timestamp> as string)，要求TIMESTAMP按照与当前时区相符的方式来展现。</td></tr><tr><td><strong>BOOLEAN</strong></td><td>boolean</td><td>BOOLEAN类型。取值：True、False。</td></tr></tbody></table><h4 id="复杂的数据类型"><a href="#复杂的数据类型" class="headerlink" title="复杂的数据类型"></a>复杂的数据类型</h4><table><thead><tr><th>类型</th><th>定义方法</th><th>构造方法</th></tr></thead><tbody><tr><td>ARRAY</td><td><code>array&lt;int&gt;``array&lt;struct&lt;a:int, b:string&gt;&gt;</code></td><td><code>array(1, 2, 3)``array(array(1, 2), array(3, 4))</code></td></tr><tr><td>MAP</td><td><code>map&lt;string, string&gt;``map&lt;smallint, array&lt;string&gt;&gt;</code></td><td><code>map(“k1”, “v1”, “k2”, “v2”)``map(1S, array(‘a’, ‘b’), 2S, array(‘x’, ‘y’))</code></td></tr><tr><td>STRUCT</td><td></td><td>struct&lt;x:int, y:int&gt;<code>struct&lt;field1:bigint, field2:array&lt;int&gt;, field3:map&lt;int, int&gt;&gt;    named_struct(‘x’, 1, ‘y’, 2)</code>named_struct(‘field1’, 100L, ‘field2’, array(1, 2), ‘field3’, map(1, 100, 2, 200))</td></tr></tbody></table><blockquote><p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。还有一个uniontype&lt; 所有类型，所有类型… &gt; 。</p><p>​        数组：array&lt; 所有类型 &gt;；<br>​        Map &lt; 基本数据类型，所有数据类型 &gt;；<br>​        struct &lt; 名：所有类型[注释] &gt;;<br>​        uniontype&lt; 所有类型，所有类型… &gt;</p></blockquote><h3 id="2-3-Hive表操作"><a href="#2-3-Hive表操作" class="headerlink" title="2.3    Hive表操作"></a>2.3    Hive表操作</h3><blockquote><p>Hive的存储格式:</p><p>Hive没有专门的数据文件格式,常见的有以下几种:</p><p>​        <strong>TEXTFILE</strong><br>​        SEQUENCEFILE<br>​        AVRO<br>​        <strong>RCFILE</strong><br>​        <strong>ORCFILE</strong><br>​        <strong>PARQUET</strong></p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">TextFile:</span><br><span class="line">       TEXTFILE 即正常的文本格式，是Hive默认文件存储格式，因为大多数情况下源数据文件都是以text文件格式保存（便于查看验数和防止乱码）。此种格式的表文件在HDFS上是明文，可用hadoop fs -cat命令查看，从HDFS上get下来后也可以直接读取。</span><br><span class="line">        TEXTFILE 存储文件默认每一行就是一条记录，可以指定任意的分隔符进行字段间的分割。但这个格式无压缩，需要的存储空间很大。虽然可结合Gzip、Bzip2、Snappy等使用，使用这种方式，Hive不会对数据进行切分，从而无法对数据进行并行操作。</span><br><span class="line">一般只有与其他系统由数据交互的接口表采用TEXTFILE 格式，其他事实表和维度表都不建议使用。</span><br><span class="line"></span><br><span class="line">RCFile:</span><br><span class="line">Record Columnar的缩写。是Hadoop中第一个列文件格式。能够很好的压缩和快速的查询性能。通常写操作比较慢，比非列形式的文件格式需要更多的内存空间和计算量。 RCFile是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据`列式存储`，有利于数据压缩和快速的列存取。</span><br><span class="line"></span><br><span class="line">ORCFile:</span><br><span class="line">Hive从0.11版本开始提供了ORC的文件格式，ORC文件不仅仅是一种列式文件存储格式，最重要的是有着很高的压缩比，并且对于MapReduce来说是可切分（Split）的。因此，在Hive中使用ORC作为表的文件存储格式，不仅可以很大程度的节省HDFS存储资源，而且对数据的查询和处理性能有着非常大的提升，因为ORC较其他文件格式压缩比高，查询任务的输入数据量减少，使用的Task也就减少了。ORC能很大程度的节省存储和计算资源，但它在读写时候需要消耗额外的CPU资源来压缩和解压缩，当然这部分的CPU消耗是非常少的。</span><br><span class="line"></span><br><span class="line">Parquet:</span><br><span class="line">通常我们使用关系数据库存储结构化数据，而关系数据库中使用数据模型都是扁平式的，遇到诸如List、Map和自定义Struct的时候就需要用户在应用层解析。但是在大数据环境下，通常数据的来源是服务端的埋点数据，很可能需要把程序中的某些对象内容作为输出的一部分，而每一个对象都可能是嵌套的，所以如果能够原生的支持这种数据，这样在查询的时候就不需要额外的解析便能获得想要的结果。Parquet的灵感来自于2010年Google发表的Dremel论文，文中介绍了一种支持嵌套结构的存储格式，并且使用了列式存储的方式提升查询性能。Parquet仅仅是一种存储格式，它是语言、平台无关的，并且不需要和任何一种数据处理框架绑定。这也是parquet相较于orc的仅有优势：支持嵌套结构。Parquet 没有太多其他可圈可点的地方,比如他不支持update操作(数据写成后不可修改),不支持ACID等.</span><br><span class="line"></span><br><span class="line">SEQUENCEFILE:</span><br><span class="line">SequenceFile是Hadoop API 提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中。这种二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。它与Hadoop API中的MapFile 是互相兼容的。Hive 中的SequenceFile 继承自Hadoop API 的SequenceFile，不过它的key为空，使用value 存放实际的值， 这样是为了避免MR 在运行map 阶段的排序过程。SequenceFile支持三种压缩选择：NONE, RECORD, BLOCK。 Record压缩率低，一般建议使用BLOCK压缩。 SequenceFile最重要的优点就是Hadoop原生支持较好，有API，但除此之外平平无奇，实际生产中不会使用。</span><br><span class="line"></span><br><span class="line">AVRO:</span><br><span class="line">Avro是一种用于支持数据密集型的二进制文件格式。它的文件格式更为紧凑，若要读取大量数据时，Avro能够提供更好的序列化和反序列化性能。并且Avro数据文件天生是带Schema定义的，所以它不需要开发者在API 级别实现自己的Writable对象。Avro提供的机制使动态语言可以方便地处理Avro数据。最近多个Hadoop 子项目都支持Avro 数据格式，如Pig 、Hive、Flume、Sqoop和Hcatalog。</span><br></pre></td></tr></table></figure><p><strong>Hive的四大常用存储格式存储效率及执行速度对比</strong></p><blockquote><p>结论：ORCFILE存储文件读操作效率最高</p><p>耗时比较：ORC&lt;Parquet&lt;RC&lt;Text</p></blockquote><blockquote><p>结论：ORCFILE存储文件占用空间少，压缩效率高</p><p>占用空间：ORC&lt;Parquet&lt;RC&lt;Text</p></blockquote><h4 id="2-3-1-创建表"><a href="#2-3-1-创建表" class="headerlink" title="2.3.1    创建表"></a>2.3.1    创建表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[COMMENT table_comment] </span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span><span class="operator">|</span><span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] </span><br><span class="line">[<span class="type">ROW</span> FORMAT row_format] </span><br><span class="line">[STORED <span class="keyword">AS</span> file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">字段解释说明:</span><br><span class="line"><span class="operator">-</span> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> </span><br><span class="line">创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> 选项来忽略这个异常。</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> <span class="keyword">EXTERNAL</span></span><br><span class="line">关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION）</span><br><span class="line">创建内部表时，会将数据移动到数据仓库指向的路径（默认位置）；</span><br><span class="line">创建外部表时，仅记录数据所在的路径，不对数据的位置做任何改变。在</span><br><span class="line">删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> COMMENT：</span><br><span class="line">为表和列添加注释。</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> PARTITIONED <span class="keyword">BY</span></span><br><span class="line">创建分区表</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> CLUSTERED <span class="keyword">BY</span></span><br><span class="line">创建分桶表</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> SORTED <span class="keyword">BY</span></span><br><span class="line">不常用</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> <span class="type">ROW</span> FORMAT </span><br><span class="line">  DELIMITED [FIELDS TERMINATED <span class="keyword">BY</span> <span class="type">char</span>] [COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="type">char</span>] [MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="type">char</span>] [LINES TERMINATED <span class="keyword">BY</span> <span class="type">char</span>] <span class="operator">|</span> SERDE serde_name [<span class="keyword">WITH</span> SERDEPROPERTIES (property_name<span class="operator">=</span>property_value, property_name<span class="operator">=</span>property_value, ...)]</span><br><span class="line">用户在建表的时候可以自定义SerDe或者使用自带的SerDe。</span><br><span class="line">如果没有指定<span class="type">ROW</span> FORMAT 或者<span class="type">ROW</span> FORMAT DELIMITED，将会使用自带的SerDe。</span><br><span class="line">在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。</span><br><span class="line">SerDe是Serialize<span class="operator">/</span>Deserilize的简称，目的是用于序列化和反序列化。</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> STORED <span class="keyword">AS</span>指定存储文件类型</span><br><span class="line">常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</span><br><span class="line">如果文件数据是纯文本，可以使用STORED <span class="keyword">AS</span> TEXTFILE。</span><br><span class="line">如果数据需要压缩，使用 STORED <span class="keyword">AS</span> SEQUENCEFILE。</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> LOCATION ：</span><br><span class="line">指定表在HDFS上的存储位置。</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> <span class="keyword">LIKE</span></span><br><span class="line">允许用户复制现有的表结构，但是不复制数据。</span><br></pre></td></tr></table></figure><h5 id="建表1：全部使用默认建表方式"><a href="#建表1：全部使用默认建表方式" class="headerlink" title="建表1：全部使用默认建表方式"></a>建表1：全部使用默认建表方式</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table students</span><br><span class="line">(</span><br><span class="line">    id bigint,</span><br><span class="line">    name string,</span><br><span class="line">    age int,</span><br><span class="line">    gender string,</span><br><span class="line">    clazz string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;; // 必选，指定列分隔符 </span><br></pre></td></tr></table></figure><h5 id="建表2：指定location-（这种方式也比较常用）"><a href="#建表2：指定location-（这种方式也比较常用）" class="headerlink" title="建表2：指定location （这种方式也比较常用）"></a>建表2：指定location （这种方式也比较常用）</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table students2</span><br><span class="line">(</span><br><span class="line">    id bigint,</span><br><span class="line">    name string,</span><br><span class="line">    age int,</span><br><span class="line">    gender string,</span><br><span class="line">    clazz string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;</span><br><span class="line">LOCATION &#x27;/input1&#x27;; // 指定Hive表的数据的存储位置，一般在数据已经上传到HDFS，想要直接使用，会指定Location，通常Locaion会跟外部表一起使用，内部表一般使用默认的location</span><br></pre></td></tr></table></figure><h5 id="建表3：指定存储格式"><a href="#建表3：指定存储格式" class="headerlink" title="建表3：指定存储格式"></a>建表3：指定存储格式</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table students3</span><br><span class="line">(</span><br><span class="line">    id bigint,</span><br><span class="line">    name string,</span><br><span class="line">    age int,</span><br><span class="line">    gender string,</span><br><span class="line">    clazz string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;</span><br><span class="line">STORED AS rcfile; // 指定储存格式为rcfile，inputFormat:RCFileInputFormat,outputFormat:RCFileOutputFormat，如果不指定，默认为textfile，注意：除textfile以外，其他的存储格式的数据都不能直接加载，需要使用从表加载的方式。</span><br></pre></td></tr></table></figure><h5 id="建表4：create-table-xxxx-as-select-statement-SQL语句-这种方式比较常用"><a href="#建表4：create-table-xxxx-as-select-statement-SQL语句-这种方式比较常用" class="headerlink" title="建表4：create table xxxx as select_statement(SQL语句) (这种方式比较常用)"></a>建表4：create table xxxx as select_statement(SQL语句) (这种方式比较常用)</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table students4 as select * from students2;</span><br></pre></td></tr></table></figure><h5 id="建表5：create-table-xxxx-like-table-name-只想建表，不需要加载数据"><a href="#建表5：create-table-xxxx-like-table-name-只想建表，不需要加载数据" class="headerlink" title="建表5：create table xxxx like table_name  只想建表，不需要加载数据"></a>建表5：create table xxxx like table_name  只想建表，不需要加载数据</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table students5 like students;</span><br></pre></td></tr></table></figure><blockquote><p><strong>简单用户信息表创建：</strong></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_user(</span><br><span class="line">id <span class="type">int</span>,</span><br><span class="line">uname string,</span><br><span class="line">pwd string,</span><br><span class="line">gender string,</span><br><span class="line">age <span class="type">int</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1,admin,123456,男,18</span><br><span class="line">2,zhangsan,abc123,男,23</span><br><span class="line">3,lisi,654321,女,16</span><br></pre></td></tr></table></figure><p>复杂人员信息表创建：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> t_person(</span><br><span class="line">name string,</span><br><span class="line">friends <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>,</span><br><span class="line">children map<span class="operator">&lt;</span>string,<span class="type">int</span><span class="operator">&gt;</span>,</span><br><span class="line">address struct<span class="operator">&lt;</span>street:string ,city:string<span class="operator">&gt;</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">collection items terminated <span class="keyword">by</span> <span class="string">&#x27;_&#x27;</span></span><br><span class="line">map keys terminated <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:<span class="number">18</span>_xiaoxiao song:<span class="number">19</span>,beng bu_anhui</span><br><span class="line">yangyang,caicai_susu,xiao yang:<span class="number">18</span>_xiaoxiao yang:<span class="number">19</span>,he fei_anhui</span><br></pre></td></tr></table></figure></blockquote><h4 id="2-3-2-显示表"><a href="#2-3-2-显示表" class="headerlink" title="2.3.2    显示表"></a>2.3.2    显示表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> tables;</span><br><span class="line"><span class="keyword">show</span> tables <span class="keyword">like</span> <span class="string">&#x27;u&#x27;</span>;</span><br><span class="line"><span class="keyword">desc</span> t_person;</span><br><span class="line"><span class="keyword">desc</span> formatted t_person;</span><br></pre></td></tr></table></figure><h4 id="2-3-3-加载数据"><a href="#2-3-3-加载数据" class="headerlink" title="2.3.3    加载数据"></a>2.3.3    加载数据</h4><h5 id="1、使用hdfs-dfs-put-39-本地数据-39-39-hive表对应的HDFS目录下-39"><a href="#1、使用hdfs-dfs-put-39-本地数据-39-39-hive表对应的HDFS目录下-39" class="headerlink" title="1、使用hdfs dfs -put &#39;本地数据&#39; &#39;hive表对应的HDFS目录下&#39;"></a>1、使用<code>hdfs dfs -put &#39;本地数据&#39; &#39;hive表对应的HDFS目录下&#39;</code></h5><h5 id="2、使用-load-data-inpath"><a href="#2、使用-load-data-inpath" class="headerlink" title="2、使用 load data inpath"></a>2、使用 load data inpath</h5><blockquote><p>下列命令需要在hive shell里执行</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 将HDFS上的/input1目录下面的数据 移动至 students表对应的HDFS目录下，注意是 移动、移动、移动</span><br><span class="line">load data inpath &#x27;/input1/students.txt&#x27; into table students;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 清空表</span><br><span class="line">truncate table students;</span><br><span class="line">// 加上 local 关键字 可以将Linux本地目录下的文件 上传到 hive表对应HDFS 目录下 原文件不会被删除</span><br><span class="line">load data local inpath &#x27;/usr/local/soft/data/students.txt&#x27; into table students;</span><br><span class="line">// overwrite 覆盖加载</span><br><span class="line">load data local inpath &#x27;/usr/local/soft/data/students.txt&#x27; overwrite into table students;</span><br></pre></td></tr></table></figure><h5 id="3、create-table-xxx-as-SQL语句"><a href="#3、create-table-xxx-as-SQL语句" class="headerlink" title="3、create table xxx as SQL语句"></a>3、create table xxx as SQL语句</h5><h5 id="4、insert-into-table-xxxx-SQL语句-（没有as）"><a href="#4、insert-into-table-xxxx-SQL语句-（没有as）" class="headerlink" title="4、insert into table xxxx SQL语句 （没有as）"></a>4、insert into table xxxx SQL语句 （没有as）</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 将 students表的数据插入到students2 这是复制 不是移动 students表中的表中的数据不会丢失</span><br><span class="line">insert into table students2 select * from students;</span><br><span class="line"></span><br><span class="line">// 覆盖插入 把into 换成 overwrite</span><br><span class="line">insert overwrite table students2 select * from students;</span><br></pre></td></tr></table></figure><h4 id="2-3-4-修改列"><a href="#2-3-4-修改列" class="headerlink" title="2.3.4    修改列"></a>2.3.4    修改列</h4><blockquote><p>查询表结构</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">desc</span> students2;</span><br></pre></td></tr></table></figure><blockquote><p>添加列</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> students2 <span class="keyword">add</span> columns (education string);</span><br></pre></td></tr></table></figure><blockquote><p>查询表结构</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">desc</span> students2;</span><br></pre></td></tr></table></figure><blockquote><p>更新列</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> stduents2 change education educationnew string;</span><br></pre></td></tr></table></figure><h4 id="2-3-5-删除表"><a href="#2-3-5-删除表" class="headerlink" title="2.3.5    删除表"></a>2.3.5    删除表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> students2;</span><br></pre></td></tr></table></figure><h3 id="2-4-Hive内外部表"><a href="#2-4-Hive内外部表" class="headerlink" title="2.4    Hive内外部表"></a>2.4    Hive内外部表</h3><blockquote><p><strong>面试题：内部表和外部表的区别？如何创建外部表？工作中使用外部表</strong></p></blockquote><h4 id="2-4-1-hive内部表"><a href="#2-4-1-hive内部表" class="headerlink" title="2.4.1    hive内部表"></a>2.4.1    hive内部表</h4><blockquote><p>当<strong>创建好表的时候，HDFS会在当前表所属的库中创建一个文件夹</strong></p><p>当设置表路径的时候，如果直接指向一个已有的路径,可以直接去使用文件夹中的数据</p><p><strong>当load数据的时候，就会将数据文件存放到表对应的文件夹中</strong></p><p>而且<strong>数据一旦被load，就不能被修改</strong></p><p>我们查询数据也是查询文件中的文件,这些数据最终都会存放到HDFS</p><p>当我们<strong>删除表的时候，表对应的文件夹会被删除，同时数据也会被删除</strong></p><p><strong>默认建表的类型就是内部表</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> 内部表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> students_internal</span><br><span class="line">(</span><br><span class="line">    id <span class="type">bigint</span>,</span><br><span class="line">    name string,</span><br><span class="line">    age <span class="type">int</span>,</span><br><span class="line">    gender string,</span><br><span class="line">    clazz string</span><br><span class="line">)</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">LOCATION <span class="string">&#x27;/input2&#x27;</span>;</span><br><span class="line"></span><br><span class="line">hive<span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>usr<span class="operator">/</span><span class="keyword">local</span><span class="operator">/</span>soft<span class="operator">/</span>data<span class="operator">/</span>students.txt <span class="operator">/</span>input2<span class="operator">/</span>;</span><br></pre></td></tr></table></figure><h4 id="2-4-1-Hive外部表"><a href="#2-4-1-Hive外部表" class="headerlink" title="2.4.1    Hive外部表"></a>2.4.1    Hive外部表</h4><blockquote><p>外部表说明</p><p>​    <strong>外部表因为是指定其他的hdfs路径的数据加载到表中来，所以hive会认为自己不完全独占这份数据</strong></p><p>​    <strong>删除hive表的时候，数据仍然保存在hdfs中，不会删除。</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> 外部表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> students_external</span><br><span class="line">(</span><br><span class="line">    id <span class="type">bigint</span>,</span><br><span class="line">    name string,</span><br><span class="line">    age <span class="type">int</span>,</span><br><span class="line">    gender string,</span><br><span class="line">    clazz string</span><br><span class="line">)</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">LOCATION <span class="string">&#x27;/input3&#x27;</span>;</span><br><span class="line"></span><br><span class="line">hive<span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>usr<span class="operator">/</span><span class="keyword">local</span><span class="operator">/</span>soft<span class="operator">/</span>data<span class="operator">/</span>students.txt <span class="operator">/</span>input3<span class="operator">/</span>;</span><br></pre></td></tr></table></figure><blockquote><p>删除表测试一下：</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">drop</span> <span class="keyword">table</span> students_internal;</span><br><span class="line">Moved: <span class="string">&#x27;hdfs://master:9000/input2&#x27;</span> <span class="keyword">to</span> trash <span class="keyword">at</span>: hdfs:<span class="operator">/</span><span class="operator">/</span>master:<span class="number">9000</span><span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>root<span class="operator">/</span>.Trash<span class="operator">/</span><span class="keyword">Current</span></span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.474</span> seconds</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">drop</span> <span class="keyword">table</span> students_external;</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.09</span> seconds</span><br><span class="line">hive<span class="operator">&gt;</span> </span><br></pre></td></tr></table></figure><blockquote><p>一般在公司中，使用外部表多一点，因为数据可以需要被多个程序使用，避免误删，通常外部表会结合location一起使用</p><p>外部表还可以将其他数据源中的数据 映射到 hive中，比如说：hbase，ElasticSearch……</p><p>设计外部表的初衷就是 让 表的元数据 与 数据 解耦</p></blockquote><ul><li>操作案例:  分别创建dept，emp，salgrade。并加载数据。</li></ul><blockquote><p>创建数据文件存放的目录</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hdfs dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p <span class="operator">/</span>shujia<span class="operator">/</span>bigdata17<span class="operator">/</span>dept</span><br><span class="line">hdfs dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p <span class="operator">/</span>shujia<span class="operator">/</span>bigdata17<span class="operator">/</span>emp</span><br><span class="line">hdfs dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p <span class="operator">/</span>shujia<span class="operator">/</span>bigdata17<span class="operator">/</span>salgrade</span><br></pre></td></tr></table></figure><ul><li>创建dept表</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> dept (</span><br><span class="line">  DEPTNO <span class="type">int</span>,</span><br><span class="line">  DNAME <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">  LOC <span class="type">varchar</span>(<span class="number">255</span>)</span><br><span class="line">) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">location <span class="string">&#x27;/shujia/bigdata17/dept&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="number">10</span>,ACCOUNTING,<span class="keyword">NEW</span> YORK</span><br><span class="line"><span class="number">20</span>,RESEARCH,DALLAS</span><br><span class="line"><span class="number">30</span>,SALES,CHICAGO</span><br><span class="line"><span class="number">40</span>,OPERATIONS,BOSTON</span><br></pre></td></tr></table></figure><ul><li>创建emp表</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> emp (</span><br><span class="line">   EMPNO <span class="type">int</span>,</span><br><span class="line">   ENAME <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">   JOB <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">   MGR <span class="type">int</span>,</span><br><span class="line">   HIREDATE <span class="type">date</span>,</span><br><span class="line">   SAL <span class="type">decimal</span>(<span class="number">10</span>,<span class="number">0</span>),</span><br><span class="line">   COMM <span class="type">decimal</span>(<span class="number">10</span>,<span class="number">0</span>),</span><br><span class="line">   DEPTNO <span class="type">int</span></span><br><span class="line"> ) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line"> location <span class="string">&#x27;/shujia/bigdata17/emp&#x27;</span>;</span><br><span class="line"> </span><br><span class="line"><span class="number">7369</span>,SMITH,CLERK,<span class="number">7902</span>,<span class="number">1980</span><span class="number">-12</span><span class="number">-17</span>,<span class="number">800</span>,<span class="keyword">null</span>,<span class="number">20</span></span><br><span class="line"><span class="number">7499</span>,ALLEN,SALESMAN,<span class="number">7698</span>,<span class="number">1981</span><span class="number">-02</span><span class="number">-20</span>,<span class="number">1600</span>,<span class="number">300</span>,<span class="number">30</span></span><br><span class="line"><span class="number">7521</span>,WARD,SALESMAN,<span class="number">7698</span>,<span class="number">1981</span><span class="number">-02</span><span class="number">-22</span>,<span class="number">1250</span>,<span class="number">500</span>,<span class="number">30</span></span><br><span class="line"><span class="number">7566</span>,JONES,MANAGER,<span class="number">7839</span>,<span class="number">1981</span><span class="number">-04</span><span class="number">-02</span>,<span class="number">2975</span>,<span class="keyword">null</span>,<span class="number">20</span></span><br><span class="line"><span class="number">7654</span>,MARTIN,SALESMAN,<span class="number">7698</span>,<span class="number">1981</span><span class="number">-09</span><span class="number">-28</span>,<span class="number">1250</span>,<span class="number">1400</span>,<span class="number">30</span></span><br><span class="line"><span class="number">7698</span>,BLAKE,MANAGER,<span class="number">7839</span>,<span class="number">1981</span><span class="number">-05</span><span class="number">-01</span>,<span class="number">2850</span>,<span class="keyword">null</span>,<span class="number">30</span></span><br><span class="line"><span class="number">7782</span>,CLARK,MANAGER,<span class="number">7839</span>,<span class="number">1981</span><span class="number">-06</span><span class="number">-09</span>,<span class="number">2450</span>,<span class="keyword">null</span>,<span class="number">10</span></span><br><span class="line"><span class="number">7788</span>,SCOTT,ANALYST,<span class="number">7566</span>,<span class="number">1987</span><span class="number">-07</span><span class="number">-13</span>,<span class="number">3000</span>,<span class="keyword">null</span>,<span class="number">20</span></span><br><span class="line"><span class="number">7839</span>,KING,PRESIDENT,<span class="keyword">null</span>,<span class="number">1981</span><span class="number">-11</span><span class="number">-17</span>,<span class="number">5000</span>,<span class="keyword">null</span>,<span class="number">10</span></span><br><span class="line"><span class="number">7844</span>,TURNER,SALESMAN,<span class="number">7698</span>,<span class="number">1981</span><span class="number">-09</span><span class="number">-08</span>,<span class="number">1500</span>,<span class="number">0</span>,<span class="number">30</span></span><br><span class="line"><span class="number">7876</span>,ADAMS,CLERK,<span class="number">7788</span>,<span class="number">1987</span><span class="number">-07</span><span class="number">-13</span>,<span class="number">1100</span>,<span class="keyword">null</span>,<span class="number">20</span></span><br><span class="line"><span class="number">7900</span>,JAMES,CLERK,<span class="number">7698</span>,<span class="number">1981</span><span class="number">-12</span><span class="number">-03</span>,<span class="number">950</span>,<span class="keyword">null</span>,<span class="number">30</span></span><br><span class="line"><span class="number">7902</span>,FORD,ANALYST,<span class="number">7566</span>,<span class="number">1981</span><span class="number">-12</span><span class="number">-03</span>,<span class="number">3000</span>,<span class="keyword">null</span>,<span class="number">20</span></span><br><span class="line"><span class="number">7934</span>,MILLER,CLERK,<span class="number">7782</span>,<span class="number">1982</span><span class="number">-01</span><span class="number">-23</span>,<span class="number">1300</span>,<span class="keyword">null</span>,<span class="number">10</span></span><br></pre></td></tr></table></figure><ul><li>创建salgrade表</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> salgrade (</span><br><span class="line">  GRADE <span class="type">int</span>,</span><br><span class="line">  LOSAL <span class="type">int</span>,</span><br><span class="line">  HISAL <span class="type">int</span></span><br><span class="line">) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">location <span class="string">&#x27;/shujia/bigdata17/salgrade&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="number">1</span>,<span class="number">700</span>,<span class="number">1200</span></span><br><span class="line"><span class="number">2</span>,<span class="number">1201</span>,<span class="number">1400</span></span><br><span class="line"><span class="number">3</span>,<span class="number">1401</span>,<span class="number">2000</span></span><br><span class="line"><span class="number">4</span>,<span class="number">2001</span>,<span class="number">3000</span></span><br><span class="line"><span class="number">5</span>,<span class="number">3001</span>,<span class="number">9999</span></span><br></pre></td></tr></table></figure><h3 id="2-5-Hive导出数据"><a href="#2-5-Hive导出数据" class="headerlink" title="2.5    Hive导出数据"></a>2.5    Hive导出数据</h3><blockquote><p>将表中的数据备份</p></blockquote><p>将查询结果存放到本地</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span>创建存放数据的目录</span><br><span class="line">mkdir <span class="operator">-</span>p <span class="operator">/</span>usr<span class="operator">/</span><span class="keyword">local</span><span class="operator">/</span>soft<span class="operator">/</span>shujia</span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>导出查询结果的数据(导出到Node01上)</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/usr/local/soft/shujia/person_data&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_person;</span><br></pre></td></tr></table></figure><p>按照指定的方式将数据输出到本地</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建存放数据的目录</span></span><br><span class="line">mkdir <span class="operator">-</span>p <span class="operator">/</span>usr<span class="operator">/</span><span class="keyword">local</span><span class="operator">/</span>soft<span class="operator">/</span>shujia</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 导出查询结果的数据</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/usr/local/soft/shujia/person&#x27;</span> </span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span> </span><br><span class="line">collection items terminated <span class="keyword">by</span> <span class="string">&#x27;-&#x27;</span> </span><br><span class="line">map keys terminated <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span> </span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span> </span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_person;</span><br></pre></td></tr></table></figure><p>将查询结果输出到HDFS</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建存放数据的目录</span></span><br><span class="line">hdfs dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p <span class="operator">/</span>shujia<span class="operator">/</span>bigdata17<span class="operator">/</span><span class="keyword">copy</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 导出查询结果的数据</span></span><br><span class="line"><span class="keyword">insert</span> overwrite directory <span class="string">&#x27;/shujia/bigdata17/user&#x27;</span> </span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span> </span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_user;</span><br></pre></td></tr></table></figure><p>直接使用HDFS命令保存表对应的文件夹</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">// 创建存放数据的目录</span><br><span class="line">hdfs dfs -mkdir -p /shujia/bigdata17/person</span><br><span class="line"></span><br><span class="line">// 使用HDFS命令拷贝文件到其他目录</span><br><span class="line">hdfs dfs -cp /hive/warehouse/t_person/*  /shujia/bigdata17/person</span><br></pre></td></tr></table></figure><p>将表结构和数据同时备份将数据导出到HDFS</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span>创建存放数据的目录</span><br><span class="line">hdfs dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p <span class="operator">/</span>shujia<span class="operator">/</span>bigdata17<span class="operator">/</span><span class="keyword">copy</span></span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>导出查询结果的数据</span><br><span class="line">export <span class="keyword">table</span> t_person <span class="keyword">to</span> <span class="string">&#x27;/shujia/bigdata17/copy&#x27;</span>;</span><br></pre></td></tr></table></figure><p>删除表结构</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> t_person;</span><br></pre></td></tr></table></figure><p>恢复表结构和数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">import <span class="keyword">from</span> <span class="string">&#x27;/shujia/bigdata17&#x27;</span>;</span><br></pre></td></tr></table></figure><blockquote><p>注意：时间不同步，会导致导入导出失败</p></blockquote><p>hive表查询时使用中文别名</p><p>在hive查询时 使用英文别名是没有任何问题的，</p><p>SELECT st.source_task_order A, st.creation_date B FROM tr_source_task st;</p><p>但是有某些特殊需求，需要使用中文别名时</p><p><strong>解决方法：</strong></p><p>将中文别名用<strong>反单引号</strong>（ tab键上面的那个键可以敲出来）引起来即可。</p><p>SELECT unit_name as <code>单位名称</code> FROM table_company_task;</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库概述</title>
      <link href="/2022/05/31/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A6%82%E8%BF%B0/"/>
      <url>/2022/05/31/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h2><h3 id="一、数据库、数据仓库概述"><a href="#一、数据库、数据仓库概述" class="headerlink" title="一、数据库、数据仓库概述"></a>一、数据库、数据仓库概述</h3><blockquote><p>如今，随着诸如互联网以及物联网等技术的不断发展，越来越多的数据被生产出来。据统计，每天大约有超过2.5亿亿字节的各种各样数据产生。这些数据需要被存储起来并且能够被方便的分析和利用。</p><p>随着大数据技术的不断更新和迭代，数据管理工具得到了飞速的发展，相关概念如雨后春笋一般应运而生，如从最初决策支持系统(DSS)到商业智能(BI)、数据仓库、数据湖、数据中台等，这些概念特别容易混淆，本文对这些名词术语及内涵进行系统的解析，便于读者对数据平台相关的概念有全面的认识。</p></blockquote><h3 id="1-1-数据库"><a href="#1-1-数据库" class="headerlink" title="1.1 数据库"></a>1.1 数据库</h3><blockquote><p>关系数据库本质上是一个二元关系，说的简单一些，就是一个二维表格，对普通人来说，最简单的理解就是一个Excel表格。这种数据库类型，具有结构化程度高，独立性强，冗余度低等等优点，一下子就促进了计算机的发展。</p></blockquote><h3 id="1-2-操作型数据库和分析型数据库"><a href="#1-2-操作型数据库和分析型数据库" class="headerlink" title="1.2    操作型数据库和分析型数据库"></a>1.2    操作型数据库和分析型数据库</h3><p>随着关系数据库理论的提出，诞生了一系列经典的RDBMS，如Oracle，MySQL，SQL Server等。这些RDBMS被成功推向市场，并为社会信息化的发展做出的重大贡献。然而随着数据库使用范围的不断扩大，它被逐步划分为两大基本类型：</p><h4 id="操作型数据库"><a href="#操作型数据库" class="headerlink" title="操作型数据库"></a>操作型数据库</h4><blockquote><p>主要用于业务支撑。一个公司往往会使用并维护若干个操作型数据库，这些数据库保存着公司的日常操作数据，比如商品购买、酒店预订、学生成绩录入等；</p></blockquote><h4 id="分析型数据库"><a href="#分析型数据库" class="headerlink" title="分析型数据库"></a>分析型数据库</h4><blockquote><p>主要用于历史数据分析。这类数据库作为公司的单独数据存储，负责利用历史数据对公司各主题域进行统计分析；</p><p>那么为什么要”分家”？在一起不合适吗？能不能构建一个同样适用于操作和分析的统一数据库？答案是NO。一个显然的原因是它们会”打架”…如果操作型任务和分析型任务抢资源怎么办呢？再者，它们有太多不同，以致于早已”貌合神离”。</p></blockquote><h3 id="1-3-操作型数据库-VS-分析型数据库"><a href="#1-3-操作型数据库-VS-分析型数据库" class="headerlink" title="1.3    操作型数据库 VS 分析型数据库"></a>1.3    操作型数据库 VS 分析型数据库</h3><p>因为主导功能的不同(面向操作&#x2F;面向分析)，两类数据库就产生了很多细节上的差异。这就好像同样是人，但一个和尚和一个穆斯林肯定有很多行为&#x2F;观念上的不同。</p><p>接下来本文将详细分析两类数据库的不同点：</p><h4 id="1-数据组成差别-数据时间范围差别"><a href="#1-数据组成差别-数据时间范围差别" class="headerlink" title="1) 数据组成差别 - 数据时间范围差别"></a>1) 数据组成差别 - 数据时间范围差别</h4><blockquote><p>一般来讲，操作型数据库只会存放90天以内的数据，而分析型数据库存放的则是数年内的数据。这点也是将操作型数据和分析型数据进行物理分离的主要原因。</p></blockquote><h4 id="2-数据组成差别-数据细节层次差别"><a href="#2-数据组成差别-数据细节层次差别" class="headerlink" title="2) 数据组成差别 - 数据细节层次差别"></a>2) 数据组成差别 - 数据细节层次差别</h4><blockquote><p>操作型数据库存放的主要是细节数据，而分析型数据库中虽然既有细节数据，又有汇总数据，但对于用户来说，重点关注的是汇总数据部分。</p><p>操作型数据库中自然也有汇总需求，但汇总数据本身不存储而只存储其生成公式。这是因为操作型数据是动态变化的，因此汇总数据会在每次查询时动态生成。</p><p>而对于分析型数据库来说，因为汇总数据比较稳定不会发生改变，而且其计算量也比较大(因为时间跨度大)，因此它的汇总数据可考虑事先计算好，以避免重复计算。</p></blockquote><h4 id="3-数据组成差别-数据时间表示差别"><a href="#3-数据组成差别-数据时间表示差别" class="headerlink" title="3) 数据组成差别 - 数据时间表示差别"></a>3) 数据组成差别 - 数据时间表示差别</h4><blockquote><p>操作型数据通常反映的是现实世界的当前状态；而分析型数据库既有当前状态，还有过去各时刻的快照，分析型数据库的使用者可以综合所有快照对各个历史阶段进行统计分析。</p></blockquote><h4 id="4-技术差别-查询数据总量和查询频度差别"><a href="#4-技术差别-查询数据总量和查询频度差别" class="headerlink" title="4) 技术差别 - 查询数据总量和查询频度差别"></a>4) 技术差别 - 查询数据总量和查询频度差别</h4><blockquote><p>操作型查询的数据量少而频率多，分析型查询则反过来，数据量大而频率少。要想同时实现这两种情况的配置优化是不可能的，这也是将两类数据库物理分隔的原因之一。</p></blockquote><h4 id="5-技术差别-数据更新差别"><a href="#5-技术差别-数据更新差别" class="headerlink" title="5) 技术差别 - 数据更新差别"></a>5) 技术差别 - 数据更新差别</h4><blockquote><p>操作型数据库允许用户进行增，删，改，查；分析型数据库用户则只能进行查询。</p></blockquote><h4 id="6-技术差别-数据冗余差别"><a href="#6-技术差别-数据冗余差别" class="headerlink" title="6) 技术差别 - 数据冗余差别"></a>6) 技术差别 - 数据冗余差别</h4><blockquote><p>数据的意义是什么？就是减少数据冗余，避免更新异常。而如5所述，分析型数据库中没有更新操作。因此，减少数据冗余也就没那么重要了。</p><p>现在回到开篇是提到的第二个问题”某大公司Hadoop Hive里的关系表不完全满足完整&#x2F;参照性约束，也不完全满足范式要求，甚至第一范式都不满足。这种情况正常吗？”，答曰是正常的。因为Hive是一种数据仓库，而数据仓库和分析型数据库的关系非常紧密(后文会讲到)。它只提供查询接口，不提供更新接口，这就使得消除冗余的诸多措施不需要被特别严格地执行了。</p></blockquote><h4 id="7-功能差别-数据读者差别"><a href="#7-功能差别-数据读者差别" class="headerlink" title="7) 功能差别 - 数据读者差别"></a>7) 功能差别 - 数据读者差别</h4><blockquote><p>操作型数据库的使用者是业务环境内的各个角色，如用户，商家，进货商等；分析型数据库则只被少量用户用来做综合性决策。</p></blockquote><h4 id="8-功能差别-数据定位差别"><a href="#8-功能差别-数据定位差别" class="headerlink" title="8) 功能差别 - 数据定位差别"></a>8) 功能差别 - 数据定位差别</h4><blockquote><p>这里说的定位，主要是指以何种目的组织起来。操作型数据库是为了支撑具体业务的，因此也被称为”面向应用型数据库”；分析型数据库则是针对各特定业务主题域的分析任务创建的，因此也被称为”面向主题型数据库”。</p></blockquote><h3 id="2-1-数据仓库概述"><a href="#2-1-数据仓库概述" class="headerlink" title="2.1    数据仓库概述"></a>2.1    数据仓库概述</h3><p><strong>数据仓库之父比尔·恩门，1991年提出</strong></p><blockquote><p>数据仓库就是为了解决数据库不能解决的问题而提出的。那么数据库无法解决什么样的问题呢？这个我们得先说说什么是OLAP和OLTP。**(重点)**</p></blockquote><h3 id="2-2-OLTP和OLAP（重点）"><a href="#2-2-OLTP和OLAP（重点）" class="headerlink" title="2.2 OLTP和OLAP（重点）"></a>2.2 OLTP和OLAP（重点）</h3><h4 id="2-2-1-OLTP"><a href="#2-2-1-OLTP" class="headerlink" title="2.2.1 OLTP"></a>2.2.1 OLTP</h4><blockquote><p>OLTP（OnLine Transaction Processing 联机事务处理） 。简单一些，就是数据库的增删查改。举个例子，你到银行，去取一笔钱出来，或者转账，或者只是想查一下你还有多少存款，这些都是面向“事务”类型的操作。这样的操作有几个显著的特点:</p><p>首先要求速度很快， 基本上都是高可靠的在线操作（比如银行）， 还有这些操作涉及的数据内容不会特别大（否则速度也就相应的降低）， 最后，“事务”型的操作往往都要求是精准操作，比如你去银行取款，必须要求一个具体的数字，你是不可能对着柜台员工说我大概想取400到500快之间吧，那样人家会一脸懵逼。</p></blockquote><h4 id="2-2-2-OLAP"><a href="#2-2-2-OLAP" class="headerlink" title="2.2.2 OLAP"></a>2.2.2 OLAP</h4><blockquote><p>这个东西又是上面发明关系型数据库的科德发明的。OLAP略有复杂，但这里我举一个简单的例子，大家就很容易理解了。</p><p>比如说，沃尔玛超市的数据库里有很多张表格，记录着各个商品的交易记录。超市里销售一种运动饮料，我们不妨称之为红牛。数据库中有一张表A，记录了红牛在一年的各个月份的销售额；还有一张表B，记录了红牛每个月在美国各个州的销售额：；甚至还有一张表C，记录了这家饮料公司在每个州对红牛饮料的宣传资金投入；甚至后来沃尔玛又从国家气象局拿到了美国各个州的一年365天每天的天气表D。好，最后问题来了，请根据以上数据分析红牛在宣传资金不超过三百万的情况下，什么季节，什么天气，美国哪个州最好卖？凭借我们的经验，可能会得出，夏季的晴天，在美国的佛罗里达，最好卖，而且宣传资金投入越高销售额应该也会高。可能这样的结论是正确的，但决策者想要看到的是确凿的数据结论，而不是“可能”这样的字眼。</p><p>科学是不相信直觉的，如果我们人工进行手动分析，会发现这个要考虑的维度实在太多了，根本无法下手，何况这才四五个维度，要是更多了怎么办？OLAP就是为了解决这样的问题诞生的，但糟糕的是，传统数据库是无法满足OLAP所需要的数据信息的。</p></blockquote><h3 id="2-3-数据仓库概念"><a href="#2-3-数据仓库概念" class="headerlink" title="2.3    数据仓库概念"></a>2.3    数据仓库概念</h3><h4 id="2-3-1-概述"><a href="#2-3-1-概述" class="headerlink" title="2.3.1 概述"></a>2.3.1 概述</h4><p>数据库的大规模应用，使得信息行业的数据爆炸式的增长，为了研究数据之间的关系，挖掘数据隐藏的价值，人们越来越多的需要使用OLAP来为决策者进行分析，探究一些深层次的关系和信息。但很显然，不同的数据库之间根本做不到数据共享，就算同一家数据库公司，数据库之间的集成也存在非常大的挑战（最主要的问题是庞大的数据如何有效合并、存储）。</p><p>1988年，为解决企业的数据集成问题，IBM的两位研究员（Barry Devlin和Paul Murphy）创造性地提出了一个新的术语：数据仓库（Data Warehouse）。但只是将这个名词作为市场宣传的花哨概念，并没有在技术领域有什么实质性的研究和突破。</p><p>然而，尽管IBM不为所动，其他企业却在加紧对数据仓库的研究和开发，大家都想在这个领域寻找到第一桶金。终于，到了1992年，后来被誉为“数据仓库之父”的比尔 恩门（Bill Inmon）给出了数据仓库的定义，二十多年后的今天他的定义依然没有被时代淘汰。我们来看看他是怎么定义的：<strong>数据仓库是一个面向主题的、集成的、相对稳定的、反映历史变化的数据集合，用于支持管理中的决策制定。</strong></p><p>对于数据仓库的概念我们可以从两个层次予以理解：</p><p>首先,数据仓库用于支持决策,面向分析型数据处理,它不同于企业现有的操作型数据库; 其次,数据仓库是对多个异构的数据源有效集成,集成后按照主题进行了重组,并包含历史数据,而且存放在数据仓库中的数据一般不再修改。</p><p>我们可以不用管这个定义，简单的理解，其实就是我们为了进行OLAP，把分布在各个散落独立的数据库孤岛整合在了一个数据结构里面，称之为数据仓库。</p><p>这个数据仓库在技术上是怎么建立的读者朋友们并不需要关心，但是我们要知道，原来各个数据孤岛中的数据，可能会在物理位置（比如沃尔玛在各个州可能都有自己的数据中心）、存储格式（比如月份是数值类型，但但天气可能是字符类型）、商业平台（不同数据库可能用的是Oracle数据库，有的是微软SQL Server数据库）、编写的语言（Java或者Scale等）等等各个方面完全不同，数据仓库要做的工作就是将他们按照所需要的格式提取出来，再进行必要的转换（统一数据格式）、清洗（去掉无效或者不需要的数据）等，最后装载进数据仓库（我们所说的ETL工具就是用来干这个的）。这样，拿我们上面红牛的例子来说，所有的信息就统一放在了数据仓库中了。</p><p>自从数据仓库出现之后，信息产业就开始从以关系型数据库为基础的运营式系统慢慢向决策支持系统发展。这个决策支持系统，其实就是我们现在说的商务智能（Business Intelligence）即BI。</p><p>可以这么说，数据仓库为OLAP解决了数据来源问题，数据仓库和OLAP互相促进发展，进一步驱动了商务智能的成熟，但真正将商务智能赋予“智能”的，正是我们现在热谈的下一代技术：数据挖掘。</p><h4 id="2-3-2-数据仓库特点-重点"><a href="#2-3-2-数据仓库特点-重点" class="headerlink" title="2.3.2 数据仓库特点(重点)"></a>2.3.2 数据仓库特点(<strong>重点</strong>)</h4><h5 id="面向主题"><a href="#面向主题" class="headerlink" title="面向主题"></a><strong>面向主题</strong></h5><blockquote><p>面向主题特性是数据仓库和操作型数据库的根本区别。</p><p>操作型数据库是为了支撑各种业务而建立。</p><p>而分析型数据库则是为了对从各种繁杂业务中抽象出来的分析主题(如用户、成本、商品等)进行分析而建立；所谓主题：是指用户使用数据仓库进行决策时所关心的重点方面，如：收入、客户、销售渠道等；所谓面向主题，是指数据仓库内的信息是按主题进行组织的，而不是像业务支撑系统那样是按照业务功能进行组织的。</p></blockquote><h5 id="集成性"><a href="#集成性" class="headerlink" title="集成性"></a>集成性</h5><blockquote><p>集成性是指数据仓库会将不同源数据库中的数据汇总到一起；</p><p>具体来说，是指数据仓库中的信息不是从各个业务系统中简单抽取出来的，而是经过一系列加工、整理和汇总的过程，因此数据仓库中的信息是关于整个企业的一致的全局信息。</p></blockquote><h5 id="企业范围"><a href="#企业范围" class="headerlink" title="企业范围"></a>企业范围</h5><blockquote><p>数据仓库内的数据是面向公司全局的。比如某个主题域为成本，则全公司和成本有关的信息都会被汇集进来；</p></blockquote><h5 id="历史性"><a href="#历史性" class="headerlink" title="历史性"></a>历史性</h5><blockquote><p>较之操作型数据库，数据仓库的时间跨度通常比较长。前者通常保存几个月，后者可能几年甚至几十年；</p></blockquote><h5 id="时变性"><a href="#时变性" class="headerlink" title="时变性"></a>时变性</h5><blockquote><p>时变性是指数据仓库包含来自其时间范围不同时间段的数据快照。有了这些数据快照以后，用户便可将其汇总，生成各历史阶段的数据分析报告；</p><p>数据仓库内的信息并不只是反映企业当前的状态，而是记录了从过去某一时点到当前各个阶段的信息。通过这些信息，可以对企业的发展历程和未来趋势做出定量分析和预测。</p></blockquote><h3 id="2-4-数据仓库的趋势"><a href="#2-4-数据仓库的趋势" class="headerlink" title="2.4    数据仓库的趋势"></a>2.4    <strong>数据仓库的趋势</strong></h3><ul><li>实时数据仓库以满足实时化&amp;自动化决策需求；</li><li>大数据&amp;数据湖以支持大量&amp;复杂数据类型（文本、图像、视频、音频）；</li></ul><p><img src="https://s2.loli.net/2022/05/31/MVoEpGJ98mltv26.png" alt="image-20220530231142943.png"></p><h3 id="2-5-数据仓库的发展"><a href="#2-5-数据仓库的发展" class="headerlink" title="2.5    数据仓库的发展"></a>2.5    <strong>数据仓库的发展</strong></h3><blockquote><p>数据仓库有两个环节：<strong>数据仓库的构建</strong>与<strong>数据仓库的应用</strong>。</p><p>早期数据仓库构建主要指的是把企业的业务数据库如ERP、CRM、SCM等数据按照决策分析的要求建模并汇总到数据仓库引擎中，其应用以报表为主，目的是支持管理层和业务人员决策（中长期策略型决策）。</p><p>随着业务和环境的发展，这两方面都在发生着剧烈变化。</p><ul><li>随着IT技术走向互联网、移动化，数据源变得越来越丰富，在原来业务数据库的基础上出现了非结构化数据，比如网站log，IoT设备数据，APP埋点数据等，这些数据量比以往结构化的数据大了几个量级，对ETL过程、存储都提出了更高的要求；</li><li>互联网的在线特性也将业务需求推向了实时化，随时根据当前客户行为而调整策略变得越来越常见，比如大促过程中库存管理，运营管理等（即既有中远期策略型，也有短期操作型）；同时公司业务互联网化之后导致同时服务的客户剧增，有些情况人工难以完全处理，这就需要机器自动决策。比如欺诈检测和用户审核。</li></ul></blockquote><p><img src="https://s2.loli.net/2022/05/31/xkLpAqZB4QbJTdo.png" alt="image-20220530231227848.png"></p><p>总结来看，对数据仓库的需求可以抽象成两方面：<strong>实时产生结果、处理和保存大量异构数据</strong>。</p><h3 id="2-6-数据仓库建设方法论"><a href="#2-6-数据仓库建设方法论" class="headerlink" title="2.6    数据仓库建设方法论"></a>2.6    <strong>数据仓库建设方法论</strong></h3><blockquote><p><strong>1）面向主题</strong></p><p>从公司业务出发，是分析的宏观领域，比如供应商主题、商品主题、客户主题和仓库主题</p><p><strong>2）为多维数据分析服务</strong></p><p>数据报表；数据立方体，上卷、下钻、切片、旋转等分析功能。</p><p><strong>3）反范式数据模型</strong></p><p>以事实表和维度表组成的星型数据模型</p></blockquote><p><img src="https://s2.loli.net/2022/05/31/GZpvP38h4fKtlzk.png" alt="image-20220530231437440.png"></p><blockquote><p>数据仓库层的划分：</p></blockquote><p><img src="https://s2.loli.net/2022/06/01/E9Cn7JUyhBX31ck.png" alt="image-20220601194719608"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop的一些优化</title>
      <link href="/2022/05/27/Hadoop%E6%A1%88%E4%BE%8B%E5%92%8C%E4%BC%98%E5%8C%96/"/>
      <url>/2022/05/27/Hadoop%E6%A1%88%E4%BE%8B%E5%92%8C%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h1 id="Hadoop优化"><a href="#Hadoop优化" class="headerlink" title="Hadoop优化"></a>Hadoop优化</h1><h2 id="优化1：Combiner"><a href="#优化1：Combiner" class="headerlink" title="优化1：Combiner"></a>优化1：Combiner</h2><blockquote><p>减少了reduce 从map拉取数据的过程，提高计算效率。</p><p>hadoop 的计算特点：<strong>将计算任务向数据靠拢，而不是将数据向计算靠拢。</strong></p><p>特点：数据本地化，减少网络io。</p><p>首先需要知道，hadoop数据本地化是指的map任务，reduce任务并不具备数据本地化特征。<br>   通常输入的数据首先在<strong>逻辑上</strong>（<strong>注意这里不是真正物理上划分</strong>）将会分片split，每个分片上构建一个map任务，由该任务执行执行用户自定义的map函数，从而处理分片中的每条记录。<br>   那么切片的大小一般是趋向一个HDFS的block块的大小。为什么最佳的分片大小是趋向block块的大小呢？是因为这样能够确保单节点上最大输入块的大小，如果分片跨越两个数据块，没有一个block能够同时存储这两块数据，因此需要通过网络传输将部分数据传输到map任务节点上。这样明显比使用本地数据的map效率更低。<br>    注意，map任务执行后的结果并没有写到HDFS中，而是作为中间结果存储到本地硬盘，那为什么没有存储到HDFS呢？因为，该中间结果会被reduce处理后产生最终结果后，该中间数据会被删除，如果存储到HDFS中，他会进行备份，这样明显没有意义。如果map将中间结果传输到reduce过程中出现了错误，Hadoop会在另一个节点上重新执行map产生中间结果。<br>    那么为什么reduce没有数据本地化的特点呢？对于单个reduce任务来说，他的输入通常是所有mapper经过排序输出，这些输出通过网络传输到reduce节点，数据在reduce节点合并然后由reduce函数进行处理。最终结果输出到HDFS上。当多个有reduce任务的时候，map会针对输出进行分区partition，也就是为每个reduce构建一个分区，分区是由用户指定的partition函数，效率很高。<br>   同时为了高效传输可以指定combiner函数，他的作用就是，<strong>减少网络传输和本地传输</strong></p><p>假设文件是500mb</p><p>long bytesRemaining &#x3D; length; 500mb</p><p>​     while (((double) bytesRemaining)&#x2F;splitSize &gt; SPLIT_SLOP) {</p><p>​      int blkIndex &#x3D; getBlockIndex(blkLocations, length-bytesRemaining  );</p><p>​      splits.add(makeSplit(path, length-bytesRemaining 256 , splitSize 128,</p><p>​            blkLocations[blkIndex].getHosts(),</p><p>​            blkLocations[blkIndex].getCachedHosts()));</p><p>​      bytesRemaining &#x3D; bytesRemaining-splitSize;116</p><p>​     }</p><p><strong>注意：将reduce端的聚合操作，放到map 进行执行。适合求和，计数，等一些等幂操作。不适合求平均值，次幂等类似操作</strong></p></blockquote><h2 id="优化2：Join（数据倾斜）"><a href="#优化2：Join（数据倾斜）" class="headerlink" title="优化2：Join（数据倾斜）"></a>优化2：Join（数据倾斜）</h2><blockquote><p>MapReduce中的join</p><p>　　其实就是类似于关系型数据库中的连接查询一样。需要计算的数据可能存储在不同的文件中或不同表中，两个文件又有一些相同的字段可以相互关联，这时候我们就可以通过这些关联字段将两个文件中的数据组合到一起进行计算了。</p><p>　　我知道的mr有三种join方式。Map join、SemiJoin、reduce join。</p><p>Reduce Join（我们之前做的代码连接就是这个方式）</p><p>思路：</p><p>　　分为两个阶段</p><p>　　 （1）map函数主要是对不同文件中的数据打标签。</p><p>　　（2）reduce函数获取key相同的value list，进行笛卡尔积。</p><p>Map Join思路：</p><p>　　比如有两个表，有一个表非常大，而另一个表非常小，以至于小表可以直接存放到内存中。这样，我们可以将小表复制多份，让每个map task内存中保存一个hash map，将小表数据放入这个hash map中，key是小表与大表的内个连接字段，value是小表一条记录，然后只扫描大表：对于大表中的每一条记录key&#x2F;value，在hash map中查找是否有相同的key的记录，如果有，则连接输出即可。</p><p><strong>Semi Join 这个SemiJoin其实就是对reduce join的一种优化。</strong></p><p>　　就是在map端过滤掉不参加join操作的数据，则可以大大减少数据量，提高网络传输速度。</p><p>这三种join方式适用于不同的场景：</p><p>　　Reduce join要考虑数据量过大时的网络传输问题。</p><p>　　Map join和SemiJoin则要考虑数据量过大时的内存问题。 如果只考虑网络传输，忽略内存问题则。</p><p>　　Map join效率最高，其次是SemiJoin，最低的是reduce join。</p><p>DistributedCache DistributedCache是Hadoop提供的文件缓存工具，它能够自动将指定的文件分发到各个节点上，缓存到本地，供用户程序读取使用。一般用户数据字典的分发，和map join使用。一般缓存的文件都是只读。</p></blockquote><h2 id="优化3：根据实际情况调整切片大小"><a href="#优化3：根据实际情况调整切片大小" class="headerlink" title="优化3：根据实际情况调整切片大小"></a>优化3：根据实际情况调整切片大小</h2><blockquote><p><strong>为什么默认切片是128MB和blk大小一致？（优化）</strong></p><p>1 切片大小默认一致，是为了数据本地化，减少数据拉取消耗网络io</p><p>2 并不是越大越好，也不是越小越好。根据集群的资源情况而定。</p><p> 当集群计算资源充足的情况下：将切片的大小调小，增加map数量，提高读取效率。</p><p> 当集群计算资源紧张的情况下：将切片的大小调大，减少资源占用，让任务正常运转。</p><p> mapred.min.split.size、mapred.max.split.size、blockSize</p></blockquote><h2 id="优化4：可以设置yarn资源和队列。"><a href="#优化4：可以设置yarn资源和队列。" class="headerlink" title="优化4：可以设置yarn资源和队列。"></a>优化4：可以设置yarn资源和队列。</h2><blockquote><p>调整计算资源：<a href="https://blog.csdn.net/qq_36753550/article/details/83065546">https://blog.csdn.net/qq_36753550/article/details/83065546</a></p><p> 设置队列：<a href="https://blog.csdn.net/weixin_30607029/article/details/96507281">https://blog.csdn.net/weixin_30607029/article/details/96507281</a></p><p>mr运行日志信息：百分比是按照完成的m或r的任务的个数&#x2F;m或r的总个数。</p><p>MRv1&#x2F;MRv2&#x2F;YARN MRv1:</p><p>　　对于经典的MRv1它由三部分组成 :</p><p>　　　　编程模型、 数据处理引擎和运行时环境。</p><p>　　　　编程模型由新旧 API 两部分组成，新旧api只是代码封装上略有变化，性能没变化。</p><p>　　　　数据处理引擎由 MapTask 和 ReduceTask 组成。 运行时环境由 JobTracker 和 TaskTracker 两类服务组成。</p><p>　　MRv2:</p><p>　　　　由于MRv1对JobTracker的功能过多造成负载过重在扩展性、 资源利用率和多框架支持等方面存在不足，因此MRv2框架 的基本设计思想是将MRv1中的JobTracker包含的资源管理和应用管理两部分功能进行拆分，分别交给两个进程实现。 资源管理进程与具体应用程序无关，它负责整个集群的资源管理（内存、 CPU、 磁盘）。 应用管理进程负责管理应用程序，并且每个应用管理进程只管理一个作业。 由于资源管理可以共享给其他框架使用，因此MRv2将其做成了一个通用的系统YARN,YARN系统使得MRv2计算框架在可扩展性，资源利用率，多框架支持方面得到了很大改进。</p><p>　　YARN：yarn由4部分组成。</p><p>　　　　1. ResourceManager主要功能是：</p><p>　　　　　　（1）接收用户请求</p><p>　　　　　　（2）管理调度资源</p><p>　　　　　　（3）启动管理am　　　　</p><p>　　　　　　（4）管理所有nm,处理nm的状态汇报，向nm下达命令。</p><p>　2.Container：yarn的应用都是运行在容器上的，容器包含cpu，内存等信息。</p><p>　3.NodeManager：NM是每个节点上的资源和任务管理器，它会定时地向RM汇报本节点上的资源使用情况和各个容器的运行状态；同时负责对容器的启动和停止。</p><p>　　　　4. ApplicationMaster：管理应用程序。向RM获取资源、为应用程序分配任务、 监控所有任务运行状态。</p><ol><li>作业提交</li></ol><p>　　首先我们将任务提交给JobClient,JobClient会向RM获取一个appId。 然后我们的JobClient会对作业进行处理, 切分InputSplit, 将作业的Jar包, 配置文件和拷贝InputSplit信息拷贝到HDFS。 最后, 通过调用RM的submitApplication()来提交作业。</p><ol start="2"><li>作业初始化</li></ol><p>　　当RM收到submitApplciation()的请求时, 就将该请求发给调度器, 调度器分配第一个容器, 然后RM在该容器内启动ApplicationMaster进程。该进程上运行着一个MRAppMaster的Java应用。其通过创造一些bookkeeping对象来监控作业的进度。 然后通过hdfs得到由JobClient已经处理好的作业信息。为每个Inputsplit创建一个map任务, 并创建相应的reduce任务。然后ApplicationMaster会对整个作业量进行判断，<strong>如果作业量很小, ApplicationMaster会选择在其自己的JVM中运行任务</strong>, <strong>这种作业称作是uber task的方式</strong>。在任务运行之前, 作业的<strong>setup</strong>方法被调用来创建输出路径。</p><ol start="3"><li>任务分配</li></ol><p>　　如果不是小作业, 那么ApplicationMaster向RM请求更多的容器来运行所有的map和reduce任务，<strong>每个容器只能对应一个任务</strong>。这些请求是通过心跳来传输的, 包括每个map任务的数据位置, 比如Inputsplit的主机名和机架。调度器利用这些信息来调度任务, 尽量将任务分配给有存储数据的节点, 或者分配给和存放Inputsplit的节点相同机架的节点。</p><ol start="4"><li>任务运行</li></ol><p>　　当一个任务由RM的调度器分配了一个容器后, ApplicationMaster与NM通信来启动容器。任务由一个为YarnChild的Java应用执行。在运行任务之前首先本地化任务需要的资源, 比如作业配置, JAR文件, 以及hdfs中保存的任务所需的所有文件。最后, map任务或者reduce运行在一个叫YarnChild的进程当中。</p><ol start="5"><li>进度和状态更新</li></ol><p>　　每个NM会向applicationmaster汇报自己的工作状态，JobClient会每秒轮询检测applicationmaster，这样就能随时收到更新信息。</p><ol start="6"><li>作业完成</li></ol><p>　　除了向applicationmaster请求作业进度外, JobClient每5分钟都会通过调用waitForCompletion()来检查作业是否完成。作业完成之后,applicationmaster和NM会清理工作状态, OutputCommiter的作业清理方法也会被调用. 作业的信息会被作业历史服务器存储以备之后用户核查.</p><p><strong>yarn对异常task的处理（推测执行）？(重要！！！)</strong></p><p>　　推测执行是在分布式环境下，因为某种原因造成同一个job的多个task运行速度不一致，有的task运行速度明显慢于其他task，则这些task拖慢了整个job的执行进度，为了避免这种情况发生，Hadoop会为该task启动备份任务，让该speculative task与原始task同时处理一份数据，哪个先运行完，则将谁的结果作为最终结果。推测执行优化机制采用了<strong>典型的以空间换时间的优化策略</strong>，它同时启动多个相同task（备份任务）处理相同的数据块，哪个完成的早，则采用哪个task的结果，这样可防止拖后腿Task任务出现，进而提高作业计算速度，但是，这样却会占用更多的资源。</p><p><strong>yarn调度器的策略？(重要！！！)</strong></p><p>　　yarn默认是计算能力调度 FifoScheduler:根据先进先出排队，最简单的调度器。  FIFO</p><p>​        CapacityScheduler(计算能力调度)、FairScheduler(公平调度)：</p><p>　　相同点：</p><p>　　　　(1)都是多队列。</p><p>　　　　(2)都有资源最大最小上线限制。</p><p>　　　　(3)都是资源共享，每个队列剩余的资源可以给其他队列使用。</p><p>　　不同点：</p><p>　　　　(1)队列排序算法不同：计算能力调度资源使用量小的优先。公平调度根据公平排序算法排序。</p><p>　　　　(2)应该用选择算法不同：计算能力调度是先进先出。公平调度先进先出或者公平排序算法。</p><p>　　　　(3)资源抢占：公平调度如果当前队列有新应用提交后，会把共享出去的资源抢夺回来。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop-MapReduce</title>
      <link href="/2022/05/26/Hadoop-MapReduce/"/>
      <url>/2022/05/26/Hadoop-MapReduce/</url>
      
        <content type="html"><![CDATA[<h3 id="一、MapReduce设计理念"><a href="#一、MapReduce设计理念" class="headerlink" title="一、MapReduce设计理念"></a>一、MapReduce设计理念</h3><blockquote><p>map—&gt;映射</p><p>reduce—&gt;归纳</p><p>mapreduce必须构建在hdfs之上的一种大数据离线计算框架</p><p>​        在线：实时数据处理</p><p>​        离线：数据处理时效性没有在线那么强，但是相对也需要很快得到结果</p><p>mapreduce不会马上得到结果，他会有一定的延时（磁盘IO）</p><p>​        如果数据量小，使用mapreduce反而不合适</p><p>​        杀鸡焉用宰牛刀</p><p>原始数据–&gt;map(Key,Value)–&gt;Reduce</p><p>分布式i计算</p><p>​        将大的数据切分成多个小数据，交给更多的节点参与运算</p><p>计算向数据靠拢</p><p>​        将计算传递给有数据的节点上进行工作</p></blockquote><h3 id="二、MapReduce架构特点"><a href="#二、MapReduce架构特点" class="headerlink" title="二、MapReduce架构特点"></a>二、MapReduce架构特点</h3><h4 id="MapReduce1-x"><a href="#MapReduce1-x" class="headerlink" title="MapReduce1.x"></a>MapReduce1.x</h4><blockquote><p><strong>JobTracker</strong></p><p>　　　主节点，单点，负责调度所有的作用和监控整个集群的资源负载。</p><p><strong>TaskTracker</strong></p><p>　　　从节点，自身节点资源管理和JobTracker进行心跳联系，汇报资源和获取task。</p><p><strong>Client</strong></p><p>　　　以作业为单位，规划作业计算分布，提交作业资源到HDFS，最终提交作业到JobTracker。</p><h5 id="MapReduce1-x的弊端"><a href="#MapReduce1-x的弊端" class="headerlink" title="MapReduce1.x的弊端"></a>MapReduce1.x的弊端</h5><p>　　1.JobTracker负载过重，存在单点故障。</p><p>　　2.资源管理和计算调度强耦合，其它计算框架难以复用其资源管理。</p><p>　　3.不同框架对资源不能全局管理。</p></blockquote><h4 id="MapReduce2-x"><a href="#MapReduce2-x" class="headerlink" title="MapReduce2.x"></a>MapReduce2.x</h4><blockquote><p>ResourceManager</p><p>　　　主节点，负责整个集群的资源管理。</p><p>NodeManager</p><p>　　　与ResourceManager汇报资源，管理Container生命周期，计算框架中的角色都以Container表示。</p><p>Container</p><p>　　　默认NodeManager启动线程监控Container大小，超出申请资源额度会kill掉。支持Linux内核的Cgroup。</p><p>Client</p><p>　　　ResourceManager-client：请求资源创建ApplicationMaster-client。</p><p>　　　ApplicationMaster-client：与ApplicationMaster交互。</p><p><strong>YARN【Yet Another Resource Negotiator】：Hadoop 2.0新引入的资源管理系统，直接从MRv1演化而来的。</strong></p><p><strong>核心思想：将MRv1中JobTracker的资源管理和任务调度两个功能分开，分别由ResourceManager和ApplicationMaster进程实现：</strong></p><p>　　　ResourceManager：负责整个集群的资源管理和调度。</p><p>　　　ApplicationMaster：负责应用程序相关的事务，比如任务调度、任务监控和容错等。</p><p>YARN的引入，使得多个计算框架可运行在一个集群中 每个应用程序对应一个ApplicationMaster 目前多个计算框架可以运行在YARN上，比如MapReduce、Spark、Storm等。</p></blockquote><h3 id="三、扑克牌的问题"><a href="#三、扑克牌的问题" class="headerlink" title="三、扑克牌的问题"></a>三、扑克牌的问题</h3><p><strong>你想数出一摞牌中有多少张黑桃，红桃，方块，梅花。直观方式是一张一张检查并且数出分别有多少张。</strong><br><strong>MapReduce方法则是：</strong><br>        1.给在座的所有玩家中分配这摞牌<br>        2.让每个玩家数自己手中的牌有几张是黑桃，然后把这个数目汇报给你<br>        3.你把所有玩家告诉你的数字加起来，得到最后的结论</p><h3 id="四、MR的计算流程"><a href="#四、MR的计算流程" class="headerlink" title="四、MR的计算流程"></a>四、MR的计算流程</h3><h4 id="4-1-原始数据File-可以从网上找一篇英文的文章"><a href="#4-1-原始数据File-可以从网上找一篇英文的文章" class="headerlink" title="4.1    原始数据File(可以从网上找一篇英文的文章)"></a>4.1    原始数据File(可以从网上找一篇英文的文章)</h4><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">The books chronicle the adventures of the adolescent wizard Harry Potter and his best friends Ron Weasley and Hermione Granger, all of whom are students at Hogwarts School of Witchcraft and Wizardry. </span><br></pre></td></tr></table></figure><blockquote><p>1T数据被切分成块存放在HDFS上，每一个块有128M大小</p></blockquote><h4 id="4-2-数据块Block"><a href="#4-2-数据块Block" class="headerlink" title="4.2    数据块Block"></a>4.2    数据块Block</h4><blockquote><p>block块是hdfs上存储的一个单元，同一个文件块的大小都是相同</p><p>因为数据存储到HDFS上不可变，所以有可能快的数量和集群的计算能力不匹配</p><p>我们需要一个动态调整本次参与计算节点数量的单位</p><p>我们可以动态的改变这个单位—&gt;参与的节点</p></blockquote><h4 id="5-3-切片Split"><a href="#5-3-切片Split" class="headerlink" title="5.3    切片Split"></a>5.3    切片Split</h4><blockquote><p>目的：动态地控制计算单元的数量 </p></blockquote><blockquote><p>切片是个逻辑概念</p><p>在不改变现有数据存储的情况下，可以控制参与计算的节点数目</p><p>通过切片大小可以达到控制计算节点数量的目的</p></blockquote><p><strong>有多少切片就会有多少个Map任务</strong></p><blockquote><p>一般切片大小为Block的整数倍（2    1&#x2F;2）</p><p>防止多余创建和很多的数据连接</p><p>如果Split大小 &gt; Block大小，计算节点少了  </p><p>如果Split大小 &lt; Block大小，计算节点多了</p><p>默认情况下，Split切片的大小等于Block的大小，默认128M，如果读取到最后一个block块的时候，与前一个block块组合起来大小小于128*1.1M的话，它们会结合生成一个Split切片，生成一个map任务</p><p>一个切片对应一个MapTask</p></blockquote><h4 id="4-4-MapTask"><a href="#4-4-MapTask" class="headerlink" title="4.4    MapTask"></a>4.4    MapTask</h4><blockquote><p>map默认从所属切片读取数据，每次读取一行（默认读取器）到内存中（map中的逻辑作用在每一行上）我们可以根据自己书写的分词逻辑（空格，逗号等分隔符），计算每个单词出现的次数 （wordcount），这时会产生（map &lt;String，Integer&gt;）临时数据，存放带内存中</p></blockquote><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">the books chronicle the adventures of the adolescent wizard Harry Potter and his best friends Ron Weasley and Hermione Granger, all of whom are students at Hogwarts School of Witchcraft and Wizardry</span><br><span class="line"></span><br><span class="line">the 1</span><br><span class="line">books 1</span><br><span class="line">chronicle 1</span><br><span class="line">the 1</span><br><span class="line">adventures 1</span><br><span class="line">of 1</span><br><span class="line">...</span><br><span class="line">Wizardry 1</span><br></pre></td></tr></table></figure><blockquote><p>但是内存的大小是有限的，如果每个任务随机的去占用内存，会导致内存不可控。多个任务同时执行有可能内存溢出（OOM）</p><p>如果把数据都直接放到硬盘上，效率低</p><p>考虑到把内存和硬盘结合，可以先往内存中写入一部分数据，然后写到硬盘上</p></blockquote><h4 id="4-5-环形缓冲区（KV-Buffer）"><a href="#4-5-环形缓冲区（KV-Buffer）" class="headerlink" title="4.5    环形缓冲区（KV-Buffer）"></a>4.5    环形缓冲区（KV-Buffer）</h4><blockquote><p>可以循环利用这块内存区域，减少数据溢写时map的停止时间</p><p>每一个Map可以独享的一个内存区域</p><p>在内存中构建一个环形缓冲区（kv-Buffer），默认大小为100M</p><p>设置缓冲区的阈值为80%（设置阈值的目的是为了同时写入和写出），当缓冲区的数据达到80M开始溢写到硬盘</p><p>溢写的时候有20M的空间可以被使用并且使用效率不会被减缓，不用担心出现OOM问题</p></blockquote><h4 id="4-6-分区Partition（环形缓冲区做的）"><a href="#4-6-分区Partition（环形缓冲区做的）" class="headerlink" title="4.6    分区Partition（环形缓冲区做的）"></a>4.6    分区Partition（环形缓冲区做的）</h4><blockquote><p>根据Key直接计算出对应的Reduce</p><p>分区的数量和reduce的数量是相等的</p><p>hash(key) % partition(reduce的数量) &#x3D; num</p><p>默认分区的算法是Hash然后取余</p><p>Object的hashCode()—equals()</p><p>如果两个对象equals，那么两个对象的hashcode一定相等</p><p>如果两个对象的hashcode相等，但是对象不一定equals</p></blockquote><h4 id="4-7-排序Sort（环形缓冲区做的，快速排序，对前面分区后的编号进行排序，目的是使得相同编号在一起）"><a href="#4-7-排序Sort（环形缓冲区做的，快速排序，对前面分区后的编号进行排序，目的是使得相同编号在一起）" class="headerlink" title="4.7    排序Sort（环形缓冲区做的，快速排序，对前面分区后的编号进行排序，目的是使得相同编号在一起）"></a>4.7    排序Sort（环形缓冲区做的，快速排序，对前面分区后的编号进行排序，目的是使得相同编号在一起）</h4><blockquote><p>对要溢写的数据进行排序（QuickSort）</p><p>按照先Partition后Key的顺序排序–&gt;相同分区在一起，相同Key的在一起</p><p>将来溢写出来的小文件也是有序的</p></blockquote><h4 id="4-8-溢写Spill"><a href="#4-8-溢写Spill" class="headerlink" title="4.8    溢写Spill"></a>4.8    溢写Spill</h4><blockquote><p>将内存中的数据循环写到硬盘上，无需担心OOM问题</p><p>每次会产生一个80M的文件</p><p>如果本次Map产生的数据较多，可能会溢写多个文件</p></blockquote><h4 id="4-9-合并Merge"><a href="#4-9-合并Merge" class="headerlink" title="4.9    合并Merge"></a>4.9    合并Merge</h4><blockquote><p>因为溢写会产生很多有序（分区 key）的小文件，而且小文件的数目也不确定</p><p>后面向reduce传递数据带来很大问题</p><p>所以将小文件合并成一个大文件，将来来取得数据直接从大文件拉去即可 </p><p>合并小文件的时候同样进行排序（<strong>归并排序</strong>），最终产生一个有序的大文件</p></blockquote><h4 id="4-10-组合器Combiner"><a href="#4-10-组合器Combiner" class="headerlink" title="4.10 组合器Combiner"></a>4.10 组合器Combiner</h4><blockquote><p>a.    集群的带宽限制了mapreduce作业的数量 ，因此应该尽量避免map和reduce任务之间的数据传输，hadoop允许用户对map的输出数据进行处理，用户可自定义combiner函数（如同map函数和reduce函数一般），其逻辑一般和reduce函数一样，combiner的输入是map的输出，combiner的输出作为reduce的输入，很多情况下可以直接将reduce函数作为combiner函数来试用</p><p>（job.setCombinerClass(FlowCountReducer.class)）</p><p>b.    combiner属于优化方案，所以无法确定combiner函数会调用多少次，可以在环形缓冲区溢出文件时调用combiner函数，也可以在溢出的小文件合并成大文件时调用combiner，但是要保证不管调用多少次，combiner函数都不影响最终结果，所以不是所有处理逻辑都可以使用combiner组件，有些逻辑如果使用了combiner函数会影响最后reduce的输出结果（如求几个数的平均值，就不能先用combiner求一次各个map输出结果的平均值，再求这些平均值的平均值，那样会导致结果错误）</p><p>c.  combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量：</p><p>​        原先传给reduce的数据时a1 a1 a1 a1 a1</p><p>​        第一次combiner组合后变成a(1,1,1,1,1)</p><p>​        第二次combiner后传给reduce的数据变为a(5,5,6,7,23,…)</p></blockquote><h4 id="4-11-拉取Fetch"><a href="#4-11-拉取Fetch" class="headerlink" title="4.11    拉取Fetch"></a>4.11    拉取Fetch</h4><blockquote><p>我们需要将Map的临时结果拉取到Reduce节点</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;第一种方式：两两合并</span><br><span class="line">&gt;第二种方式：相同的进一个reduce</span><br><span class="line">&gt;第三种对第二种优化，排序</span><br><span class="line">&gt;第四种对第三种优化：如果一个reduce处理两种key，而key分布一个首一个尾，解决不连续的问题，给个编号，这个编号怎么算呢，`回到分区，排序`</span><br></pre></td></tr></table></figure><p>第一种方式：两两合并<br>第二种方式：相同的进一个reduce<br>第三种对第二种优化，排序<br>第四种对第三种优化：如果一个reduce处理两种key，而key分布一个首一个尾，解决不连续的问题，给个编号，这个编号怎么算呢，<code>回到分区，排序</code></p></blockquote><h4 id="4-12-合并Merge"><a href="#4-12-合并Merge" class="headerlink" title="4.12    合并Merge"></a>4.12    合并Merge</h4><blockquote><p>因为reduce拉取的时候，会从多个map拉取数据</p><p>那么每个map都会产生一个小文件,这些小文件（文件与文件之间无序，文件内部有序）</p><p>为了方便计算（没必要读取N个小文件）,需要合并文件</p><p>归并算法合并成2个(qishishilia)</p><p>相同的key都在一起</p></blockquote><h4 id="4-13-归并Reduce"><a href="#4-13-归并Reduce" class="headerlink" title="4.13    归并Reduce"></a>4.13    归并Reduce</h4><blockquote><p>将文件中的数据读取到内存中</p><p>一次性将相同的key全部读取到内存中</p><p>直接将相同的key得到结果-&gt;最终结果 </p></blockquote><h4 id="4-14-写出Output"><a href="#4-14-写出Output" class="headerlink" title="4.14    写出Output"></a>4.14    写出Output</h4><blockquote><p>每个reduce将自己计算的最终结果都会存放到HDFS上</p></blockquote><p><img src="https://s2.loli.net/2022/05/26/QXb2lUNZsR7PB8g.png" alt="image-20220526222212188"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop高可用集群搭建（HA）</title>
      <link href="/2022/05/25/Hadoop%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88HA%EF%BC%89/"/>
      <url>/2022/05/25/Hadoop%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88HA%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h3 id="1、zookeeper搭建"><a href="#1、zookeeper搭建" class="headerlink" title="1、zookeeper搭建"></a>1、zookeeper搭建</h3><p>1、上传安装包到master并解压<br>    tar -xvf zookeeper-3.4.6.tar.gz</p><p>2、配置环境变量<br>    vim &#x2F;etc&#x2F;profile</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export ZOOKEEPER_HOME=/usr/local/soft/zookeeper-3.4.6</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br><span class="line"></span><br><span class="line">别忘记source /etc/profile</span><br></pre></td></tr></table></figure><p>3、修改配置文件<br>    cd conf<br>    cp  zoo_sample.cfg zoo.cfg</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">修改</span><br><span class="line">dataDir=/usr/local/soft/zookeeper-3.4.6/data</span><br><span class="line"></span><br><span class="line">增加</span><br><span class="line">server.0=master:2888:3888</span><br><span class="line">server.1=node1:2888:3888</span><br><span class="line">server.2=node2:2888:3888</span><br></pre></td></tr></table></figure><p>4、同步到其它节点</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp -r zookeeper-3.4.6 node1:`pwd`</span><br><span class="line">scp -r zookeeper-3.4.6 node2:`pwd`</span><br><span class="line"></span><br><span class="line">配置node1和node2的环境变量</span><br><span class="line">scp /etc/profile node1:/etc/</span><br><span class="line">scp /etc/profile node2:/etc/</span><br><span class="line"></span><br><span class="line">在所有节点执行</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>5、创建&#x2F;usr&#x2F;local&#x2F;soft&#x2F;zookeeper-3.4.6&#x2F;data目录,所有节点都要创建</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /usr/local/soft/zookeeper-3.4.6/data</span><br><span class="line"></span><br><span class="line">在data目录下创建myid文件</span><br><span class="line">vim myid </span><br><span class="line">master,node1,node2分别加上0，1，2</span><br></pre></td></tr></table></figure><p>6、启动zk，</p><pre><code>zkServer.sh start  三台都需要执行zkServer.sh status 查看状态当有一个leader的时候启动成功</code></pre><p>连接zk</p><pre><code>zkCli.shzk  是一个目录结构 ，每个节点可以存数据，同时可以有子节点</code></pre><p>zk shell</p><pre><code>创建目录create /test testcreate /test/a 1获取数据get /test ls /testdelete 只能删除没有子节点的节点rmr /test  删除节点</code></pre><p><strong>关闭命令</strong></p><p>zkServer.sh stop</p><p><strong>拍摄快照</strong></p><p><strong>重置zk</strong><br>1、杀掉所有zk进程<br>kiil -9 pid</p><p>2、删除data目录下的version文件, 所有节点都要删除<br>rm -rf &#x2F;usr&#x2F;local&#x2F;soft&#x2F;zookeeper-3.4.6&#x2F;data&#x2F;version-2</p><p>2、启动zk<br>zkServer.sh start</p><h3 id="2、Hadoop-HA"><a href="#2、Hadoop-HA" class="headerlink" title="2、Hadoop-HA"></a>2、Hadoop-HA</h3><table><thead><tr><th></th><th>ZK</th><th>NN</th><th>DN</th><th>RN</th><th>NM</th><th>JN</th><th>ZKFC</th></tr></thead><tbody><tr><td>master</td><td>1</td><td>1</td><td></td><td>1</td><td></td><td>1</td><td>1</td></tr><tr><td>node1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td>node2</td><td>1</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td></tr></tbody></table><h4 id="防火墙、时间同步、免密配置操作不再赘述"><a href="#防火墙、时间同步、免密配置操作不再赘述" class="headerlink" title="防火墙、时间同步、免密配置操作不再赘述"></a>防火墙、时间同步、免密配置操作不再赘述</h4><p>1、修改hadoop配置文件</p><p><strong>core-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://cluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/soft/hadoop-2.7.6/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1440<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:2181,node1:2181,node2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>hdfs-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定hdfs元数据存储的路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/soft/hadoop-2.7.6/data/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定hdfs数据存储的路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/soft/hadoop-2.7.6/data/datanode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 数据备份的个数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 关闭权限验证 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启WebHDFS功能（基于REST的接口服务） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- //////////////以下为HDFS HA的配置////////////// --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定hdfs的nameservices名称为mycluster --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定cluster的两个namenode的名称分别为nn1,nn2 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.cluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置nn1,nn2的rpc通信端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.cluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.cluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置nn1,nn2的http通信端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.cluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.cluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定namenode元数据存储在journalnode中的路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://master:8485;node1:8485;node2:8485/cluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定journalnode日志文件存储的路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/soft/hadoop-2.7.6/data/journal<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定HDFS客户端连接active namenode的java类 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.cluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置隔离机制为ssh --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定秘钥的位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启自动故障转移 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>yarn-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- NodeManager上运行的附属服务，需配置成mapreduce_shuffle才可运行MapReduce程序 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置Web Application Proxy安全代理（防止yarn被攻击） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启日志 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置日志删除时间为7天，-1为禁用，单位为秒 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 修改日志目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.remote-app-log-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置nodemanager可用的资源内存 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置nodemanager可用的资源CPU --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- //////////////以下为YARN HA的配置////////////// --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启YARN HA --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 启用自动故障转移 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN HA的名称 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarncluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定两个resourcemanager的名称 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置rm1，rm2的主机 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>node1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置YARN的http端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置zookeeper的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:2181,node1:2181,node2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置zookeeper的存储位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-state-store.parent-path<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/rmstore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启yarn resourcemanager restart --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置resourcemanager的状态存储到zookeeper中 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启yarn nodemanager restart --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置nodemanager IPC的通信端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0:45454<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>mapred-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定MapReduce计算框架使用YARN --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定jobhistory server的rpc地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定jobhistory server的http地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启uber模式（针对小作业的优化） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置启动uber模式的最大map数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.maxmaps<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>9<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置启动uber模式的最大reduce数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.maxreduces<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>2、删除hadoop数据存储目录下的文件  每个节点都需要删除</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">rm</span> -rf /usr/local/soft/hadoop-2.7.6/tmp</span><br></pre></td></tr></table></figure><p>3、启动zookeeper<strong>三台都需要启动</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zkServer.sh start启动</span><br><span class="line">zkServer.sh status查看状态</span><br></pre></td></tr></table></figure><p>4、启动 JN 存储hdfs元数据</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop-daemon.sh start journalnode每个节点都要执行</span><br></pre></td></tr></table></figure><p> 5、格式化 在一台NN上执行</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format在master上执行</span><br></pre></td></tr></table></figure><p>6、启动当前的NN</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop-daemon.sh start namenode在master上执行</span><br></pre></td></tr></table></figure><p>7、执行同步 没有格式化的NN上执行  在另外一个namenode上面执行</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs namenode -bootstrapStandby在node1上执行</span><br></pre></td></tr></table></figure><p>8、格式化ZK   在已经启动的namenode上面执行<br>    <strong>！！一定要先 把zk集群正常 启动起来！！</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs zkfc -formatZKmaster上执行</span><br></pre></td></tr></table></figure><p>9、启动hdfs集群,在启动了namenode的节点上执行</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">start-dfs.shmaster上执行</span><br></pre></td></tr></table></figure><h3 id="3、Hadoop-HA遇到的问题"><a href="#3、Hadoop-HA遇到的问题" class="headerlink" title="3、Hadoop-HA遇到的问题"></a>3、Hadoop-HA遇到的问题</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dfs.ha.fencing.methods</span><br><span class="line">表示：a list of scripts or Java classes which will be used to fence the Active NameNode during a failover</span><br><span class="line"></span><br><span class="line">而配置为shell(true)就是直接返回隔离成功，即表示没进行任何操作，为什么不会导致脑裂现象的发生，这是因为Quorun Journal方式内置了fencing功能，不需要实现单独的fencing机制（epoch number解决互斥问题）。</span><br><span class="line">而如果使用共享存储NAS+NFS那种方式的话，就需要配置具体的真正有fencing功能的，比如：sshfence，下面是sshfence的说明：</span><br><span class="line"></span><br><span class="line">sshfence - SSH to the Active NameNode and kill the process</span><br><span class="line">The sshfence option SSHes to the target node and uses fuser to kill the process listening on the service’s TCP port. In order for this fencing option to work, it must be able to SSH to the target node without providing a passphrase. Thus, one must also configure the dfs.ha.fencing.ssh.private-key-files option, which is a comma-separated list of SSH private key files. 即配置sshfence需要两个namenode之间配置无密码认证，如下:(hdfs-site.xml)</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;sshfence&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">但如果只配置sshfence，如果在机器宕机后不可达，则sshfence会返回false，即fence失败，所以得要配置成：</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;</span><br><span class="line">            sshfence</span><br><span class="line">            shell(/bin/true)</span><br><span class="line">        &lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">这样子配置，顺序执行时，如果可达就执行sshfence执行杀死namenode后返回true，不可达就直接shell(true)返回true。</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop进程相关</title>
      <link href="/2022/05/24/Hadoop%E8%BF%9B%E7%A8%8B%E7%9B%B8%E5%85%B3/"/>
      <url>/2022/05/24/Hadoop%E8%BF%9B%E7%A8%8B%E7%9B%B8%E5%85%B3/</url>
      
        <content type="html"><![CDATA[<h3 id="1、进程理解"><a href="#1、进程理解" class="headerlink" title="1、进程理解"></a>1、进程理解</h3><h4 id="HDFS相关（NN、DN、SNN）"><a href="#HDFS相关（NN、DN、SNN）" class="headerlink" title="HDFS相关（NN、DN、SNN）"></a>HDFS相关（NN、DN、SNN）</h4><h5 id="NameNode（NN）"><a href="#NameNode（NN）" class="headerlink" title="NameNode（NN）"></a>NameNode（NN）</h5><p><img src="https://s2.loli.net/2022/05/24/OAdzRlIwaLnFTB7.png" alt="image-20220524205656034"></p><blockquote><p>功能：</p><p>​    1、接收客户端的读&#x2F;写服务    因为NN知道数据文件与DN的对应（映射）关系</p><p>​    2、保存文件的时候会保存文件的元数据信息</p><p>​            a、文件的归属</p><p>​            b、文件的权限</p><p>​            c、文件的大小 、时间</p><p>​            d、Block块的信息，但是Block块的位置信息不会持久化，需要每次开启集群的时候DN向NN汇报。</p><p>​    3、收集Block块的位置信息</p><p>​        3.1    系统启动</p><p>​            a、NN关机的时候不会存储任何的Block块与DN的映射信息</p><p>​            b、DN启动的时候会自动将自己节点上存储的Block块信息汇报给NN</p><p>​            c、NN接收请求之后会重新生成映射关系</p><p>​                        File –&gt;Block</p><p>​                        Block–&gt;DN</p><p>​            d、如果数据块的副本数小于设置数，NN会将整个副本拷贝到其他节点</p><p>​        3.2    集群运行中</p><p>​            a、NN与DN保持心跳机制，三秒钟发送一次</p><p>​            b、如果客户端需要读取或者上传数据的时候，NN可以知道DN的健康情况</p><p>​            c、可以让客户端读取存活的DN节点</p><p>​            d、如果NN与DN三秒没有心跳反馈，就会认为DN出现异常（掉线），此时不会让新的数据写到这个异常的DN中，客户端访问的时候不提供异常的DN节点地址</p><p>​            e、如果超过十分钟没有心跳，那么NN会认为它宕机，会将当前DN节点存储的数据转移到其他节点</p><p>​    4、NameNode为了效率，将所有操作都在内存中进行</p><p>​        a、执行速度快</p><p>​        b、NameNode不会和磁盘进行任何的数据交换</p><p>​        但是会存在两个问题：</p><p>​        1、数据的持久化</p><p>​        2、数据保存在内存中，断电会丢失</p></blockquote><h5 id="DataNode（DN）"><a href="#DataNode（DN）" class="headerlink" title="DataNode（DN）"></a>DataNode（DN）</h5><blockquote><p>1、存放的是文件的数据信息，以及验证文件完整性的校验信息 </p><p>2、数据会存放在硬盘上</p><p>​        a、1m&#x3D;1条数据</p><p>​        b、1G&#x3D;1条数据</p><p>​        c、NN非常排斥存储小文件（能存，但是不推荐）</p><p>​            一般小文件在存储之前需要进行压缩</p><p>3、汇报</p><p>​        1、启动时</p><p>​                汇报之前会先验证Block文件是否损坏</p><p>​                向NN汇报当前DN上Block的信息</p><p>​        2、运行时</p><p>​                向NN保持心跳机制</p><p>4、当客户端读写数据的时候，首先会先去查询file与block与DN的映射，然后客户端直接与DN建立连接，然后读写数据</p></blockquote><h5 id="SecondaryNameNode（SNN）"><a href="#SecondaryNameNode（SNN）" class="headerlink" title="SecondaryNameNode（SNN）"></a>SecondaryNameNode（SNN）</h5><blockquote><h5 id="1、传统的内存持久化方案"><a href="#1、传统的内存持久化方案" class="headerlink" title="1、传统的内存持久化方案"></a>1、传统的内存持久化方案</h5><p>​    1）日志机制</p><p>​            a、做任何操作之前先记录日志</p><p>​            b、在数据改变之前先记录对应的日志，当NN停止的时候</p><p>​            c、当我下次启动的时候，只需要重新按照以前的日志”重做一遍”即可</p><p>​            <strong>缺点：</strong></p><p>​                a、log日志文件的大小不可控，随着时间的变化，集群启动的时间也会越来越长</p><p>​                b、日志中会存在大量无效日志</p><p>​            <strong>优点：</strong></p><p>​                a、不会丢失数据</p><h5 id="2）拍摄快照"><a href="#2）拍摄快照" class="headerlink" title="2）拍摄快照"></a>2）拍摄快照</h5><p>​            a、将内存中的数据写到硬盘上（序列化）</p><p>​            b、启动时还可以将硬盘上的数据写回到内存中（反序列化）</p><p>​            <strong>缺点</strong></p><p>​                a、关机时间长</p><p>​                b、如果时异常关机，数据还在内存中，没法写入到硬盘</p><p>​                c、如果写出的频率过高，导致内存使用效率低</p><p>​            <strong>优点</strong></p><p>​                启动时间较短</p><h5 id="2、SNN的解决方案"><a href="#2、SNN的解决方案" class="headerlink" title="2、SNN的解决方案"></a>2、SNN的解决方案</h5><p>​    1）解决思路</p><p>​            a、让日志大小可控（每64M）</p><p>​            b、快照需要定时保存（每隔1h）</p><p>​            c、日志+快照</p><p>​    2）解决方案 </p><p>​            a、当我们启动一个集群的时候，会产生4个文件 …&#x2F;name&#x2F;current&#x2F;</p><p><img src="https://s2.loli.net/2022/05/24/Us7rSuhcvWiNzdJ.png" alt="image-20220524222400282"></p><p>​            b、我们每次操作都会记录日志–&gt;edits-inprogress- edits_00000001，随着时间的推移，日志文件会越来越大-当达到阈值的时候（64M或3600秒），会生成新的日志文件，edits_inprogress-000000001 –&gt;edits_0000001，创建新的日志文件 edits_inprogress-0000000016。</p><p><img src="https://s2.loli.net/2022/05/24/N5eijCIE2SUzJ7v.png" alt="image-20220524222453522"></p></blockquote><h3 id="2、安全模式"><a href="#2、安全模式" class="headerlink" title="2、安全模式"></a>2、安全模式</h3><blockquote><p>安全模式是 HDFS 的一种工作状态，处于安全模式的状态下，只向客户端提供文件的只读视图，不接受对命名空间的修改；同时 NameNode 节点也不会进行数据块的复制或者删除，<br><strong>NameNode 启动时，</strong><br>        1）首先将镜像文件（ fsimage ）载入内存，并执行编辑日志（ edits ）中的各项操作。<br>        2）一旦在内存中成功建立文件系统元数据的映射，则创建一个新的 fsimage 文件和一个空的编辑日志。<br>        3）NameNode 开始监听 RPC 和 Http 请求。<br>        4）此时 NameNode 处于<strong>安全模式</strong>，只接受客户端的读请求。</p><p>​        5）处于这个状态是为了保护数据的安全所以只能被客户端访问读取数据</p></blockquote><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="section"># 对安全模式的理解</span></span><br><span class="line"><span class="section"># 1.工作流程</span></span><br><span class="line"><span class="code">a.启动 NameNode，NameNode 加载 fsimage 到内存，对内存数据执行 edits log 日 志中的事务操作。</span></span><br><span class="line"><span class="code">b.文件系统元数据内存镜像加载完毕，进行 fsimage 和 edits log 日志的合并，并创 建新的 fsimage 文件和一个空的 edits log 日志文件。</span></span><br><span class="line"><span class="code">c.NameNode 等待 DataNode 上传 block 列表信息，直到副本数满足最小副本条件。</span></span><br><span class="line"><span class="code">d.当满足了最小副本条件，再过 30 秒，NameNode 就会退出安全模式。最小副本条件指 整个文件系统中有 99.9%的 block 达到了最小副本数（默认值是 1，可设置）</span></span><br><span class="line"><span class="code"># 在 NameNode 安全模式（safemode）</span></span><br><span class="line"><span class="code">对文件系统元数据进行只读操作</span></span><br><span class="line"><span class="code">当文件的所有 block 信息具备的情况下，对文件进行只读操作</span></span><br><span class="line"><span class="code">不允许进行文件修改（写，删除或重命名文件）</span></span><br><span class="line"><span class="code"># 2.注意事项</span></span><br><span class="line"><span class="code">a.NameNode 不会持久化 block 位置信息；DataNode 保有各自存储的 block 列表信息。 正常操作时，NameNode 在内存中有一个 blocks 位置的映射信息（所有文件的所有文 件块的位置映射信息）。</span></span><br><span class="line"><span class="code">b.NameNode 在安全模式，NameNode 需要给 DataNode 时间来上传 block 列表信息到 NameNode。如果 NameNode 不等待 DataNode 上传这些信息的话，则会在 DataNode 之间进行 block 的复制，而这在大多数情况下都是非必须的（因为只需要等待 DataNode 上传就行了），还会造成资源浪费。</span></span><br><span class="line"><span class="code">c.在安全模式 NameNode 不会要求 DataNode 复制或删除 block。</span></span><br><span class="line"><span class="code">d.新格式化的 HDFS 不进入安全模式，因为 DataNode 压根就没有 block。</span></span><br><span class="line"><span class="code"># 4.命令操作</span></span><br><span class="line"><span class="code"># 通过命令查看 namenode 是否处于安全模式：</span></span><br><span class="line"><span class="code">hdfs dfsadmin -safemode get</span></span><br><span class="line"><span class="code">Safe mode is ON HDFS 的前端 webUI 页面也可以查看 NameNode 是否处于安全模式。 有时候我们希望等待安全模式退出，之后进行文件的读写操作，尤其是在脚本中，此时：</span></span><br><span class="line"><span class="code">`hdfs dfsadmin -safemode wait`</span></span><br><span class="line"><span class="code"># your read or write command goes here 管理员有权在任何时间让 namenode 进入或退出安全模式。进入安全模式：</span></span><br><span class="line"><span class="code">`hdfs dfsadmin -safemode enter`</span></span><br><span class="line"><span class="code">Safe mode is ON 这 样 做 可 以 让 namenode 一 直 处 于 安 全 模 式 ， 也 可 以 设 置 `dfs.namenode.safemode.threshold-pct` 为 1 做到这一点。 离开安全模式：</span></span><br><span class="line"><span class="code">`hdfs dfsadmin -safemode leave`</span></span><br><span class="line"><span class="code">Safe mode is OFF</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>系统中的数据块的位置并不是由 NameNode 维护的，而是以块列表的形式存储在 DataNode 中。</strong><br>[root@node01 ~]# rm -rf &#x2F;var&#x2F;yjx&#x2F;hadoop&#x2F;full&#x2F;dfs&#x2F;name&#x2F;current&#x2F;*<br>[root@node01 ~]# scp -r<br>root@node02:&#x2F;var&#x2F;yjx&#x2F;hadoop&#x2F;full&#x2F;dfs&#x2F;namesecondary&#x2F;current&#x2F;*<br>&#x2F;var&#x2F;yjx&#x2F;hadoop&#x2F;full&#x2F;dfs&#x2F;name&#x2F;current </p><p><strong>安全模式下</strong><br>        a. 安全模式下，各个 DataNode 会向 NameNode 发送自身的数据块列表<br>        b. 当 NameNode 有足够的数据块信息后，便在 30 秒后退出安全模式<br>        c. NameNode 发现数据节点过少会启动数据块复制过程<br><strong>如果 NN 收集的 Block 信息没有达到最少副本数，就会将缺失的副本 , 从有的 DN 上拷贝到其他 DN</strong><br>        a. dfs.replication.min&#x3D;2<br>        b. 但是默认最低副本数为 1<br>        c. 在拷贝的过程中系统还是处于安全模式<br><strong>安全模式相关命令</strong><br>hadoop dfsadmin -safemode leave 强制 NameNode 退出安全模式<br>hadoop dfsadmin -safemode enter 进入安全模式<br>hadoop dfsadmin -safemode get 查看安全模式状态<br>hadoop dfsadmin -safemode wait 等待一直到安全模式结束</p></blockquote><h3 id="3、HDFS的权限"><a href="#3、HDFS的权限" class="headerlink" title="3、HDFS的权限"></a>3、HDFS的权限</h3><blockquote><p>HDFS对权限的控制</p><p>​        a. 只能防止好人做错事</p><p>​        b. 不能防止坏人做坏事</p><p><strong>但是告诉你是谁，他就认为你是谁！！</strong></p></blockquote><h3 id="4、机架感知"><a href="#4、机架感知" class="headerlink" title="4、机架感知"></a>4、机架感知</h3><blockquote><p>机架感知是为了保证副本在集群中的安全性<br>我们需要将节点放在不同的DN节点上，节点也需要一定的考量<br>     可靠性，可用性，带宽消耗<br>第一个节点：<br>     集群内部（优先考虑和客户端相同的节点作为第一个节点）<br>     集群外部（选择资源丰富且不繁忙的节点作为第一个节点）<br>第二个节点：<br>     第二个节点选择与第一个节点不同机架的其他节点<br>第三个节点：<br>     与第二个相同机架相同的其他节点<br>第N个节点：<br>     与前面节点不重复的其他节点</p></blockquote><h3 id="5、HDFS的读写流程（重点）"><a href="#5、HDFS的读写流程（重点）" class="headerlink" title="5、HDFS的读写流程（重点）"></a>5、HDFS的读写流程（重点）</h3><h4 id="写数据"><a href="#写数据" class="headerlink" title="写数据"></a>写数据</h4><blockquote><p> <strong>写数据就是将客户端上的数据上传到HDFS</strong></p><h4 id="宏观过程"><a href="#宏观过程" class="headerlink" title="宏观过程"></a>宏观过程</h4><p> <img src="https://s2.loli.net/2022/05/24/wTPqf3aR9eGsKv7.png" alt="image-20220524222751307"></p><p> <strong>1.客户端向HDFS发送写数据请求</strong></p><p>   hdfs dfs -put students.txt &#x2F;shujia&#x2F;</p><p> <strong>2. Filesystem通过rpc调用namenode的put方法</strong></p><p> a. nn首先检查是否有足够的空间权限等条件创建这个文件,或者这个路径是否已经存在，权限</p><p> b. 有：NN会针对这个文件创建一个空的Entry对象,并返回成功状态给DFS        </p><p> c. 没有：直接抛出对应的异常，给予客户端错误提示信息</p><p> <strong>3.如果DFS接收到成功的状态，会创建一个FSDataOutputStream的对象给客户端使用</strong></p><p> <strong>4.客户端要向nn询问第一个Block存放的位置</strong></p><p> ​    NN通过机架感知策略 (node1 node 2 node3)</p><p> <strong>5.需要将客户端和DN节点创建连接</strong></p><pre><code>pipeline(管道)客户端 和 node1 创建连接 socketnode1 和 node2 创建连接 socketnode2 和 Node3 创建连接 socket</code></pre><p> <strong>6.客户端按照文件块切分数据，但是按照packet发送数据</strong><br>    默认一个packet大小为64K,Block128M为2048个packet</p><p> <strong>7.客户端通过pipeline管道开始使用FDSOutputStream对象将数据输出</strong></p><pre><code>    1. 客户端首先将一个 packet 发送给 node1, 同时给予 node1 一个 ack 状态    2. node1接受数据后会将数据继续传递给 node2, 同时给予 node2 一个 ack 状态    3. node2接受数据后会将数据继续传递给 node3, 同时给予 node3 一个 ack 状态    4. node3将这个 packet 接受完成后，会响应这个 ack 给 node2 为 true    5. node2会响应给 node1 , 同理 node1 响应给客户端</code></pre><p> <strong>8.客户端接收到成功的状态 , 就认为某个 packet 发送成功了，直到当前块所有的 packet 都发送完成</strong></p><p> ​    1. 如果客户端接收到最后一个 pakcet 的成功状态 , 说明当前 block 传输完成，管道就会被撤销</p><p> ​    2. 客户端会将这个消息传递给 NN ， NN 确认传输完成</p><p> ​        1. NN会将 block 的信息记录到 Entry, 客户端会继续向 NN 询问第二个块的存储位置 , 依次类推</p><p> ​                block1 (node1 node2 node3)</p><p> ​                block2 (node1 node3 node6)</p><p> ​                ….</p><p> ​                blockn(node1 node4 node6)</p><pre><code> 3. 当所有的 block 传输完成后， NN 在 Entry 中存储所有的 File 与 Block 与 DN 的映射关系关闭FsDataOutPutStream</code></pre><h4 id="微观过程（如何保证package发送的时候不出错呢？）"><a href="#微观过程（如何保证package发送的时候不出错呢？）" class="headerlink" title="微观过程（如何保证package发送的时候不出错呢？）"></a>微观过程（如何保证package发送的时候不出错呢？）</h4><p> <strong>1.客户端首先从自己的硬盘中以流的形式将自己的数据读取到缓存中</strong><br> <strong>2.然后将缓存中的数据以chunk(512B)和checksum(4B)的方式放入到packet（64k)</strong></p> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. chunk:checksum=128:1</span><br><span class="line">2. checksum:在数据处理和数据通信领域中，用于校验目的的一组数据项的和</span><br><span class="line">3. Packet中的数据分为两类，一类是实际数据包，另一类是 header 包。</span><br><span class="line">4. 一个 Packet 数据包的组成结构（分两类，一类是实际的数据包，另一类是header包。）</span><br></pre></td></tr></table></figure><p> <strong>一个数据包的组成结构：</strong></p><p> <img src="https://s2.loli.net/2022/05/24/jfzr86gaTiWdvlD.png" alt="image-20220524225301985"></p><p> <strong>参数理解：</strong></p><p> <img src="https://s2.loli.net/2022/05/24/yp9kOTUjeMYHxEZ.png" alt="image-20220524225333906"></p><p> <strong>3.（默认生成的快，发送的慢）当packet满的时候添加到dataqueue</strong><br> <strong>4.datastreamer开始从dataqueue队列上读取一个packet,通过FDSDataOPS发送到Poepleline</strong><br>     在取出的时候，也会将 packet 加入到 ackQueue, 典型的生产者消费者模式</p><p> ​    客户端发送一个 Packet 数据包以后开始接收 ack ，会有一个用来接收 ack 的 ResponseProcessor 进<br> 程，如果收到成功的 ack </p> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 如果某一个 packet 的 ack 为 true, 那么就从 ackqueue 删除掉这个 packet</span><br><span class="line">2. 如果某一个 packet 的 ack 为 false, 将 ackqueue 中所有的 packet 重新挂载到 发送队列 , 重新发送</span><br></pre></td></tr></table></figure><p> <img src="https://s2.loli.net/2022/05/24/1W63lkyhUTdDBGg.png" alt="image-20220524225407656"></p><p> <strong>最终DFS保存的数据格式：</strong></p><p> <img src="https://s2.loli.net/2022/05/24/9lJKUgNxXzPv16Q.png" alt="image-20220524225430756"></p><p> <strong>读数据</strong></p><p> <img src="https://s2.loli.net/2022/05/24/C9qYBsOL6RjZyM4.png" alt="image-20220524225514935"></p><p> <strong>1.首先客户端发送请求到 DFS ，申请读取某一个文件</strong><br> <strong>2.DFS 去 NN 查找这个文件的信息 ( 权限 , 文件是否存在 )</strong><br>     如果文件不存在，抛出指定的错误<br>     如果文件存在，返回成功状态<br> <strong>3.DFS 创建 FSDataInputStream 对象，客户端通过这个对象读取数据</strong><br> <strong>4.客户端获取文件第一个 Block 信息 , 返回 DN1 DN2 DN8</strong><br> <strong>5.客户端直接就近原则选择 DN1 对应的数据即可</strong><br> <strong>6.依次类推读取其他块的信息，直到最后一个块 , 将 Block 合并成一个文件</strong><br> <strong>7.关闭 FSDataInputStream</strong></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop-2.7.6-基础</title>
      <link href="/2022/05/23/Hadoop-2.7.6-%E5%9F%BA%E7%A1%80/"/>
      <url>/2022/05/23/Hadoop-2.7.6-%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<p>hadoop的<strong>特点</strong>：</p><p><strong>扩容能力</strong></p><p>扩容能力(Scalable)：能可靠(reliably)地存储和处理PB级别的数据。如果数据量更大，存储不下了,再增加节点就可以了。</p><p><strong>成本低</strong></p><p>成本低(Economical):可以通过普通机器组成的服务器集群来分发以及处理数据.这些服务器集群可达数千个节点。</p><p><strong>高效率</strong></p><p>高效率(Efficient):通过分发计算程序,hadoop可以在数据所在节点上(本地)并行地(parallel)处理他们,这使得处理非常的迅速</p><p><strong>可靠性</strong></p><p>可靠性(Reliable):hadoop能够自动地维护数据的多份副本,并且在任务失败后能够自动地重新部署(redeploy)计算任务</p><p>作者Doug Cutting 受Google三篇论文的启发，开发了hadoop</p><blockquote><p><strong>Google FS</strong></p><p><strong>MapReduce</strong></p><p><strong>BigTable</strong></p></blockquote><p>hadoop是一个统称，目前hadoop主要包含<strong>三大组件</strong></p><blockquote><p><strong>hdfs</strong>：是一个分布式存储框架，适合海量数据存储</p><p><strong>mapreduce</strong>：是一个分布式计算框架，适合海量数据计算</p><p><strong>yarn</strong>：是一个资源调度平台，负责给计算框架分配计算资源</p></blockquote><p>HDFS具有<strong>主从架构</strong>。HDFS集群由单个名称节点组成，主服务器管理文件系统名称空间并控制客户机对文件的访问。此外，还有许多数据节点，通常是集群中每个节点一个，它们管理连接到运行它们的节点的存储。</p><p><img src="https://s2.loli.net/2022/05/17/SODawkZY6AnX4RL.png" alt="image-20220517200532901"></p><p>hadoop的三种启动（停止）方式</p><p>第一种：全部启动集群所有进程</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">启动：sbin/start-all.sh</span><br><span class="line">停止：sbin/stop-all.sh</span><br></pre></td></tr></table></figure><p>第二种：单独启动hdfs【web端口50070】和【web端口8088】的相关进程</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">启动：sbin/start-dfs.sh sbin/start-yarn.sh</span><br><span class="line">停止：sbin/stop-dfs.sh  sbin/stop-yarn.sh</span><br><span class="line">**每次重新启动集群的时候使用**</span><br></pre></td></tr></table></figure><p>第三种：单独启动某一个进程</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">启动hdfs：sbin/hadoop-daemon.sh start (namenode | datanode)</span><br><span class="line">停止hdfs：sbin/hadoop-daemon.sh stop (namenode | datanode)</span><br><span class="line">启动yarn：sbin/hadoop-daemon.sh start (resourcemanager | nodemanager)</span><br><span class="line">停止yarn：sbin/hadoop-daemon.sh stop (resourcemanager | nodemanager)</span><br><span class="line">**用于当某个进程启动失败或者down掉的时候，重启进程**</span><br></pre></td></tr></table></figure><p><strong>hdfs shell</strong></p><p>调用文件系统(FS)Shell命令应使用 bin&#x2F;hdfs dfs -xxx 的形式。</p><p>所有的FS shell命令使用URI路径作为参数。</p><p>URI格式是scheme:&#x2F;&#x2F;authority&#x2F;path。HDFS的scheme是hdfs，对本地文件系统，scheme是file。其中scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。</p><p>例如：&#x2F;parent&#x2F;child可以表示成hdfs:&#x2F;&#x2F;namenode:namenodePort&#x2F;parent&#x2F;child，或者更简单的&#x2F;parent&#x2F;child（假设配置文件是namenode:namenodePort）</p><p>大多数FS Shell命令的行为和对应的Linux Shell命令类似。</p><p>常用操作</p><p>-ls            查看hdfs上目录，如hdfs dfs -ls &#x2F;</p><p>-put         将本地文件上传到hdfs，如hdfs dfs -put 本地文件路径 hdfs路径</p><p>-get         将hdfs文件下载到本地，如hdfs dfs -get hdfs的文件路径 本地文件路径</p><p>-mkdir    在hdfs上创建文件夹，如hdfs dfs -mkdir &#x2F;test</p><p>-cp          将hdfs文件或目录复制，如hdfs dfs -cp &#x2F;test.txt &#x2F;a&#x2F;</p><p>-cat         查看hdfs上文件内容，如hdfs dfs -cat &#x2F;test.txt</p><p><strong>运行word count实例</strong></p><p>hadoop jar  &#x2F;usr&#x2F;local&#x2F;soft&#x2F;hadoop-2.7.6&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.7.6.jar wordcount  inputpath outputpath</p><p>运行结果：</p><p><img src="https://s2.loli.net/2022/05/17/ix2tcnZwKjelkMo.png" alt="image-20220517211619031"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop集群搭建（完全分布式）</title>
      <link href="/2022/05/22/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%EF%BC%89/"/>
      <url>/2022/05/22/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h3 id="一、准备工作"><a href="#一、准备工作" class="headerlink" title="一、准备工作"></a>一、准备工作</h3><ul><li><p>三台虚拟机：master、node1、node2</p></li><li><p>时间同步</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ntpdate ntp.aliyun.com</span><br></pre></td></tr></table></figure></li><li><p>调整时区</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp  /usr/share/zoneinfo/Asia/Shanghai  /etc/localtime</span><br></pre></td></tr></table></figure></li><li><p>jdk1.8</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></li><li><p>修改主机名</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">三台分别执行 vim /etc/hostname 并将内容指定为对应的主机名</span><br></pre></td></tr></table></figure></li><li><p>关闭防火墙：systemctl stop firewalld   </p><ul><li>查看防火墙状态：systemctl status firewalld </li><li>取消防火墙自启：systemctl disable firewalld</li></ul></li><li><p>静态IP配置（两种方式）</p><ul><li><p>1、直接使用图形化界面配置</p></li><li><p>2、手动编辑配置文件进行配置</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、编辑网络配置文件</span><br><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br><span class="line"></span><br><span class="line">TYPE=Ethernet</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">HWADDR=00:0C:29:E2:B8:F2</span><br><span class="line">NAME=ens33</span><br><span class="line">DEVICE=ens33</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.168.190.100</span><br><span class="line">GATEWAY=192.168.190.2</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">DNS1=192.168.190.2</span><br><span class="line">DNS2=223.6.6.6</span><br><span class="line"></span><br><span class="line">需要修改：HWADDR（mac地址,centos7不需要手动指定mac地址）</span><br><span class="line">IPADDR（根据自己的网段，自定义IP地址）</span><br><span class="line">GATEWAY（根据自己的网段填写对应的网关地址）</span><br><span class="line"></span><br><span class="line">2、关闭NetworkManager，并取消开机自启</span><br><span class="line">systemctl stop NetworkManager</span><br><span class="line">systemctl disable NetworkManager</span><br><span class="line"></span><br><span class="line">3、重启网络服务</span><br><span class="line">systemctl restart network</span><br></pre></td></tr></table></figure></li></ul></li><li><p>免密登录</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1、生成密钥</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line"># 2、配置免密登录</span><br><span class="line">ssh-copy-id master</span><br><span class="line">ssh-copy-id node1</span><br><span class="line">ssh-copy-id node2</span><br><span class="line"># 3、测试免密登录</span><br><span class="line">ssh node1</span><br></pre></td></tr></table></figure></li><li><p>配置好映射文件：&#x2F;etc&#x2F;hosts</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">192.168.151.81 master</span><br><span class="line">192.168.151.82 node1</span><br><span class="line">192.168.151.83 node2</span><br></pre></td></tr></table></figure></li></ul><h3 id="二、搭建Hadoop集群"><a href="#二、搭建Hadoop集群" class="headerlink" title="二、搭建Hadoop集群"></a>二、搭建Hadoop集群</h3><blockquote><p>NameNode：接受客户端的读&#x2F;写服务,收集 DataNode 汇报的 Block 列表信息</p><p>DataNode：真实数据存储的地方（block）</p><p>SecondaryNameNode：做持久化的时候用到</p></blockquote><table><thead><tr><th>进程</th><th>master（主）</th><th>node1（从）</th><th>node2（从）</th></tr></thead><tbody><tr><td>NameNode</td><td>√</td><td></td><td></td></tr><tr><td>SecondaryNameNode</td><td>√</td><td></td><td></td></tr><tr><td>ResourceManager</td><td>√</td><td></td><td></td></tr><tr><td>DataNode</td><td></td><td>√</td><td>√</td></tr><tr><td>NodeManager</td><td></td><td>√</td><td>√</td></tr></tbody></table><h3 id="2-1-完全分布式搭建"><a href="#2-1-完全分布式搭建" class="headerlink" title="2.1    完全分布式搭建"></a>2.1    完全分布式搭建</h3><h4 id="1、上传安装包并解压"><a href="#1、上传安装包并解压" class="headerlink" title="1、上传安装包并解压"></a>1、上传安装包并解压</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 使用xftp上传压缩包至master的/usr/local/soft/packages/</span><br><span class="line">cd /urs/local/soft/packages/</span><br><span class="line"># 解压</span><br><span class="line">tar -zxvf hadoop-2.7.6.tar.gz -C /usr/local/soft/</span><br></pre></td></tr></table></figure><h4 id="2、配置环境变量"><a href="#2、配置环境变量" class="headerlink" title="2、配置环境变量"></a>2、配置环境变量</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">JAVA_HOME=/usr/local/soft/jdk1.8.0_171</span><br><span class="line">HADOOP_HOME=/usr/local/soft/hadoop-2.7.6</span><br><span class="line">export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</span><br><span class="line"></span><br><span class="line"># 重新加载环境变量</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h4 id="3、修改Hadoop配置文件"><a href="#3、修改Hadoop配置文件" class="headerlink" title="3、修改Hadoop配置文件"></a>3、修改Hadoop配置文件</h4><ul><li><p><code>cd /usr/local/soft/hadoop-2.7.6/etc/hadoop/</code></p></li><li><p>core-site.xml</p><blockquote><p>fs.defaultFS： 默认文件系统的名称。其方案和权限决定文件系统实现的URI。uri的方案确定命名文件系统实现类的配置属性（fs.scheme.impl）。uri的权限用于确定文件系统的主机、端口等。</p><p>hadoop.tmp.dir：是 hadoop文件系统依赖的基本配置，很多配置路径都依赖它，它的默认位置是在 &#x2F;tmp&#x2F;{$user}下面，注意这是个临时目录！！！</p><p>因此，它的持久化配置很重要的！ 如果选择默认，一旦因为断电等外在因素影响，&#x2F;tmp&#x2F;{$user}下的所有东西都会丢失。</p><p>fs.trash.interval：启用垃圾箱配置，dfs命令删除的文件不会立即从HDFS中删除。相反，HDFS将其移动到垃圾目录（每个用户在<code>/user/&lt;username&gt;/.Trash</code>下都有自己的垃圾目录）。只要文件保留在垃圾箱中，文件可以快速恢复。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://master:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/usr/local/soft/hadoop-2.7.6/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1440&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li><li><p>hadoop-env.sh</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/soft/jdk1.8.0_171</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/05/24/ABd2KfenPMkySoT.png" alt="image.png"></p></li><li><p>hdfs-site.xml</p></li><li><blockquote><p>dfs.replication：每个datanode上只能存放一个副本。我这里就2个datanode</p><p>dfs.permissions：如果为“true”，则在HDFS中启用权限检查。如果为“false”，则关闭权限检查，但所有其他行为保持不变。从一个参数值切换到另一个参数值不会更改文件或目录的模式、所有者或组。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.permissions&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li><li><p>mapred-site.xml.template</p></li><li><blockquote><p>mapreduce.framework.name：用于执行MapReduce作业的运行时框架。</p><p>mapreduce.jobhistory.address：Hadoop自带了一个历史服务器，可以通过历史服务器查看已经运行完的Mapreduce作业记录，比如用了多少个Map、用了多少个Reduce、作业提交时间、作业启动时间、作业完成时间等信息。默认情况下，Hadoop历史服务器是没有启动的，我们可以通过*mr-<strong>jobhistory-daemon.sh start historyserver</strong>命令来启动Hadoop历史服务器。我们可以通过Hadoop jar的命令来实现我们的程序jar包的运行，关于运行的日志，我们一般都需要通过启动一个服务来进行查看，就是我们的JobHistoryServer，我们可以启动一个进程，专门用于查看我们的任务提交的日志。mapreduce.jobhistory.address和mapreduce.jobhistory.webapp.address默认的值分别是0.0.0.0:10020和0.0.0.0:19888</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1、重命名文件</span><br><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line"># 2、修改</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;  </span><br><span class="line">        &lt;value&gt;master:10020&lt;/value&gt;  </span><br><span class="line">    &lt;/property&gt;  </span><br><span class="line"></span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;  </span><br><span class="line">        &lt;value&gt;master:19888&lt;/value&gt;  </span><br><span class="line">    &lt;/property&gt; </span><br></pre></td></tr></table></figure></li><li><p>slaves</p></li><li><blockquote><p>从节点的信息</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">node1</span><br><span class="line">node2</span><br></pre></td></tr></table></figure></li><li><p>yarn-site.xml</p></li><li><blockquote><p>yarn.resourcemanager.hostname：指定yarn主节点</p></blockquote></li></ul><blockquote><p>yarn.nodemanager.aux-services：NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序。默认值：“”</p><p>yarn.log-aggregation-enable：yarn日志聚合功能开关</p><p>yarn.log-aggregation.retain-seconds：日志保留时限，默认7天</p></blockquote>  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;master&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h4 id="4、分发Hadoop到node1、node2"><a href="#4、分发Hadoop到node1、node2" class="headerlink" title="4、分发Hadoop到node1、node2"></a>4、分发Hadoop到node1、node2</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /usr/local/soft/</span><br><span class="line">scp -r hadoop-2.7.6/ node1:`pwd`</span><br><span class="line">scp -r hadoop-2.7.6/ node2:`pwd`</span><br></pre></td></tr></table></figure><h4 id="5、格式化namenode（第一次启动的时候需要执行，以及每次修改核心配置文件后都需要）"><a href="#5、格式化namenode（第一次启动的时候需要执行，以及每次修改核心配置文件后都需要）" class="headerlink" title="5、格式化namenode（第一次启动的时候需要执行，以及每次修改核心配置文件后都需要）"></a>5、格式化namenode（第一次启动的时候需要执行，以及每次修改核心配置文件后都需要）</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/05/24/tQ9ZOuvozXcPIB2.png" alt="image.png"></p><h4 id="6、启动Hadoop集群"><a href="#6、启动Hadoop集群" class="headerlink" title="6、启动Hadoop集群"></a>6、启动Hadoop集群</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure><h4 id="7、检查master、node1、node2上的进程"><a href="#7、检查master、node1、node2上的进程" class="headerlink" title="7、检查master、node1、node2上的进程"></a>7、检查master、node1、node2上的进程</h4><ul><li><p>master：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master soft]# jps</span><br><span class="line">2597 NameNode</span><br><span class="line">2793 SecondaryNameNode</span><br><span class="line">2953 ResourceManager</span><br><span class="line">3215 Jps</span><br></pre></td></tr></table></figure></li><li><p>node1：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node1 jdk1.8.0_171]# jps</span><br><span class="line">11361 DataNode</span><br><span class="line">11459 NodeManager</span><br><span class="line">11559 Jps</span><br></pre></td></tr></table></figure></li><li><p>node2：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node2 ~]# jps</span><br><span class="line">11384 DataNode</span><br><span class="line">11482 NodeManager</span><br><span class="line">11582 Jps</span><br></pre></td></tr></table></figure></li></ul><h4 id="8、访问HDFS的WEB界面"><a href="#8、访问HDFS的WEB界面" class="headerlink" title="8、访问HDFS的WEB界面"></a>8、访问HDFS的WEB界面</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http://master:50070</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/05/24/7MHt8o3gIjN2rBn.png" alt="image.png"></p><h4 id="9、访问YARN的WEB界面"><a href="#9、访问YARN的WEB界面" class="headerlink" title="9、访问YARN的WEB界面"></a>9、访问YARN的WEB界面</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http://master:8088</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/05/24/6bOFBZ7GKSyxYEU.png" alt="image.png"></p><h3 id="强制格式化集群（遇到问题的简单暴力的方法）"><a href="#强制格式化集群（遇到问题的简单暴力的方法）" class="headerlink" title="强制格式化集群（遇到问题的简单暴力的方法）"></a>强制格式化集群（遇到问题的简单暴力的方法）</h3><blockquote><p>1、停止正在运行的集群</p><p>​    <strong>stop-all.sh</strong></p><p>2、删除所有节点hadoop根目录中的tmp文件夹</p><p>3、在主节点（master）中hadoop的根目录中的bin目录下，重新格式化HDFS</p><p>​    <strong>.&#x2F;hdfs namenode -format</strong></p><p>4、启动集群</p><p>​    <strong>start-all.sh</strong></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis数据库</title>
      <link href="/2022/05/22/Redis%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
      <url>/2022/05/22/Redis%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
      
        <content type="html"><![CDATA[<h3 id="1-NoSQL的引言"><a href="#1-NoSQL的引言" class="headerlink" title="1.  NoSQL的引言"></a>1.  NoSQL的引言</h3><p><strong>NoSQL</strong>(<code> Not Only SQL</code> )，意即<strong>不仅仅是SQL</strong>, 泛指非关系型的数据库。Nosql这个技术门类,早期就有人提出,发展至2009年趋势越发高涨。</p><h3 id="2-为什么是NoSQL"><a href="#2-为什么是NoSQL" class="headerlink" title="2. 为什么是NoSQL"></a>2. 为什么是NoSQL</h3><p>随着互联网网站的兴起，传统的关系数据库在应付动态网站，特别是超大规模和高并发的纯动态网站已经显得力不从心，暴露了很多难以克服的问题。如<code>商城网站中对商品数据频繁查询</code>、<code>对热搜商品的排行统计</code>、<code>订单超时问题</code>、以及微信朋友圈（音频，视频）存储等相关使用传统的关系型数据库实现就显得非常复杂，虽然能实现相应功能但是在性能上却不是那么乐观。nosql这个技术门类的出现，更好的解决了这些问题，它告诉了世界不仅仅是sql。</p><h3 id="3-NoSQL的四大分类"><a href="#3-NoSQL的四大分类" class="headerlink" title="3. NoSQL的四大分类"></a>3. NoSQL的四大分类</h3><h4 id="3-1键值-Key-Value-存储数据库"><a href="#3-1键值-Key-Value-存储数据库" class="headerlink" title="3.1键值(Key-Value)存储数据库"></a>3.1键值(Key-Value)存储数据库</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1.说明: </span><br><span class="line">- 这一类数据库主要会使用到一个哈希表，这个表中有一个特定的键和一个指针指向特定的数据。</span><br><span class="line"></span><br><span class="line"># 2.特点</span><br><span class="line">- Key/value模型对于IT系统来说的优势在于简单、易部署。  </span><br><span class="line">- 但是如果DBA只对部分值进行查询或更新的时候，Key/value就显得效率低下了。</span><br><span class="line"></span><br><span class="line"># 3.相关产品</span><br><span class="line">- Tokyo Cabinet/Tyrant,</span><br><span class="line">- Redis  基于内存的    运行软件---&gt;磁盘---&gt;内存中</span><br><span class="line">- SSDB   基于磁盘的    直接与磁盘做交互--&gt; IO</span><br><span class="line">- Voldemort </span><br><span class="line">- Oracle BDB</span><br></pre></td></tr></table></figure><h4 id="3-2列存储数据库-gt-Hbase"><a href="#3-2列存储数据库-gt-Hbase" class="headerlink" title="3.2列存储数据库-&gt;Hbase"></a>3.2列存储数据库-&gt;Hbase</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1.说明- 这部分数据库通常是用来应对分布式存储的海量数据。</span><br><span class="line">2.特点- 键仍然存在，但是它们的特点是指向了多个列。这些列是由列家族来安排的。列簇- rowkey</span><br><span class="line">3.相关产品- Cassandra、`HBase`、Riak.</span><br></pre></td></tr></table></figure><h4 id="3-3文档型数据库"><a href="#3-3文档型数据库" class="headerlink" title="3.3文档型数据库"></a>3.3文档型数据库</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1.说明</span><br><span class="line">- 文档型数据库的灵感是来自于Lotus Notes办公软件的，而且它同第一种键值存储相类似该类型的数据模型是版本化的文档，半结构化的文档以特定的格式存储，比如JSON。文档型数据库可 以看作是键值数据库的升级版，允许之间嵌套键值。而且文档型数据库比键值数据库的查询效率更高</span><br><span class="line">&#123;&#x27;id&#x27;:1001,&#x27;name&#x27;:xiaohu&#125;</span><br><span class="line">&#123;&#x27;id&#x27;:1001,&#x27;name&#x27;:&#x27;xiaohu2,&#x27;address&#x27;:&#x27;anhuihefei&#x27;,&#x27;likes&#x27;:[&#x27;play&#x27;,&#x27;eat&#x27;],&#x27;study&#x27;:&#123;&#x27;yuyan&#x27;:java,&#x27;ruanjian&#x27;:&#x27;mysql&#x27;&#125;&#125;</span><br><span class="line">文档数据库对于单条数据来说，他的事务支持并没有那么强大</span><br><span class="line">目前的mongodb5，支持了单条数据的事务，但是多条不行</span><br><span class="line"></span><br><span class="line"># 2.特点</span><br><span class="line">- 以文档形式存储</span><br><span class="line"></span><br><span class="line"># 3.相关产品</span><br><span class="line">- MongoDB、CouchDB、 MongoDb(4.x). 国内也有文档型数据库SequoiaDB，已经开源。</span><br></pre></td></tr></table></figure><h4 id="3-4图形-Graph-数据库"><a href="#3-4图形-Graph-数据库" class="headerlink" title="3.4图形(Graph)数据库"></a>3.4图形(Graph)数据库</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1.说明</span><br><span class="line">- 图形结构的数据库同其他行列以及刚性结构的SQL数据库不同，它是使用灵活的图形模型，并且能够扩展到多个服务器上。</span><br><span class="line"></span><br><span class="line"># 2.特点</span><br><span class="line">- NoSQL数据库没有标准的查询语言(SQL)，因此进行数据库查询需要制定数据模型。许多NoSQL数据库都有REST式的数据接口或者查询API。</span><br><span class="line"></span><br><span class="line"># 3.相关产品</span><br><span class="line">- Neo4J、InfoGrid、 Infinite Graph</span><br></pre></td></tr></table></figure><h3 id="4-NoSQL应用场景"><a href="#4-NoSQL应用场景" class="headerlink" title="4.NoSQL应用场景"></a>4.NoSQL应用场景</h3><p>数据模型比较简单 </p><p>需要灵活性更强的IT系统 </p><p>对数据库性能要求较高</p><p>不需要高度的数据一致性（NoSQL数据库对事物的支持不是很好）</p><h3 id="5-什么是Redis"><a href="#5-什么是Redis" class="headerlink" title="5.什么是Redis"></a>5.什么是Redis</h3><p>redis是一个内存型的数据库，开源遵循BSD基于内存数据存储被用于作为数据库缓存消息中间件 </p><h3 id="6-Redis特点"><a href="#6-Redis特点" class="headerlink" title="6.Redis特点"></a>6.Redis特点</h3><p>Redis是一个高性能Key-Value内存型数据库在redis中，所有的数据形式都是以键值对的方式来存储的</p><p>redis支持丰富的数据类型string，list，set，sorted set 其中指的是键值对中的值的类型</p><p>redis支持持久化（将数据落盘）</p><p>redis单线程，单进程    由于是单进程和单进程的，所以它是线程安全的，在java中的多线程安全在分布式中不起作用，当时只针对一个JVM有效。</p><h3 id="7-Redis数据库相关指令"><a href="#7-Redis数据库相关指令" class="headerlink" title="7.Redis数据库相关指令"></a>7.Redis数据库相关指令</h3><h4 id="7-1数据库操作指令"><a href="#7-1数据库操作指令" class="headerlink" title="7.1数据库操作指令"></a>7.1数据库操作指令</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1.Redis中库说明</span><br><span class="line">- 使用redis的默认配置器动redis服务后,默认会存在16个库,编号从0-15 配置问价中有个database相关的</span><br><span class="line">- 可以使用select 库的编号 来选择一个redis的库</span><br><span class="line"></span><br><span class="line"># 2.Redis中操作库的指令</span><br><span class="line">- 清空当前的库  FLUSHDB</span><br><span class="line">- 清空全部的库  FLUSHALL</span><br><span class="line"></span><br><span class="line"># 3.redis客户端显示中文</span><br><span class="line">-./redis-cli  -p 7000 --raw</span><br></pre></td></tr></table></figure><h4 id="7-2操作key相关指令"><a href="#7-2操作key相关指令" class="headerlink" title="7.2操作key相关指令"></a>7.2操作key相关指令</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1.DEL指令</span><br><span class="line">- 语法 :  DEL key [key ...] </span><br><span class="line">- 作用 :  删除给定的一个或多个key 。不存在的key 会被忽略。多个key之间使用空格隔开</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值： 被删除key 的数量。 </span><br><span class="line"></span><br><span class="line"># 2.EXISTS指令</span><br><span class="line">- 语法:  EXISTS key</span><br><span class="line">- 作用:  检查给定key 是否存在。多个key之间使用空格隔开，只要有一个key存在，返回值就是1</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值： 若key 存在，返回1 ，否则返回0。 </span><br><span class="line"></span><br><span class="line"># 3.EXPIRE</span><br><span class="line">- 语法:  EXPIRE key seconds</span><br><span class="line">- 作用:  为给定key 设置生存时间，当key 过期时(生存时间为0 )，它会被自动删除。</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 时间复杂度： O(1)</span><br><span class="line">- 返回值：设置成功返回1 。</span><br><span class="line"></span><br><span class="line"># 4.KEYS</span><br><span class="line">- 语法 :  KEYS pattern</span><br><span class="line">- 作用 :  查找所有符合给定模式pattern 的key 。</span><br><span class="line">- 语法:</span><br><span class="line">KEYS * 匹配数据库中所有key 。</span><br><span class="line">KEYS h?llo 匹配hello ，hallo 和hxllo 等。</span><br><span class="line">KEYS h*llo 匹配hllo 和heeeeello 等。</span><br><span class="line">KEYS h[ae]llo 匹配hello 和hallo ，但不匹配hillo 。特殊符号用 &quot;\&quot; 隔开</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值： 符合给定模式的key 列表。</span><br><span class="line"></span><br><span class="line"># 5.MOVE</span><br><span class="line">- 语法 :  MOVE key db  （move name 1----将name键移动到1号库）</span><br><span class="line">- 作用 :  将当前数据库的key 移动到给定的数据库db 当中。</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值： 移动成功返回1 ，失败则返回0 。</span><br><span class="line"></span><br><span class="line"># 6.PEXPIRE</span><br><span class="line">- 语法 :  PEXPIRE key milliseconds</span><br><span class="line">- 作用 :  这个命令和EXPIRE 命令的作用类似，但是它以毫秒为单位设置key 的生存时间，而不像EXPIRE 命令那样，以秒为单位。</span><br><span class="line">- 可用版本： &gt;= 2.6.0</span><br><span class="line">- 时间复杂度： O(1)</span><br><span class="line">- 返回值：设置成功，返回1  key 不存在或设置失败，返回0</span><br><span class="line"></span><br><span class="line"># 7.PEXPIREAT</span><br><span class="line">- 语法 :  PEXPIREAT key milliseconds-timestamp</span><br><span class="line">- 作用 :  这个命令和EXPIREAT 命令类似，但它以毫秒为单位设置key 的过期unix 时间戳，而不是像EXPIREAT那样，以秒为单位。</span><br><span class="line">- 可用版本： &gt;= 2.6.0</span><br><span class="line">- 返回值：如果生存时间设置成功，返回1 。当key 不存在或没办法设置生存时间时，返回0 。(查看EXPIRE 命令获取更多信息)</span><br><span class="line"></span><br><span class="line"># 8.TTL</span><br><span class="line">- 语法 :   TTL key</span><br><span class="line">- 作用 :   以秒为单位，返回给定key 的剩余生存时间(TTL, time to live)。</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值：</span><br><span class="line">当key 不存在时，返回-2 。</span><br><span class="line">当key 存在但没有设置剩余生存时间时，返回-1 。</span><br><span class="line">否则，以秒为单位，返回key 的剩余生存时间。</span><br><span class="line">- Note : 在Redis 2.8 以前，当key 不存在，或者key 没有设置剩余生存时间时，命令都返回-1 。</span><br><span class="line"></span><br><span class="line"># 9.PTTL</span><br><span class="line">- 语法 :  PTTL key</span><br><span class="line">- 作用 :  这个命令类似于TTL 命令，但它以毫秒为单位返回key 的剩余生存时间，而不是像TTL 命令那样，以秒为单位。</span><br><span class="line">- 可用版本： &gt;= 2.6.0</span><br><span class="line">- 返回值： 当key 不存在时，返回-2 。当key 存在但没有设置剩余生存时间时，返回-1 。</span><br><span class="line">- 否则，以毫秒为单位，返回key 的剩余生存时间。</span><br><span class="line">- 注意 : 在Redis 2.8 以前，当key 不存在，或者key 没有设置剩余生存时间时，命令都返回-1 。</span><br><span class="line"></span><br><span class="line"># 10.RANDOMKEY</span><br><span class="line">- 语法 :  RANDOMKEY</span><br><span class="line">- 作用 :  从当前数据库中随机返回(不删除) 一个key 。</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值：当数据库不为空时，返回一个key 。当数据库为空时，返回nil 。</span><br><span class="line"></span><br><span class="line"># 11.RENAME</span><br><span class="line">- 语法 :  RENAME key newkey</span><br><span class="line">- 作用 :  将key 改名为newkey 。当key 和newkey 相同，或者key 不存在时，返回一个错误。当newkey 已经存在时，RENAME 命令将覆盖旧值。</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值： 改名成功时提示OK ，失败时候返回一个错误。</span><br><span class="line"></span><br><span class="line"># 12.TYPE</span><br><span class="line">- 语法 :  TYPE key</span><br><span class="line">- 作用 :  返回key 所储存的值的类型。</span><br><span class="line">- 可用版本： &gt;= 1.0.0</span><br><span class="line">- 返回值：</span><br><span class="line">none (key 不存在)</span><br><span class="line">string (字符串)</span><br><span class="line">list (列表)</span><br><span class="line">set (集合)</span><br><span class="line">zset (有序集)</span><br><span class="line">hash (哈希表)</span><br></pre></td></tr></table></figure><h4 id="7-3-String类型"><a href="#7-3-String类型" class="headerlink" title="7.3 String类型"></a>7.3 String类型</h4><p><img src="https://s2.loli.net/2022/05/18/q9nKXHYv7iJgRDT.png"></p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>set</td><td>设置一个key&#x2F;value</td></tr><tr><td>get</td><td>根据key获得对应的value</td></tr><tr><td>mset</td><td>一次设置多个key value</td></tr><tr><td>mget</td><td>一次获得多个key的value</td></tr><tr><td>getset</td><td>获得原始key的值，同时设置新值</td></tr><tr><td>strlen</td><td>获得对应key存储value的长度</td></tr><tr><td>append</td><td>为对应key的value追加内容</td></tr><tr><td>getrange 索引0开始</td><td>截取value的内容    到末尾-1</td></tr><tr><td>setex</td><td>设置一个key存活的有效期（秒）</td></tr><tr><td>psetex</td><td>设置一个key存活的有效期（毫秒）</td></tr><tr><td>setnx</td><td>存在不做任何操作,不存在添加</td></tr><tr><td>msetnx原子操作(只要有一个存在不做任何操作)</td><td>可以同时设置多个key,只有有一个存在都不保存</td></tr><tr><td>decr</td><td>进行数值类型的-1操作</td></tr><tr><td>decrby</td><td>根据提供的数据进行减法操作</td></tr><tr><td>Incr</td><td>进行数值类型的+1操作</td></tr><tr><td>incrby</td><td>根据提供的数据进行加法操作</td></tr><tr><td>Incrbyfloat</td><td>根据提供的数据加入浮点数（不是四舍五入）</td></tr></tbody></table><h4 id="7-4-List类型"><a href="#7-4-List类型" class="headerlink" title="7.4 List类型"></a>7.4 List类型</h4><blockquote><p>list 列表 相当于java中list 集合  特点  元素有序  且 可以重复，key还是一个字符串，值是一个list</p></blockquote><p><img src="https://s2.loli.net/2022/05/18/g3awqjvZztbsJpr.png" alt="image-20200623161114380"></p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>lpush</td><td>将某个值加入到一个key列表头部  lpush list xiaohu xiaohei xiaoming    当列表不存在的时候会进行创建</td></tr><tr><td>lpushx</td><td>同lpush,但是必须要保证这个key存在  必须在列表进行存在的情况下从左插入</td></tr><tr><td>rpush</td><td>将某个值加入到一个key列表末尾</td></tr><tr><td>rpushx</td><td>同rpush,但是必须要保证这个key存在</td></tr><tr><td>lpop</td><td>返回和移除列表左边的第一个元素</td></tr><tr><td>rpop</td><td>返回和移除列表右边的第一个元素</td></tr><tr><td>lrange</td><td>获取某一个下标区间内的元素   lrange list 0 -1</td></tr><tr><td>llen</td><td>获取列表元素个数</td></tr><tr><td>lset</td><td>设置某一个指定索引的值(索引必须存在)</td></tr><tr><td>lindex</td><td>获取某一个指定索引位置的元素</td></tr><tr><td>lrem</td><td>删除重复元素</td></tr><tr><td>ltrim</td><td>保留列表中特定区间内的元素</td></tr><tr><td>linsert</td><td>在某一个元素之前，之后插入新元素</td></tr></tbody></table><h4 id="7-5-Set类型"><a href="#7-5-Set类型" class="headerlink" title="7.5 Set类型"></a>7.5 Set类型</h4><blockquote><p>特点: Set类型 Set集合 元素无序  不可以重复</p></blockquote><p><img src="https://s2.loli.net/2022/05/18/bIPlvO4NeKDwndE.png" alt="image-20200623193634316"></p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>sadd</td><td>为集合添加元素</td></tr><tr><td>smembers</td><td>显示集合中所有元素 无序</td></tr><tr><td>scard</td><td>返回集合中元素的个数</td></tr><tr><td>spop</td><td>随机返回一个元素 并将元素在集合中删除</td></tr><tr><td>smove</td><td>从一个集合中向另一个集合移动元素  必须是同一种类型</td></tr><tr><td>srem</td><td>从集合中删除一个元素</td></tr><tr><td>sismember</td><td>判断一个集合中是否含有这个元素</td></tr><tr><td>srandmember</td><td>随机返回元素   后面可以加数字 表示每次返回的个数</td></tr><tr><td>sdiff</td><td>去掉第一个集合中其它集合含有的相同元素</td></tr><tr><td>sinter</td><td>求交集</td></tr><tr><td>sunion</td><td>求和集</td></tr></tbody></table><h4 id="7-6-ZSet类型"><a href="#7-6-ZSet类型" class="headerlink" title="7.6 ZSet类型"></a>7.6 ZSet类型</h4><p><img src="https://s2.loli.net/2022/05/18/MyIESsQJzqaoh2K.png" alt="image-20200623194903967"></p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>zadd</td><td>添加一个有序集合元素     zadd zset 2 xiaohu 3 xiaohu2</td></tr><tr><td>zcard</td><td>返回集合的元素个数</td></tr><tr><td>zrange 升序 zrevrange 降序</td><td>返回一个范围内的元素       如果想看看分数 withscores</td></tr><tr><td>zrangebyscore</td><td>按照分数查找一个范围内的元素  zrangebyscore zset 0 20 withscores limit 0 2</td></tr><tr><td>zrank</td><td>返回排名</td></tr><tr><td>zrevrank</td><td>倒序排名</td></tr><tr><td>zscore</td><td>显示某一个元素的分数</td></tr><tr><td>zrem</td><td>移除某一个元素</td></tr><tr><td>zincrby</td><td>给某个特定元素加分</td></tr></tbody></table><h4 id="7-7-hash类型"><a href="#7-7-hash类型" class="headerlink" title="7.7 hash类型"></a>7.7 hash类型</h4><p><img src="https://s2.loli.net/2022/05/18/7Ga5zsBAPJCfSqh.png" alt="image-20220511234124908"></p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>hset</td><td>设置一个key&#x2F;value对</td></tr><tr><td>hget</td><td>获得一个key对应的value</td></tr><tr><td>hgetall</td><td>获得所有的key&#x2F;value对</td></tr><tr><td>hdel</td><td>删除某一个key&#x2F;value对</td></tr><tr><td>hexists</td><td>判断一个key是否存在</td></tr><tr><td>hkeys</td><td>获得所有的key</td></tr><tr><td>hvals</td><td>获得所有的value</td></tr><tr><td>hmset</td><td>设置多个key&#x2F;value</td></tr><tr><td>hmget</td><td>获得多个key的value</td></tr><tr><td>hsetnx</td><td>设置一个不存在的key的值</td></tr><tr><td>hincrby</td><td>为value进行加法运算</td></tr><tr><td>hincrbyfloat</td><td>为value加入浮点值</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell编程</title>
      <link href="/2022/05/21/Shell%E7%BC%96%E7%A8%8B/"/>
      <url>/2022/05/21/Shell%E7%BC%96%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="Shell编程"><a href="#Shell编程" class="headerlink" title="Shell编程"></a>Shell编程</h1><h3 id="1-1-Shell名词解释"><a href="#1-1-Shell名词解释" class="headerlink" title="1.1 Shell名词解释"></a>1.1 Shell名词解释</h3><p>• Kernel</p><p>​        Linux内核主要是为了和硬件打交道</p><p>• Shell</p><p>​        命令器(command interpreter)</p><p>​        Shell是一个用C语言编写的程序，它是用户使用Linux的桥梁。Shell既是一种命令语言， 又是一种程序设计语言.</p><p>​        Shell是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问操作集作系统内核的服务。</p><p>• shell两大主流：</p><p>​        sh:</p><p>​            ■ Bourne shell（sh） ,Solaris,hpux默认shell</p><p>​            ■ Bourne again shell（bash） ,Linux系统默认shell</p><p>​        bash:</p><p>​            ■ C shell(csh)</p><p>​            ■ tc shell(tcsh)</p><p>• #!声明</p><p>告诉系统其后路径所指定的程序即是解释此脚本文件的Shell程序</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello world!&quot;</span></span><br></pre></td></tr></table></figure><h3 id="1-2-Shell本的执行"><a href="#1-2-Shell本的执行" class="headerlink" title="1.2 Shell本的执行"></a>1.2 Shell本的执行</h3><p>• 输入脚本的绝对路径或相对路径</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/root/helloworld.sh</span><br><span class="line"></span><br><span class="line">./helloworld.sh</span><br><span class="line"></span><br><span class="line">注意：执行的必须是一个可执行文件</span><br></pre></td></tr></table></figure><p>• bash或sh +脚本</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sh helloworld.sh</span><br><span class="line"></span><br><span class="line">注意：当脚本没有X权限时，root和文件所有者通过该方式可以正常执行</span><br></pre></td></tr></table></figure><p>•在脚本的路径前再加”或source</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source helloworld.sh</span><br></pre></td></tr></table></figure><p>查看当前正在执行的进程：ps -ef</p><p>•区别</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">第一种和第二种会新开一个bash,不同bash中的变量无法共享。</span><br><span class="line"></span><br><span class="line">第三种是在同一个shell里面执行的</span><br></pre></td></tr></table></figure><p>•export :可以将当前进程的变量传递给子进程去使用</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">将来配置profile的时候所有的变量前必须加export</span><br></pre></td></tr></table></figure><h1 id="2-Shell基础入门"><a href="#2-Shell基础入门" class="headerlink" title="2. Shell基础入门"></a>2. Shell基础入门</h1><h3 id="2-1-shell变量"><a href="#2-1-shell变量" class="headerlink" title="2.1. shell变量"></a>2.1. shell变量</h3><p>定义变量时，变量名不加美元符号</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。</span><br><span class="line"></span><br><span class="line">​中间不能有空格，可以使用下划线（_）。</span><br><span class="line"></span><br><span class="line">​不能使用标点符号。</span><br><span class="line"></span><br><span class="line">​不能使用bash里的关键字（可用help命令查看保留关键字）</span><br></pre></td></tr></table></figure><p>变量的类型</p><p>​    局部变量</p><p>​        局部变量在脚本或命令中定义，仅在当前shell实例中有效，其他shell启动的程序不能访问局部变量。</p><p>​    环境变量</p><p>​        所有的程序，包括shell启动的程序，都能访问环境变量，有些程序需要环境变量来保证其正常运行。</p><p>​    Shell变量</p><p>​        shell变量是由shell程序设置的特殊变量。shell变量中有一部分是环境变量，有一部分是局部变量</p><p>(时间同步 ntpdate cn.ntp.org.cn)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#变量的声明</span></span><br><span class="line">name=<span class="string">&quot;zhangsan&quot;</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> `<span class="built_in">ls</span> /etc` </span><br><span class="line">或</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> $(<span class="built_in">ls</span> /etc)</span><br><span class="line"></span><br><span class="line"><span class="comment">#变量的调用 (推荐不省略大括号)</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$name</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;name&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> skill <span class="keyword">in</span> Ada Coffe Action Java; <span class="keyword">do</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;I am good at <span class="variable">$&#123;skill&#125;</span>Script&quot;</span> </span><br><span class="line"><span class="keyword">done</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># x /bin/sh: NAME: This variable is read only. </span></span><br><span class="line">url=<span class="string">&quot;https://www.google.com&quot;</span> </span><br><span class="line"><span class="built_in">readonly</span> url </span><br><span class="line">url=<span class="string">&quot;https://www.runoob.com&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除变量 </span></span><br><span class="line"><span class="built_in">unset</span> name</span><br></pre></td></tr></table></figure><h3 id="2-2-Shell的字符串"><a href="#2-2-Shell的字符串" class="headerlink" title="2.2. Shell的字符串"></a>2.2. Shell的字符串</h3><p>字符串是shell编程中最常用最有用的数据类型，字符串可以用单引号，也可以用双引号，也可以不用引号。</p><p>单引号</p><p>​    单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的；</p><p>​    单引号字串中不能出现单独一个的单引号，但可成对出现，作为字符串拼接使用。</p><p>双引号</p><p>​    双引号里可以有变量</p><p>​    双引号里可以出现转义字符</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 声明字符串 </span></span><br><span class="line">str1=<span class="string">&quot;hello world 1&quot;</span> </span><br><span class="line">str2=<span class="string">&#x27;hello world 2&#x27;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 字符串拼接--双引号 </span></span><br><span class="line">name=<span class="string">&#x27;sunwukong&#x27;</span> </span><br><span class="line">name1=<span class="string">&quot;hello, &quot;</span><span class="variable">$name</span><span class="string">&quot; !&quot;</span> </span><br><span class="line">name2=<span class="string">&quot;hello, <span class="variable">$&#123;name&#125;</span> !&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 字符串拼接--单引号 </span></span><br><span class="line">passwd=<span class="string">&#x27;123456&#x27;</span> </span><br><span class="line">passwd1=<span class="string">&#x27;hello, &#x27;</span><span class="variable">$passwd</span><span class="string">&#x27; !&#x27;</span></span><br><span class="line">passwd2=<span class="string">&#x27;hello, $&#123;passwd&#125; !&#x27;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$passwd2</span> <span class="comment"># hello, $&#123;passwd&#125; ! </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 字符串的长度 </span></span><br><span class="line">email=<span class="string">&quot;123456@qq.com&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;#email&#125;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;email:1:4&#125;</span></span><br></pre></td></tr></table></figure><h3 id="2-3-Shell数组（尾对象）伪数组"><a href="#2-3-Shell数组（尾对象）伪数组" class="headerlink" title="2.3  Shell数组（尾对象）伪数组"></a>2.3  Shell数组（尾对象）伪数组</h3><p>bash支持一维数组（不支持多维数组），并且没有限定数组的大小。</p><p>数组元素的下标由 0 开始编号。获取数组中的元素要利用下标，下标可以是整数或算术表达式，其值应大于或等于 0。 </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义数组 括号来表示数组，数组元素用&quot;空格&quot;符号分割开 </span></span><br><span class="line">数组名=(值1 值2 ... 值n) </span><br><span class="line">favs=(<span class="string">&quot;足球&quot;</span> <span class="string">&quot;蓝球&quot;</span> <span class="string">&quot;乒乓球&quot;</span> <span class="string">&quot;保龄球&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数组 $&#123;数组名[下标]&#125; </span></span><br><span class="line">fav=<span class="variable">$&#123;favs[1]&#125;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 @ 符号可以获取数组中的所有元素 </span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;favs[@]&#125;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数组的长度 </span></span><br><span class="line">length1=<span class="variable">$&#123;#favs[@]&#125;</span> </span><br><span class="line">length2=<span class="variable">$&#123;#favs[*]&#125;</span></span><br></pre></td></tr></table></figure><h3 id="2-4-Shell的注释"><a href="#2-4-Shell的注释" class="headerlink" title="2.4  Shell的注释"></a>2.4  Shell的注释</h3><p>以 <strong>#</strong> 开头的行就是注释，会被解释器忽略。</p><p>通过每一行加一个 <strong>#</strong> 号设置多行注释</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#-------------------------------------------- </span></span><br><span class="line"><span class="comment"># 这是一个注释 </span></span><br><span class="line"><span class="comment"># author： </span></span><br><span class="line"><span class="comment"># site： </span></span><br><span class="line"><span class="comment">#-------------------------------------------- </span></span><br><span class="line"><span class="comment">##### 服务器配置-start #####</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">##### 服务器配置-end ##### </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 特殊的多行注释 </span></span><br><span class="line"><span class="comment"># end of file</span></span><br><span class="line">:&lt;&lt;<span class="string">EOF  </span></span><br><span class="line"><span class="string">注释内容... </span></span><br><span class="line"><span class="string">注释内容... </span></span><br><span class="line"><span class="string">注释内容... </span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">:&lt;&lt;! </span><br><span class="line">注释内容... </span><br><span class="line">注释内容... </span><br><span class="line">注释内容... </span><br><span class="line">!</span><br></pre></td></tr></table></figure><h3 id="2-5-Shell参数传递"><a href="#2-5-Shell参数传递" class="headerlink" title="2.5  Shell参数传递"></a>2.5  Shell参数传递</h3><p>执行 Shell 脚本时，向脚本传递参数，脚本内获取参数的格式为：**$n<strong>。</strong>n** 代表一个数字</p><table><thead><tr><th><strong>参数处理</strong></th><th><strong>参数说明</strong></th></tr></thead><tbody><tr><td>$#</td><td>传递到脚本的参数个数</td></tr><tr><td>$*</td><td>以一个单字符串显示所有向脚本传递的参数。</td></tr><tr><td>$$</td><td>脚本运行的当前进程ID号</td></tr><tr><td>$!</td><td>后台运行的最后一个进程的ID号</td></tr><tr><td>$?</td><td>显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误。</td></tr><tr><td>$0</td><td>执行的文件名</td></tr></tbody></table> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Shell 传递参数实例！&quot;</span>; </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;执行的文件名：<span class="variable">$0</span>&quot;</span>; </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第一个参数为：<span class="variable">$1</span>&quot;</span>; </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第二个参数为：<span class="variable">$2</span>&quot;</span>; </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第三个参数为：<span class="variable">$3</span>&quot;</span>; </span><br><span class="line"></span><br><span class="line"><span class="comment"># ./hello.sh 11 22 33 44</span></span><br></pre></td></tr></table></figure><h1 id="3-Shell高级进阶"><a href="#3-Shell高级进阶" class="headerlink" title="3  Shell高级进阶"></a>3  Shell高级进阶</h1><h3 id="3-1-Shell运算符"><a href="#3-1-Shell运算符" class="headerlink" title="3.1 Shell运算符"></a>3.1 Shell运算符</h3><p>运算符的分类</p><p>​    算数运算符</p><table><thead><tr><th>运算符</th><th>说明</th><th>举例</th></tr></thead><tbody><tr><td>+</td><td>加法</td><td>‘expr $a + $b’ 为 30。</td></tr><tr><td>-</td><td>减法</td><td>‘expr $a-$b’结果为-10。</td></tr><tr><td>*</td><td>乘法</td><td>‘expr $a * $b’ 结果为 200。</td></tr><tr><td>&#x2F;</td><td>除法</td><td>‘expr$b&#x2F;$a’结果为2。</td></tr><tr><td>%</td><td>取余</td><td>‘expr $b % $a’ 结果为0。</td></tr><tr><td>&#x3D;</td><td>赋值</td><td>a&#x3D;$b将把变量b的值赋给a</td></tr><tr><td>&#x3D;&#x3D;</td><td>相等，用于比较两个数字，相同返回true</td><td>[$a &#x3D;&#x3D; $b]返回false。</td></tr><tr><td>!&#x3D;</td><td>不相等,用于比较两个数字，不相同返回true</td><td>[$a !&#x3D; $b]返回true。</td></tr></tbody></table><p>​    关系运算符</p><table><thead><tr><th>运算符</th><th>说明</th><th>举例</th></tr></thead><tbody><tr><td>-eq</td><td>检测两个数是否相等，相等返回true</td><td>[$a -eq $b ]返回 false。</td></tr><tr><td>-ne</td><td>检测两个数是否不相等，不相等返回true</td><td>[$a -ne $b ]返回 true。</td></tr><tr><td>-gt</td><td>检测左边的数是否大于右边的，如果是，返回true</td><td>[$a -gt $b ]返回 false.</td></tr><tr><td>-lt</td><td>检测左边的数是否小于右边的，如果是，返回true</td><td>[$a -It $b ]返回 true。</td></tr><tr><td>-ge</td><td>检测左边的数是否大于等于右边的，如果是，返回true</td><td>[$a -ge $b ]返回 false。</td></tr><tr><td>-le</td><td>检测左边的数是否小于等于右边的，如果是，返回true</td><td>[$a -le $b ]返回 true.</td></tr></tbody></table><p>​    布尔运算符</p><table><thead><tr><th>运算符</th><th>说明</th><th>举例</th></tr></thead><tbody><tr><td>！</td><td>非运算，表达式为true则返回false,否则退回true。</td><td>[! false ]返回 true。</td></tr><tr><td>-o</td><td>或运算，有一个表达式为true则返回true。</td><td>[$a -It 20 -o $b -gt100 ]返回 true。</td></tr><tr><td>-a</td><td>与运算，两个表达式都为true才返回true.</td><td>[$a -It 20 -a $b -gt100 J 返回 false。</td></tr></tbody></table><p>​    字符串运算符</p><table><thead><tr><th>运算符</th><th>说明</th><th>举例</th></tr></thead><tbody><tr><td>&amp;&amp;</td><td>逻辑的AND</td><td>[[$a -It 100 &amp;&amp; $b-gt 100 ]]返回 false</td></tr><tr><td>||</td><td>逻辑的OR</td><td>[[$a -It 100 || $b -gt 100 ]]返回 true</td></tr></tbody></table><p>​    文件测试运算符</p><table><thead><tr><th>运算符</th><th>说明</th><th>举例</th></tr></thead><tbody><tr><td>&#x3D;</td><td>检测两个字符串是否相等，相等返回true。</td><td>[$a &#x3D; $b ]返回 false。</td></tr><tr><td>!&#x3D;</td><td>检测两个字符串是否相等，不相等返回true。</td><td>[$a !&#x3D; $b ]返回 true。</td></tr><tr><td>-z</td><td>检测字符串长度是否为0,为0返回true。</td><td>[-z $a ]返回 false。</td></tr><tr><td>-n</td><td>检测字符串长度是否不为不为0返回true。</td><td>[n “$a”]返回 true.</td></tr><tr><td>$</td><td>检测字符串是否为空，不为空返回trueo</td><td>[$a]返回 true.</td></tr></tbody></table><h4 id="3-1-1-算数运算符"><a href="#3-1-1-算数运算符" class="headerlink" title="3.1.1 算数运算符"></a>3.1.1 算数运算符</h4><p> expr 是一款表达式计算工具，使用它能完成表达式的求值操作。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line">a=10 </span><br><span class="line">b=20 </span><br><span class="line"></span><br><span class="line">val=`<span class="built_in">expr</span> <span class="variable">$a</span> + <span class="variable">$b</span>` </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a + b : <span class="variable">$val</span>&quot;</span> </span><br><span class="line"></span><br><span class="line">val=`<span class="built_in">expr</span> <span class="variable">$a</span> - <span class="variable">$b</span>` </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a - b : <span class="variable">$val</span>&quot;</span> </span><br><span class="line"></span><br><span class="line">val=`<span class="built_in">expr</span> <span class="variable">$a</span> \* <span class="variable">$b</span>` </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a * b : <span class="variable">$val</span>&quot;</span> </span><br><span class="line"></span><br><span class="line">val=`<span class="built_in">expr</span> <span class="variable">$b</span> / <span class="variable">$a</span>` </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;b / a : <span class="variable">$val</span>&quot;</span> </span><br><span class="line"></span><br><span class="line">val=`<span class="built_in">expr</span> <span class="variable">$b</span> % <span class="variable">$a</span>` </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;b % a : <span class="variable">$val</span>&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> == <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a 等于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> != <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a 不等于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h4 id="3-1-2-关系运算符"><a href="#3-1-2-关系运算符" class="headerlink" title="3.1.2 关系运算符"></a>3.1.2 关系运算符</h4><p>关系运算符只支持数字，不支持字符串，除非字符串的值是数字。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"></span><br><span class="line">a=10 </span><br><span class="line">b=20 </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -eq <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -eq <span class="variable">$b</span> : a 等于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -eq <span class="variable">$b</span>: a 不等于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -ne <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -ne <span class="variable">$b</span>: a 不等于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -ne <span class="variable">$b</span> : a 等于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -gt <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -gt <span class="variable">$b</span>: a 大于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -gt <span class="variable">$b</span>: a 不大于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -lt <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -lt <span class="variable">$b</span>: a 小于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -lt <span class="variable">$b</span>: a 不小于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -ge <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -ge <span class="variable">$b</span>: a 大于或等于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -ge <span class="variable">$b</span>: a 小于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -le <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -le <span class="variable">$b</span>: a 小于或等于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -le <span class="variable">$b</span>: a 大于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h4 id="3-1-3布尔运算符"><a href="#3-1-3布尔运算符" class="headerlink" title="3.1.3布尔运算符"></a>3.1.3布尔运算符</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"></span><br><span class="line">a=10 </span><br><span class="line">b=20 </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> != <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> != <span class="variable">$b</span> : a 不等于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> == <span class="variable">$b</span>: a 等于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -lt 100 -a <span class="variable">$b</span> -gt 15 ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> 小于 100 且 <span class="variable">$b</span> 大于 15 : 返回 true&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> 小于 100 且 <span class="variable">$b</span> 大于 15 : 返回 false&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -lt 100 -o <span class="variable">$b</span> -gt 100 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> 小于 100 或 <span class="variable">$b</span> 大于 100 : 返回 true&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> 小于 100 或 <span class="variable">$b</span> 大于 100 : 返回 false&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -lt 5 -o <span class="variable">$b</span> -gt 100 ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> 小于 5 或 <span class="variable">$b</span> 大于 100 : 返回 true&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> 小于 5 或 <span class="variable">$b</span> 大于 100 : 返回 false&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h4 id="3-1-4-逻辑运算符"><a href="#3-1-4-逻辑运算符" class="headerlink" title="3.1.4 逻辑运算符"></a>3.1.4 逻辑运算符</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"></span><br><span class="line">a=10 </span><br><span class="line">b=20 </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$a</span> -lt 100 &amp;&amp; <span class="variable">$b</span> -gt 100 ]] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;返回 true&quot;</span> elseecho <span class="string">&quot;返回 false&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$a</span> -lt 100 || <span class="variable">$b</span> -gt 100 ]] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;返回 true&quot;</span> elseecho <span class="string">&quot;返回 false&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h4 id="3-1-5-字符串运算符"><a href="#3-1-5-字符串运算符" class="headerlink" title="3.1.5 字符串运算符"></a>3.1.5 字符串运算符</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"></span><br><span class="line">a=<span class="string">&quot;abc&quot;</span> </span><br><span class="line">b=<span class="string">&quot;efg&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> = <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> = <span class="variable">$b</span> : a 等于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> = <span class="variable">$b</span>: a 不等于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> != <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> != <span class="variable">$b</span> : a 不等于 b&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> != <span class="variable">$b</span>: a 等于 b&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="variable">$a</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;-z <span class="variable">$a</span> : 字符串长度为 0&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;-z <span class="variable">$a</span> : 字符串长度不为 0&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$a</span>&quot;</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;-n <span class="variable">$a</span> : 字符串长度不为 0&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;-n <span class="variable">$a</span> : 字符串长度为 0&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> : 字符串不为空&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> : 字符串为空&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h4 id="3-1-6-文件测试运算符"><a href="#3-1-6-文件测试运算符" class="headerlink" title="3.1.6 文件测试运算符"></a>3.1.6 文件测试运算符</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"></span><br><span class="line">file=<span class="string">&quot;/var/node/test.sh&quot;</span> </span><br><span class="line"><span class="keyword">if</span> [ -r <span class="variable">$file</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件可读&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件不可读&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -w <span class="variable">$file</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件可写&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件不可写&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -x <span class="variable">$file</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件可执行&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件不可执行&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -f <span class="variable">$file</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件为普通文件&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件为特殊文件&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -d <span class="variable">$file</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件是个目录&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件不是个目录&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -s <span class="variable">$file</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件不为空&quot;</span> </span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;文件为空&quot;</span> </span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h4 id="3-2-echo打印数据"><a href="#3-2-echo打印数据" class="headerlink" title="3.2 echo打印数据"></a>3.2 echo打印数据</h4><p> Shell的echo指令用于字符串的输出。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 显示普通字符串 </span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello World&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 显示转义字符 </span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;\&quot;Hello World\&quot;&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 显示变量 </span></span><br><span class="line">name=<span class="string">&quot;zhangsan&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$name</span> Hello World&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 显示换行 </span></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;OK! \n&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello World&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 显示不换行 </span></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;OK! \c&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello World&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 显示结果定向至文件 </span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello World&quot;</span> &gt; myfile </span><br><span class="line"><span class="comment">## &gt; 代表覆盖</span></span><br><span class="line"><span class="comment"># &gt;&gt; 追加写入</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 原样输出字符串 </span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;$name\&quot;&#x27;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 显示命令执行结果 </span></span><br><span class="line"><span class="built_in">echo</span> `<span class="built_in">date</span>`</span><br></pre></td></tr></table></figure><h3 id="3-4-Shell流程控制"><a href="#3-4-Shell流程控制" class="headerlink" title="3.4 Shell流程控制"></a>3.4 Shell流程控制</h3><h4 id="3-4-1-if"><a href="#3-4-1-if" class="headerlink" title="3.4.1  if"></a>3.4.1  if</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> conditionl</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">commandl</span><br><span class="line"><span class="keyword">elif</span> condition2</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">command2</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">commandN</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> == <span class="variable">$b</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a 等于 b&quot;</span></span><br><span class="line"><span class="keyword">elif</span> [ <span class="variable">$a</span> -gt <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a 大于 b&quot;</span></span><br><span class="line"><span class="keyword">elif</span> [ <span class="variable">$a</span> -lt <span class="variable">$b</span> ] </span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;a 小于 bn&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;没有符合的条件&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h4 id="Shell-case语句为多选择语句。"><a href="#Shell-case语句为多选择语句。" class="headerlink" title="Shell case语句为多选择语句。"></a>Shell case语句为多选择语句。</h4><h4 id="可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。"><a href="#可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。" class="headerlink" title="可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。"></a>可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> 值 <span class="keyword">in</span> </span><br><span class="line">模式1)</span><br><span class="line">command1 </span><br><span class="line">command2 </span><br><span class="line">... </span><br><span class="line">commandN ;; </span><br><span class="line"></span><br><span class="line">模式2）</span><br><span class="line">command1 </span><br><span class="line">command2 </span><br><span class="line">... </span><br><span class="line">commandN </span><br><span class="line">;; </span><br><span class="line"><span class="keyword">esac</span> </span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;输入 1 到 4 之间的数字:&#x27;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;你输入的数字为:&#x27;</span> </span><br><span class="line"><span class="built_in">read</span> num </span><br><span class="line"><span class="keyword">case</span> <span class="variable">$num</span> <span class="keyword">in</span> </span><br><span class="line">1) <span class="built_in">echo</span> <span class="string">&#x27;你选择了 1&#x27;</span> </span><br><span class="line">;;</span><br><span class="line">2) <span class="built_in">echo</span> <span class="string">&#x27;你选择了 2&#x27;</span> </span><br><span class="line">;;</span><br><span class="line">3) <span class="built_in">echo</span> <span class="string">&#x27;你选择了 3&#x27;</span> </span><br><span class="line">;;</span><br><span class="line">4) <span class="built_in">echo</span> <span class="string">&#x27;你选择了 4&#x27;</span> </span><br><span class="line">;;</span><br><span class="line">*) <span class="built_in">echo</span> <span class="string">&#x27;你没有输入 1 到 4 之间的数字&#x27;</span> </span><br><span class="line">;; </span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><h4 id="3-4-2-for"><a href="#3-4-2-for" class="headerlink" title="3.4.2 for"></a>3.4.2 for</h4><p>当变量值在列表里，for循环即执行一次所有命令，使用变量名获取列表中的当前取值。</p><p>命令可为田可有效的shell命令和语句。in列表可以包含替换、字符串和文件名。</p><p>in列表是可选的，如果不用它，for循环使用命令行的位置参数。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> item1 item2 ... itemN </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line">command1 </span><br><span class="line">command2 </span><br><span class="line">... </span><br><span class="line">commandN </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> loop <span class="keyword">in</span> 1 2 3 4 5 </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;The value is: <span class="variable">$loop</span>&quot;</span> </span><br><span class="line"><span class="keyword">done</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> str <span class="keyword">in</span> <span class="string">&#x27;This is a string&#x27;</span> <span class="string">&#x27;hello moto&#x27;</span> </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$str</span> </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h4 id="3-4-3-while循环"><a href="#3-4-3-while循环" class="headerlink" title="3.4.3 while循环"></a>3.4.3 while循环</h4><p>while循环用于不断执行一系列命令，也用于从输入文件中读取数据；命令通常为测试条件。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> condition </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line"><span class="built_in">command</span> </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Bash let 命令，它用于执行一个或多个表达式，变量计算中不需要加上 $ 来表示变量 </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#!/bin/bash </span></span><br><span class="line">int=1 </span><br><span class="line"><span class="keyword">while</span>(( <span class="variable">$int</span>&lt;=5 )) </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$int</span> </span><br><span class="line"><span class="built_in">let</span> <span class="string">&quot;int++&quot;</span> </span><br><span class="line"><span class="keyword">done</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 无限循环 </span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">true</span> </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line"><span class="built_in">command</span> </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h4 id="3-4-4-break"><a href="#3-4-4-break" class="headerlink" title="3.4.4 break"></a>3.4.4 break</h4><p>break命令允许跳出所有循环（终止执行后面的所有循环）。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"><span class="keyword">while</span> : </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line"><span class="built_in">echo</span> -n <span class="string">&quot;输入 1 到 5 之间的数字:&quot;</span> </span><br><span class="line"><span class="built_in">read</span> aNum </span><br><span class="line"><span class="keyword">case</span> <span class="variable">$aNum</span> <span class="keyword">in</span> </span><br><span class="line">1|2|3|4|5) <span class="built_in">echo</span> <span class="string">&quot;你输入的数字为 <span class="variable">$aNum</span>!&quot;</span> </span><br><span class="line">;;</span><br><span class="line">*) <span class="built_in">echo</span> <span class="string">&quot;你输入的数字不是 1 到 5 之间的! 游戏结束&quot;</span> </span><br><span class="line"><span class="built_in">break</span> </span><br><span class="line">;; </span><br><span class="line"><span class="keyword">esac</span> </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h4 id="3-4-5-continue"><a href="#3-4-5-continue" class="headerlink" title="3.4.5 continue"></a>3.4.5 continue</h4><p>continue命令不会跳出所有循环，仅仅跳出当前循环。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"><span class="keyword">while</span> : </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line"><span class="built_in">echo</span> -n <span class="string">&quot;输入 1 到 5 之间的数字: &quot;</span> </span><br><span class="line"><span class="built_in">read</span> aNum </span><br><span class="line"><span class="keyword">case</span> <span class="variable">$aNum</span> <span class="keyword">in</span> </span><br><span class="line">1|2|3|4|5) <span class="built_in">echo</span> <span class="string">&quot;你输入的数字为 <span class="variable">$aNum</span>!&quot;</span> </span><br><span class="line">;;</span><br><span class="line">*) <span class="built_in">echo</span> <span class="string">&quot;你输入的数字不是 1 到 5 之间的!&quot;</span></span><br><span class="line">        <span class="built_in">continue</span> </span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;游戏结束&quot;</span> </span><br><span class="line">        ;; </span><br><span class="line">    <span class="keyword">esac</span> </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h2 id="3-5-Shell函数"><a href="#3-5-Shell函数" class="headerlink" title="3.5 Shell函数"></a>3.5 Shell函数</h2><p>linux shell 可以用户定义函数，然后在shell脚本中可以随便调用。</p><p>可以带function fun() 定义，也可以直接fun() 定义,不带任何参数。</p><p>参数返回，可以显示加：return 返回，如果不加，将以最后一条命令运行结果，作为返回值。return后跟数值n(0-255</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 第一个函数------------------------------ </span></span><br><span class="line"><span class="function"><span class="title">demoFun</span></span>()&#123; </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;这是我的第一个 shell 函数!&quot;</span> </span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;-----函数开始执行-----&quot;</span> </span><br><span class="line">demoFun </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;-----函数执行完毕-----&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 函数返回值------------------------------ </span></span><br><span class="line"><span class="function"><span class="title">funWithReturn</span></span>()&#123; </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;这个函数会对输入的两个数字进行相加运算...&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;输入第一个数字: &quot;</span> </span><br><span class="line"><span class="built_in">read</span> aNum </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;输入第二个数字: &quot;</span> </span><br><span class="line"><span class="built_in">read</span> anotherNum  </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;两个数字分别为 <span class="variable">$aNum</span> 和 <span class="variable">$anotherNum</span> !&quot;</span> </span><br><span class="line"><span class="built_in">return</span> $((<span class="variable">$aNum</span>+<span class="variable">$anotherNum</span>)) </span><br><span class="line">&#125;</span><br><span class="line">funWithReturn </span><br><span class="line"><span class="comment"># 函数返回值在调用该函数后通过 $? 来获得。 </span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;输入的两个数字之和为 $? !&quot;</span> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 函数参数------------------------------ </span></span><br><span class="line"><span class="function"><span class="title">funWithParam</span></span>()&#123; </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第一个参数为 <span class="variable">$1</span> !&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第二个参数为 <span class="variable">$2</span> !&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第十个参数为 <span class="variable">$10</span> !&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第十个参数为 <span class="variable">$&#123;10&#125;</span> !&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;第十一个参数为 <span class="variable">$&#123;11&#125;</span> !&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;参数总数有 <span class="variable">$#</span> 个!&quot;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;作为一个字符串输出所有参数 $* !&quot;</span> </span><br><span class="line">&#125;</span><br><span class="line">funWithParam 1 2 3 4 5 6 7 8 9</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>shell中系统任务设置</title>
      <link href="/2022/05/21/shell%E4%BB%BB%E5%8A%A1%E7%9B%B8%E5%85%B3%E8%AE%BE%E7%BD%AE/"/>
      <url>/2022/05/21/shell%E4%BB%BB%E5%8A%A1%E7%9B%B8%E5%85%B3%E8%AE%BE%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="shell中系统任务设置"><a href="#shell中系统任务设置" class="headerlink" title="shell中系统任务设置"></a>shell中系统任务设置</h1><h3 id="1、系统启动流程"><a href="#1、系统启动流程" class="headerlink" title="1、系统启动流程"></a>1、系统启动流程</h3><p>启动计算机的硬件(BIOS)</p><p>​        读取时间</p><p>​        选择对应的启动模式(USB HDD EFI）</p><p>如果是Linux系统，回去找&#x2F;boot目录.引导这个系统启动</p><p>计算机系统开始启动,读取初始化配置文件</p><p>​        vim &#x2F;etc&#x2F;inittab</p><p>​        启动时控制着计算机的运行级别 runlevel</p><table><thead><tr><th>0</th><th>halt(关机)</th></tr></thead><tbody><tr><td>1</td><td>Single user mode(单用户模式)</td></tr><tr><td>2</td><td>Multiuser, without NFS(多用户模式，但是无网络状态) FS–&gt;FileSystem</td></tr><tr><td>3</td><td>Full multiuser mode(多用户完整版模式)</td></tr><tr><td>4</td><td>unused (保留模式)</td></tr><tr><td>5</td><td>X11(用户界面模式)</td></tr><tr><td>6</td><td>reboot(重启模式)</td></tr></tbody></table><p>​        id:3:initdefault: 默认runlevel为3 </p><p>​        以runlevel&#x3D;3开始启动对应的服务和组件</p><p>开始默认引导公共的组件或者服务</p><p>​        vim &#x2F;etc&#x2F;rc.d&#x2F;rc.sysinit</p><p>开始加载对应runlevel的服务</p><p>​        vi &#x2F;etc&#x2F;rc3.d&#x2F;</p><p>​            K:关机时需要关闭的服务</p><p>​            S:启动时需要开启的服务</p><p>​            数字代表了开启或者关闭的顺序</p><p>​            所有的文件都是软链接，链接的地址为 &#x2F;etc&#x2F;init.d</p><p>当启动完毕，所有的服务也被加载完成</p><h3 id="2、系统服务"><a href="#2、系统服务" class="headerlink" title="2、系统服务"></a>2、系统服务</h3><p>​    我们可以使用chkconfig命令查看当前虚拟机的服务</p><p>​    通过查看可以得知不同的级别对应到每一个服务确定本次开机自动启动</p><p>​    开机结束后，我们需要使用service（Centos6）Systemctl(Centos7)命令控制服务的开启或者关闭</p><h3 id="3、-开机自启动服务"><a href="#3、-开机自启动服务" class="headerlink" title="3、 开机自启动服务"></a>3、 开机自启动服务</h3><h5 id="rc-local"><a href="#rc-local" class="headerlink" title="rc.local"></a>rc.local</h5><p>​        首先创建脚本存放的文件夹</p><p>​                mkdir -p &#x2F;usr&#x2F;local&#x2F;scripts</p><p>​        在文件夹中创建脚本文件</p><p>​                vim hello.sh</p><p>​                给予执行权限</p><p>​        去&#x2F;etc&#x2F;rc.d&#x2F;rc.local文件中添加脚本的绝对路径</p><p>​                给予rc.local执行权限</p><p>​        创建一个文件夹</p><p>​                mkdir &#x2F;usr&#x2F;local&#x2F;soft&#x2F;ceshitest</p><p>​        重启虚拟机</p><p>​                reboot</p><h5 id="chkconfig"><a href="#chkconfig" class="headerlink" title="chkconfig"></a>chkconfig</h5><p>​        创建开机自启动脚本文件</p><p>​        vim schoolntpdate.sh</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash </span></span><br><span class="line"><span class="comment">#chkconfig: 2345 88 99 </span></span><br><span class="line"><span class="comment">#description:auto_run </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开机创建文件夹</span></span><br><span class="line"><span class="built_in">mkdir</span> /usr/local/soft/ceshitest2</span><br></pre></td></tr></table></figure><p>​        给其设置执行权限</p><p>​                chmod u+x schoolntpdate.sh</p><p>​        将脚本拷贝到 &#x2F;etc&#x2F;init.d    下</p><p>​                cp schoolntpdate.sh &#x2F;etc&#x2F;init.d&#x2F;</p><p>​        添加到服务</p><p>​                chkconfig –add &#x2F;etc&#x2F;init.d&#x2F;schoolntpdate.sh</p><p>​        重启服务器</p><p>​                reboot</p><h3 id="4、定时任务"><a href="#4、定时任务" class="headerlink" title="4、定时任务"></a>4、定时任务</h3><blockquote><p>在linux中最小时间是到分钟的</p></blockquote><p>在系统服务中心，crond负责周期任务</p><p>​        systemctl status crond.service</p><p>添加任务，编辑当前用户的任务列表</p><p>​        crontab -e</p><p>编辑任务</p><p>​        星 星 星 星 星 command</p><p>​        分 时 日 月 周 命令</p><p>​        第1列表示分钟1～59 每分钟用*或者 *&#x2F;2表示</p><p>​        第2列表示小时1～23（0表示0点）</p><p>​        第3列表示日期1～31</p><p>​        第4列表示月份1～12</p><p>​        第5列标识号星期0～6（0表示星期天）</p><p>​        第6列要运行的命令</p><p>​        *：表示任意时间都，实际上就是“每”的意思。可以代表00-23小时或者00-12每月或者00-59分</p><p>​        -：表示区间，是一个范围，00 17-19 * * * cmd，就是每天17,18,19点的整点执行命令</p><p>​        ,：是分割时段，30 3,19,21 * * * cmd，就是每天凌晨3和晚上19,21点的半点时刻执行命令</p><p>​        &#x2F;n：表示分割，可以看成除法，*&#x2F;5 * * * * cmd，每隔五分钟执行一次</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">30 21 * * * /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">上面的例子表示每晚的21:30重启apache。 </span><br><span class="line"></span><br><span class="line">45 4 1,10,22 * * /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">上面的例子表示每月1、10、22日的4 : 45重启apache。 </span><br><span class="line"></span><br><span class="line">10 1 * * 6,0 /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">上面的例子表示每周六、周日的1 : 10重启apache。 </span><br><span class="line"></span><br><span class="line">0,30 18-23 * * * /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">上面的例子表示在每天18 : 00至23 : 00之间每隔30分钟重启apache。 </span><br><span class="line"></span><br><span class="line">0 23 * * 6 /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">上面的例子表示每星期六的11 : 00 pm重启apache。 </span><br><span class="line"></span><br><span class="line">* */2 * * * /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">每两小时重启apache </span><br><span class="line"></span><br><span class="line">* 23-7/1 * * * /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">晚上11点到早上7点之间，每隔一小时重启apache </span><br><span class="line"></span><br><span class="line">0 11 4 * mon-wed /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">每月的4号与每周一到周三的11点重启apache </span><br><span class="line"></span><br><span class="line">0 4 1 jan * /usr/local/etc/rc.d/lighttpd restart </span><br><span class="line">一月一号的4点重启apache</span><br><span class="line"></span><br><span class="line">需求：每分钟要干一些事情</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--（功能描述：显示年月日时分秒） </span><br><span class="line"><span class="built_in">date</span> <span class="string">&quot;+%Y%m%d%H%M%S&quot;</span></span><br></pre></td></tr></table></figure><p>重启crontab，使配置生效</p><p>​        systemctl restart crond.service</p><p>通过crontab -l</p><p>​        查看当前的定时任务</p><p>查看任务的历史</p><p>​        vim &#x2F;var&#x2F;spool&#x2F;mail&#x2F;root</p><p>清除任务</p><p>​        crontab -r</p>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell中系统任务设置 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis持久化</title>
      <link href="/2022/05/20/Redis%E6%8C%81%E4%B9%85%E5%8C%96/"/>
      <url>/2022/05/20/Redis%E6%8C%81%E4%B9%85%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h3 id="1-持久化机制"><a href="#1-持久化机制" class="headerlink" title="1.持久化机制"></a>1.持久化机制</h3><p>Redis官方提供了两种不同的持久化方法来将内存的数据存储到硬盘里</p><p><strong>快照（Snapshot）</strong></p><p><strong>AOF（Append Only File）</strong>只追加日志文件</p><h4 id="1-1-快照（Snapshot）"><a href="#1-1-快照（Snapshot）" class="headerlink" title="1.1 快照（Snapshot）"></a>1.1 快照（Snapshot）</h4><h5 id="1-特点"><a href="#1-特点" class="headerlink" title="1.特点"></a>1.特点</h5><blockquote><p>这种方式可以将某一时刻的所有数据都写入硬盘中，这是Redis的默认开启持久化的方式，保存的文件时以**.rdb**后缀的文件，所以这种方式也成为RDB方式。</p></blockquote><blockquote><p>官方说法叫快照持久化</p></blockquote><p><img src="https://s2.loli.net/2022/05/18/F62H8pT9SQ5XAcy.png" alt="image-20220512214029142"></p><h5 id="2-快照生成方式"><a href="#2-快照生成方式" class="headerlink" title="2.快照生成方式"></a>2.快照生成方式</h5><blockquote><p>客户端方式：BGSAVE和SAVE指令</p></blockquote><blockquote><p>服务器配置自动触发</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、客户端方式-BGSAVE</span><br><span class="line">-客户端可以使用BGSAVE命令来创建一个快照，当接收到客户端的BGSAVE命令时，redis会调用fork来创建一个子进程，然后子进程负责将快照写入磁盘中，而父进程则继续处理命令请求</span><br><span class="line"></span><br><span class="line">名词解释：*fork*。当一个进程创建子进程的时候，底层的操作系统会创建该进程的一个副本，在类似于unix系统中创建子进程的操作会进行优化：在刚开始的时候，父子进程共享相同内存，知道父进程或子进程对内存进行了写之后，对被写入的内存才会结束服务。</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/05/18/QLUB12mTMPFrE45.png" alt="image-20220512214729500"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2、客户端方式-SAVE</span><br><span class="line">-客户端还可以使用SAVE命令来创建一个快照，接收到SAVE命令的redis服务器在快照创建完毕之前将不再响应任何其他的命令</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/05/18/671QPakdDzq8Hfg.png" alt="image-20220512214914633"></p><p>注意：SAVE命令并不常用，使用SAVE命令在快照创建完毕之前，redis处于阻塞状态，无法对外服务</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">3、服务器配置-满足自动触发</span><br><span class="line">-如果用户在redis.conf中设置了save配置选项，redis会在save选项条件满足之后自动触发一次BGSAVE命令，如果设置多个save配置选项，当任意一个save配置选项条件满足，redis也会触发一次BGSAVE命令</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">4、服务器接收客户端shutdown指令</span><br><span class="line">-当redis通过shutdown指令接收到关闭服务器的请求时，会执行一个save命令，阻塞所有的客户端，不再执行客户端执行发送的任何命令，并且在save命令执行完毕之后关闭服务器</span><br></pre></td></tr></table></figure><h5 id="3-配置生成快照名称和位置"><a href="#3-配置生成快照名称和位置" class="headerlink" title="3.配置生成快照名称和位置"></a>3.配置生成快照名称和位置</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、修改生成快照名称</span><br><span class="line">-dbfilename dump.rbd</span><br><span class="line">2、修改生成位置</span><br><span class="line">-dir ./</span><br><span class="line"></span><br><span class="line">经过断电操作测试后，发现这个持久化并不是太好，可能会造成数据丢失问题（刚刚做完一次快照，又来一次写数据请求断电）</span><br></pre></td></tr></table></figure><h4 id="1-2-AOF只追加日志文件"><a href="#1-2-AOF只追加日志文件" class="headerlink" title="1.2 AOF只追加日志文件"></a>1.2 AOF只追加日志文件</h4><h5 id="1-特点-1"><a href="#1-特点-1" class="headerlink" title="1.特点"></a>1.特点</h5><p>这种方式可以将所有客户端执行的<strong>写命令</strong>记录到日志文件中，AOF持久化会被执行的写命令写到AOF的文件末尾，以此来记录数据发生的变化，因此只要redis从头到尾执行一次AOF文件所包含的所有写命令，就可以恢复AOF文件的记录的数据集</p><p><img src="https://s2.loli.net/2022/05/18/yJIqWEfzsv3Kajp.png"></p><h5 id="2-开启AOF持久化"><a href="#2-开启AOF持久化" class="headerlink" title="2.开启AOF持久化"></a>2.开启AOF持久化</h5><p>在开启redis的默认配置中AOF持久化机制是没有开启的，需要在配置中开启</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、开启AOF持久化</span><br><span class="line">-修改 appendonly yes 开启持久化</span><br><span class="line">-修改 appendfilename &quot;appendonly.aof&quot; 指定生成文件名称</span><br></pre></td></tr></table></figure><h5 id="3-日志追加频率"><a href="#3-日志追加频率" class="headerlink" title="3.日志追加频率"></a>3.日志追加频率</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1.always 【谨慎使用】</span><br><span class="line">- 说明: 每个redis写命令都要同步写入硬盘,严重降低redis速度</span><br><span class="line">- 解释: 如果用户使用了always选项,那么每个redis写命令都会被写入硬盘,从而将发生系统崩溃时出现的数据丢失减到最少;遗憾的是,因为这种同步策略需要对硬盘进行大量的写入操作,所以redis处理命令的速度会受到硬盘性能的限制;</span><br><span class="line">- 注意: 转盘式硬盘在这种频率下200左右个命令/s ; 固态硬盘(SSD) 几百万个命令/s;</span><br><span class="line">- 警告: 使用SSD用户请谨慎使用always选项,这种模式不断写入少量数据的做法有可能会引发严重的`写入放大`问题,导致将固态硬盘的寿命从原来的几年降低为几个月。</span><br><span class="line"></span><br><span class="line"># 2.everysec 【推荐默认】</span><br><span class="line">- 说明: 每秒执行一次同步显式的将多个写命令同步到磁盘</span><br><span class="line">- 解释： 为了兼顾数据安全和写入性能,用户可以考虑使用everysec选项,让redis每秒一次的频率对AOF文件进行同步;redis每秒同步一次AOF文件时性能和不使用任何持久化特性时的性能相差无几,而通过每秒同步一次AOF文件,redis可以保证,即使系统崩溃,用户最多丢失一秒之内产生的数据。 </span><br><span class="line"></span><br><span class="line"># 3.no【不推荐】</span><br><span class="line">- 说明: 由操作系统决定何时同步 </span><br><span class="line">- 解释：最后使用no选项,将完全有操作系统决定什么时候同步AOF日志文件,这个选项不会对redis性能带来影响但是系统崩溃时,会丢失不定数量的数据,甚至丢失全部数据，另外如果用户硬盘处理写入操作不够快的话,当缓冲区被等待写入硬盘数据填满时,redis会处于阻塞状态,并导致redis的处理命令请求的速度变慢。</span><br></pre></td></tr></table></figure><h4 id="1-3-AOF文件的重写"><a href="#1-3-AOF文件的重写" class="headerlink" title="1.3 AOF文件的重写"></a>1.3 AOF文件的重写</h4><h5 id="1-AOF带来的问题"><a href="#1-AOF带来的问题" class="headerlink" title="1. AOF带来的问题"></a>1. AOF带来的问题</h5><p>AOF的方式也同时带来了另一个问题。持久化文件会变得越来越大。例如我们调用incr test命令100次，文件中必须保存全部的100条命令，其实有99条都是多余的。因为要恢复数据库的状态其实文件中保存一条set test 100就够了。为了压缩AOF的持久化文件，Redis提供了AOF重写（ReWrite）机制。</p><h5 id="2-AOF重写"><a href="#2-AOF重写" class="headerlink" title="2. AOF重写"></a>2. AOF重写</h5><p>用来在一定程度上减小AOF文件的体积,并且还能保证数据不丢失</p><h5 id="3-触发重写方式"><a href="#3-触发重写方式" class="headerlink" title="3. 触发重写方式"></a>3. 触发重写方式</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1.客户端方式触发重写</span><br><span class="line">-执行BGREWRIEAOF命令  不会阻塞redis的服务</span><br><span class="line">2.服务器配置方式自动触发</span><br><span class="line">-配置redis.conf中的auto-aof-rewrite-percentage选项</span><br><span class="line">-如果设置auto-aof-rewrite-percentage值为100和auto-aof-rewrite-min-size 64mb,并且启用的AOF持久化时,那么当AOF文件体积大于64MB,并且AOF文件的体积比上一次重写之后体积大了至少一倍(100%)时,会自动触发,如果重写过于频繁,用户可以考虑将auto-aof-rewrite-percentage设置为更大</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/05/18/YiLCvp4arWw6eBE.png" alt="image-20220512225431013"></p><h5 id="4-重写原理"><a href="#4-重写原理" class="headerlink" title="4. 重写原理"></a>4. 重写原理</h5><p>从Redis 7.0.0开始,Redis使用了多部分AOF机制.也就是将原来的单个AOF文件拆分为基础文件(最多一个)和增量文件(可能不止一个).</p><p>基本文件表示重写AOF时存在的数据的初始(RDB或AOF格式)快照.</p><p>增量文件包含自创建最后一个基本AOF文件以来的增量更改.所有这些文件都放在一个单独的目录中,并由清单文件跟踪</p><p>从 Redis 7.0.0 开始，在调度 AOF 重写时，Redis 父进程会打开一个新的增量 AOF 文件继续写入。子进程执行重写逻辑并生成新的基础 AOF。Redis 将使用一个临时清单文件来跟踪新生成的基础文件和增量文件。当它们准备好后，Redis 会执行原子替换操作，使这个临时清单文件生效。为了避免在 AOF 重写重复失败和重试的情况下创建大量增量文件的问题，Redis 引入了 AOF 重写限制机制，以确保失败的 AOF 重写以越来越慢的速度重试。</p><h4 id="日志重写"><a href="#日志重写" class="headerlink" title="日志重写"></a>日志重写</h4><p>注意AOF文件的操作,并没有读取旧的AO文件,而是将整个内存中的数据库内容用命令的方式写了一个新的aof文件替换原有的文件这点和快照有点类似</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">重写流程</span><br><span class="line">-1 redis调用fork,现在又父子两个进程,子进程根据内存中的数据库快照,往临时文件中写入重建数据库状态的命令</span><br><span class="line">-2 父进程继续处理client请求,除了把写命令写入到原来的aof文件中.同时把接收到的写命令缓存起来.这样就能保证如果子进程重写失败的话,不会丢失数据</span><br><span class="line">-3 当子进程把快照内容写入己命令写到临时文件中后,子进程发信号通知父进程,然后父进程把缓存的命令也写到临时文件中</span><br><span class="line">-4 现在父进程可以使用临时文件替换旧的aof文件,并重命名,后面收到的写命令也开始往新的aof文件中追加.</span><br></pre></td></tr></table></figure><p><strong>Redis7.0.0之前：</strong></p><p><img src="https://s2.loli.net/2022/05/18/5dzkXMxvcObFf6A.png" alt="image-20220512225149515"></p><p><strong>Redis7.0.0之后：</strong></p><p><img src="https://s2.loli.net/2022/05/18/bovIWStHZL1hFaO.png" alt="image-20220513120013948"></p><h4 id="1-4-持久化总结"><a href="#1-4-持久化总结" class="headerlink" title="1.4 持久化总结"></a>1.4 持久化总结</h4><p>两种持久化方案既可以同时使用(aof),又可以单独使用,在某种情况下也可以都不使用,具体使用哪种持久化方案取决于用户的数据和用用决定.</p><p>无论使用AOF还是快照机制持久化,将数据持久化到硬盘都是有必要的,除了持久化之外,用户还应该对持久化的文件进行备份(异地备份)以最大安全保障数据的完整性.</p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL5.7概述以及下载安装（centOS7）</title>
      <link href="/2022/04/27/MySQL%E6%A6%82%E8%BF%B0%E4%BB%A5%E5%8F%8A%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85/"/>
      <url>/2022/04/27/MySQL%E6%A6%82%E8%BF%B0%E4%BB%A5%E5%8F%8A%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h1 id="MySQL5-7概述以及下载安装（centOS7）"><a href="#MySQL5-7概述以及下载安装（centOS7）" class="headerlink" title="MySQL5.7概述以及下载安装（centOS7）"></a>MySQL5.7概述以及下载安装（centOS7）</h1><h2 id="一、MySQL简介"><a href="#一、MySQL简介" class="headerlink" title="一、MySQL简介"></a>一、MySQL简介</h2><blockquote><p>MySQL是一个典型的关系数据库，目前是Oracle公司产品之一，也是目前主流使用的关系型数据库之一。使用MySQL可以进行最基本的数据存储、管理、查询等操作，也可以方便的组建数据库集群，配置读写分离。</p><p>MySQL数据库同样使用SQL（结构化查询语言）来进行操作，同时MySQL数据库自身也有很多可以直接使用的内置函数，在部分操作的语法上和其他数据库会存在区别。</p></blockquote><h2 id="二、版本选择"><a href="#二、版本选择" class="headerlink" title="二、版本选择"></a>二、版本选择</h2><h3 id="1-应用场景"><a href="#1-应用场景" class="headerlink" title="1.    应用场景"></a>1.    应用场景</h3><h5 id="社区版"><a href="#社区版" class="headerlink" title="社区版"></a>社区版</h5><blockquote><p>在学习阶段，可以使用免费的社区版，这也是中小型企业会选用的一个版本，可以在官方网站直接进行下载。在社区版中，除了提供数据库服务端以外，同样提供了社区版相关组件，如官方的可视化工具、MySQL集群、各开发语言数据库驱动等，可以根据需要直接下载。</p></blockquote><p><img src="D:/%E9%98%BF%E9%87%8C%E4%BA%91%E7%9B%98%E6%96%87%E4%BB%B6/MySQL%E6%A6%82%E8%BF%B0%E4%BB%A5%E5%8F%8A%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85.assets/1.png" alt="1"></p><h5 id="企业版"><a href="#企业版" class="headerlink" title="企业版"></a>企业版</h5><blockquote><p>MySQL企业版是提供了商用的解决方案，相关的产品除了数据库服务外，还包括：MySQL云服务、企业级数据备份、企业级防火墙、企业级数据加密等。</p></blockquote><h3 id="2-MySQL版本"><a href="#2-MySQL版本" class="headerlink" title="2.    MySQL版本"></a>2.    MySQL版本</h3><blockquote><p>目前MySQL官网提供了三个大版本的支持，5.6.x、5.7.x、8.0.x。8.x版本相较于5.7版本，在性能方面做出了较大的改进和优化：2x Faster than MySQL5.7!</p><p>在8.0的MySQL数据库中，对某些常用语法的细节部分也做了调整，<strong>如果准备进行升级，一定要注意兼容性的问题</strong>。<br>而5.7版本相较于5.6版本而言，主要是进行了性能上的优化，并提供了更丰富的设置。如：新增了优化器、原生JSON支持、GIS扩展等。</p></blockquote><h2 id="三、下载地址"><a href="#三、下载地址" class="headerlink" title="三、下载地址"></a>三、下载地址</h2><h3 id="1-官网地址"><a href="#1-官网地址" class="headerlink" title="1.    官网地址"></a>1.    官网地址</h3><blockquote><p>首先来到MySQL数据库官网，直接在百度搜索MySQL就可以找到：<a href="https://www.mysql.com/%EF%BC%8C%E6%89%93%E5%BC%80%E4%B9%8B%E5%90%8E%E7%9B%B4%E6%8E%A5%E7%82%B9%E5%87%BB**DOWNLOADS**%E6%8C%89%E9%92%AE%E3%80%82">https://www.mysql.com/，打开之后直接点击**DOWNLOADS**按钮。</a></p></blockquote><h3 id="2-社区版下载"><a href="#2-社区版下载" class="headerlink" title="2.    社区版下载"></a>2.    社区版下载</h3><blockquote><p>进入下载界面后，点击<strong>MySQL Community（GPL）Downloads</strong>按钮进入下载界面：</p></blockquote><p><img src="https://s2.loli.net/2022/04/29/YdSUjexPNmyIiWa.png" alt="image-20220429195827454"></p><blockquote><p>选择<strong>MySQL Community Server</strong>：</p></blockquote><p><img src="https://s2.loli.net/2022/04/29/7Jgtn3lzX6OeTcF.png" alt="image-20220429195858199"></p><blockquote><p>直击链接：<a href="https://dev.mysql.com/downloads/mysql/">https://dev.mysql.com/downloads/mysql/</a></p></blockquote><h3 id="3-选择版本"><a href="#3-选择版本" class="headerlink" title="3.    选择版本"></a>3.    选择版本</h3><blockquote><p>对于Linux平台而言而言，如果是解压安装基本没有任何差别。如果是软件包安装，在下载时一定要选择相应的版本。目前官网提供两个大的稳定版，一个是<strong>5.7</strong>，一个是<strong>8.0</strong>，将演示如何在CentOS 7系统下安装MySQL 5.7。</p></blockquote><p>进入界面后点击<strong>Looking for previous GA versions</strong>链接：</p><p><img src="https://s2.loli.net/2022/04/29/Xe6UCnRI4Z1NdSf.png" alt="image-20220429195935377"></p><blockquote><p>依次选择<strong>操作系统</strong> -&gt; <strong>系统版本</strong> -&gt; <strong>需要下载的软件包</strong>：</p><p>下载的软件包需要：common、client、libs、server。</p></blockquote><p><img src="https://s2.loli.net/2022/04/29/HRfwc8uvV17gsKT.png" alt="image-20220429195958113"></p><h3 id="4-下载安装包"><a href="#4-下载安装包" class="headerlink" title="4.    下载安装包"></a>4.    下载安装包</h3><blockquote><p>进入下载界面后，直接点击 No thanks,just start my download. 链接即可直接下载。</p></blockquote><p><img src="https://s2.loli.net/2022/04/29/V1yAObzluIWtN72.png" alt="image-20220429200026929"></p><blockquote><p>windows下载后的截图</p></blockquote><p><img src="https://s2.loli.net/2022/04/29/PpeOLoHWMQJN1gS.png" alt="image-20220429200037001"></p><h2 id="四、安装步骤"><a href="#四、安装步骤" class="headerlink" title="四、安装步骤"></a>四、安装步骤</h2><blockquote><p>RPM软件包格式适用于所有基于RedHat内核的Linux发行版，包括CentOS等，在安装之前都要解决好依赖和冲突的问题。由于在安装系统时所选的组件不同，可能预安装的系统环境有所不同。</p></blockquote><h3 id="1-解决依赖冲突"><a href="#1-解决依赖冲突" class="headerlink" title="1.    解决依赖冲突"></a>1.    解决依赖冲突</h3><blockquote><p>在安装之前先检查一下是否有历史版本，包括可能产生冲突的mariadb软件包。</p></blockquote><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">rpm -qa|grep mysql</span><br><span class="line">rpm -qa|grep MySQL</span><br><span class="line">rpm -qa|grep mariadb</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/04/29/sXkPU9On2zQoLZi.png" alt="image-20220429200048782"></p><blockquote><p>查询出软件信息后进行卸载</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rpm -e --nodeps mariadb-libs-5.5.65-1.el7.x86_64</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/04/29/sXkPU9On2zQoLZi.png"></p><h3 id="2-解决依赖缺失"><a href="#2-解决依赖缺失" class="headerlink" title="2.    解决依赖缺失"></a>2.    解决依赖缺失</h3><blockquote><p>在CentOS系统中安装MySQL时通常会缺少Data::Dumper，需要先进行安装。</p><p><strong>注意：(后续需要什么我们按照同样方式安装即可)</strong></p><p>在有网的环境下，可以直接使用yum安装。</p></blockquote><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">yum -y install autoconf</span><br></pre></td></tr></table></figure><h3 id="3-MySQL服务端安装"><a href="#3-MySQL服务端安装" class="headerlink" title="3.    MySQL服务端安装"></a>3.    MySQL服务端安装</h3><blockquote><p>将下载的rpm包上传至Linux服务器中</p></blockquote><p><img src="https://s2.loli.net/2022/04/29/Sd7ulYDOUPVcgwm.png" alt="image-20220429200113190"></p><blockquote><p>MySQL服务端的安装包为server，安装的顺序为：<strong>common -&gt; libs -&gt; client -&gt; server</strong>。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rpm -ivh mysql-community-common-5.7.25-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-libs-5.7.25-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-client-5.7.25-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-server-5.7.25-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><blockquote><p>rpm -ivh mysql-community-common-5.7.25-1.el7.x86_64.rpm</p></blockquote><p><img src="https://s2.loli.net/2022/04/29/Qw5DSM23lu1zky8.png" alt="image-20220429200122777"></p><blockquote><p>rpm -ivh mysql-community-libs-5.7.25-1.el7.x86_64.rpm</p></blockquote><p><img src="https://s2.loli.net/2022/04/29/lCEse3qhYFNz2JQ.png" alt="image-20220429200141913"></p><blockquote><p>rpm -ivh mysql-community-client-5.7.25-1.el7.x86_64.rpm</p></blockquote><p><img src="https://s2.loli.net/2022/04/29/lCEse3qhYFNz2JQ.png"></p><blockquote><p>rpm -ivh mysql-community-server-5.7.25-1.el7.x86_64.rpm(<strong>到这里可能会报错，如下图所示</strong>：)</p></blockquote><p><img src="https://s2.loli.net/2022/04/29/97E13YhFpGVJOSP.png"></p><blockquote><p>只需要把对应的包安装一下即可</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install net-tools</span><br></pre></td></tr></table></figure><blockquote><p>再执行就可以了</p><p>rpm -ivh mysql-community-server-5.7.25-1.el7.x86_64.rpm</p></blockquote><p><img src="https://s2.loli.net/2022/04/29/PAThaFMHZKR23V6.png" alt="image-20220429200225159"></p><blockquote><p>安装完成后创建的配置文件存放在**&#x2F;etc&#x2F;my.cnf**，如果需要进行一些自定义配置，可以修改该文件。</p></blockquote><h2 id="五、使用测试"><a href="#五、使用测试" class="headerlink" title="五、使用测试"></a>五、使用测试</h2><h3 id="1-启动数据库服务"><a href="#1-启动数据库服务" class="headerlink" title="1.    启动数据库服务"></a>1.    启动数据库服务</h3><blockquote><p>使用root用户来启动服务：</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl start mysqld.service</span><br></pre></td></tr></table></figure><blockquote><p>检查服务状态：</p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl status mysqld.service</span><br></pre></td></tr></table></figure><blockquote><p>查看到下图状态证明启动成功。</p></blockquote><p><img src="https://s2.loli.net/2022/04/29/PAThaFMHZKR23V6.png"></p><h3 id="2-首次连接修改密码"><a href="#2-首次连接修改密码" class="headerlink" title="2.    首次连接修改密码"></a>2.    首次连接修改密码</h3><blockquote><p>MySQL在启动后会产生一个日志文件，存放在 <strong>&#x2F;var&#x2F;log&#x2F;mysqld.log</strong> ，其中包含了初始密码的信息，可以通过以下命令快速找到：</p></blockquote><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">grep <span class="string">&#x27;temporary password&#x27;</span> /var/log/mysqld.log</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/04/29/vxqckftduipNZSR.png" alt="image-20220429200247691"></p><blockquote><p><strong>复制时注意前后都不要有空格。</strong></p></blockquote><p>使用客户端命令连接</p><blockquote><p>mysql -uroot -pfqomD#lTo5lH</p></blockquote><p><img src="https://s2.loli.net/2022/04/29/2jYR1fn6AP5ogXm.png" alt="image-20220429200256023"></p><blockquote><p>修改安全策略</p><p>MySQL 5.7版本会安装一个密码校验插件，要求设置的密码必须在一定的位数并且要符合密码安全策略（有一定的复杂性），在学习阶段可以先调低策略，设置为一个比较简单的密码。<br>验证策略修改为low（只校验密码长度）：</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">global</span> validate_password_policy<span class="operator">=</span><span class="number">0</span>;</span><br></pre></td></tr></table></figure><blockquote><p>修改最小密码长度为4：</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">global</span> validate_password_length<span class="operator">=</span><span class="number">4</span>;</span><br></pre></td></tr></table></figure><blockquote><p>设置新密码：</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> password <span class="operator">=</span> password(<span class="string">&#x27;123456&#x27;</span>);</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/04/29/8BuKyF7wbpzClne.png" alt="image-20220429200422898"></p><blockquote><p>使用新密码连接<br>修改密码后使用 exit; 退出，重新使用新密码登录测试。</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql <span class="operator">-</span>uroot <span class="operator">-</span>p</span><br></pre></td></tr></table></figure><p><img src="C:\Users\xiaohu\AppData\Roaming\Typora\typora-user-images\image-20220428201943496.png" alt="image-20220428201943496"></p><blockquote><p>添加主机名连接规则<br>对于MySQL数据库，会将用户的登录密码及权限等信息存放在mysql.user表中，可以先通过以下命令查看一下：</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">user</span>,host,authentication_string <span class="keyword">from</span> mysql.user;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/04/29/SkI1CVrlNm7qiFX.png" alt="image-20220429200440293"></p><blockquote><p>为方便以后使用,添加任意主机的连接规则，命令如下：</p></blockquote><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">GRANT</span> <span class="keyword">ALL</span> PRIVILEGES <span class="keyword">ON</span> <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">TO</span> <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;%&#x27;</span> IDENTIFIED <span class="keyword">BY</span> <span class="string">&#x27;123456&#x27;</span> <span class="keyword">WITH</span> <span class="keyword">GRANT</span> OPTION;</span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><h3 id="3-数据库远程连接-课程使用Navicat"><a href="#3-数据库远程连接-课程使用Navicat" class="headerlink" title="3.    数据库远程连接(课程使用Navicat)"></a>3.    数据库远程连接(课程使用Navicat)</h3><p>在Linux系统中安装好数据库以后，通常我们都会使用界面工具来进行远程连接。这个时候可以通过两种方式实现，如果只是需要通过界面工具远程查看数据库情况，可以通过SSH通道的方式连接，这样更安全。<br>如果需要在代码中直接连接远程数据进行调试，此时就需要在MySQL数据库中开启远程连接，也就是需要添加一个连接规则。</p><p><img src="https://s2.loli.net/2022/04/29/Uwtluy85QH4nmxo.png" alt="image-20220429200457727"></p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mysql5.7 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络编程</title>
      <link href="/2022/04/25/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"/>
      <url>/2022/04/25/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h3 id="网络编程"><a href="#网络编程" class="headerlink" title="网络编程"></a>网络编程</h3><h4 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h4><p>一般是指  <strong>OSI</strong>（Open System Interconnection开放系统互连）参考模型</p><p> TCP&#x2F;IP参考模型</p><p><img src="https://s2.loli.net/2022/04/12/im2Ahos6F5baYTI.png" alt="屏幕截图 2022-04-12 223205"></p><h4 id="网络通信三要素"><a href="#网络通信三要素" class="headerlink" title="网络通信三要素"></a>网络通信三要素</h4><p>1、IP地址:InetAddress</p><p>2、端口号</p><p>3、传输协议：UDP、TCP</p><h5 id="InetAddress类的使用"><a href="#InetAddress类的使用" class="headerlink" title="InetAddress类的使用"></a>InetAddress类的使用</h5><blockquote><p>获取任意主机：getByName</p><p>主机名：getHostName</p><p>主机Ip地址：getHostAddress</p></blockquote><h5 id="端口号"><a href="#端口号" class="headerlink" title="端口号"></a>端口号</h5><blockquote><p>1、物理端口     网卡口</p></blockquote><blockquote><p>2、逻辑端口     一般端口号指的就是逻辑端口</p><p>有效端口：0<del>65535，其中0</del>1024系统使用或保留端口        通过netstat -ano可以查看端口号</p></blockquote><h5 id="传输协议：UDP、TCP"><a href="#传输协议：UDP、TCP" class="headerlink" title="传输协议：UDP、TCP"></a>传输协议：UDP、TCP</h5><blockquote><p>UDP：将数据源和目的封装成数据包中，不需要建立连接；每个数据报包的大小在限制在64k；因无连接，是不可靠协议；不需要建立连接，速度快</p></blockquote><blockquote><p>TCP：建立连接，形成传输数据的通道；在连接中进行大数据量传输；通过三次握手完成连接，是可靠协议；必须建立连接，效率会稍低</p></blockquote><h5 id="Socket"><a href="#Socket" class="headerlink" title="Socket"></a>Socket</h5><p>Socket套接字：</p><blockquote><p>•网络上具有唯一标识的IP地址和端口号组合在一起才能构成唯一能识别的标识符套接字。</p><p>Socket原理机制：</p><p>•通信的两端都有Socket。</p><p>•网络通信其实就是Socket间的通信。</p><p>•数据在两个Socket间通过IO传输。</p></blockquote><p><img src="https://s2.loli.net/2022/04/12/ztSLHWufJV4QpXm.png" alt="image-20220412213817650"></p><h5 id="UDP传输"><a href="#UDP传输" class="headerlink" title="UDP传输"></a>UDP传输</h5><blockquote><p>1、DatagramSocket与DatagramPacket</p><p>2、建立发送端，接收端。</p><p>3、建立数据包。</p><p>4、调用Socket的发送接收方法。</p><p>5、关闭Socket。</p><p>6、发送端与接收端是两个独立的运行程序。</p></blockquote><h5 id="UDP传输-发送端思路"><a href="#UDP传输-发送端思路" class="headerlink" title="UDP传输-发送端思路"></a>UDP传输-发送端思路</h5><blockquote><p>1:建立udp的socket服务</p><p>2:将要发送的数据封装成数据包</p><p>3:通过udp的socket服务,将数据包发送出</p><p>4:关闭资源</p></blockquote><h5 id="UDP传输-接收端思路"><a href="#UDP传输-接收端思路" class="headerlink" title="UDP传输-接收端思路"></a>UDP传输-接收端思路</h5><blockquote><p>1:建立udp的socket服务</p><p>2:通过receive方法接收数据</p><p>3:将收到的数据存储到数据包对象中</p><p>4:通过数据包对象的功能来完成对接收到数据进行解析</p><p>5:可以对资源进行关闭</p></blockquote><h5 id="TCP"><a href="#TCP" class="headerlink" title="TCP"></a>TCP</h5><blockquote><p>1、Socket和ServerSocket</p><p>2、建立客户端和服务器端</p><p>3、建立连接后，通过Socket中的IO流进行数据的传输</p><p>4、关闭socket</p><p>5、同样，客户端与服务器端是两个独立的应用程序。</p></blockquote><h5 id="TCP传输-客户端思路"><a href="#TCP传输-客户端思路" class="headerlink" title="TCP传输-客户端思路"></a>TCP传输-客户端思路</h5><blockquote><p>1:建立客户端的Socket服务,并明确要连接的服务器。</p><p>2:如果连接建立成功,就表明,已经建立了数据传输的通道.就可以在该通道通过IO进行数据的读取和写入.该通道称为Socket流,Socket流中既有读取流,也有写入流.</p><p>3:通过Socket对象的方法,可以获取这两个流</p><p>4:通过流的对象可以对数据进行传输</p><p>5:如果传输数据完毕,关闭资源</p></blockquote><h5 id="TCP传输-服务器端思路"><a href="#TCP传输-服务器端思路" class="headerlink" title="TCP传输-服务器端思路"></a>TCP传输-服务器端思路</h5><blockquote><p>1:建立服务器端的socket服务，需要一个端口</p><p>2:服务端没有直接流的操作,而是通过accept方法获取客户端对象，在通过获取到的客户端对象的流和客户端进行通信</p><p>3:通过客户端的获取流对象的方法,读取数据或者写入数据</p><p>4:如果服务完成,需要关闭客户端,然后关闭服务器，但是,一般会关闭客户端,不会关闭服务器,因为服务端是一直提供服务的</p></blockquote><h5 id="TCP传输容易出现的问题"><a href="#TCP传输容易出现的问题" class="headerlink" title="TCP传输容易出现的问题"></a>TCP传输容易出现的问题</h5><blockquote><p>客户端连接上服务端，两端都在等待，没有任何数据传输。</p><p>通过例程分析：</p><p>因为read方法或者readLine方法是阻塞式。</p><p>解决办法：</p><p>自定义结束标记</p><p>使用shutdownInput，shutdownOutput方法。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux用户管理</title>
      <link href="/2022/04/24/linux%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86/"/>
      <url>/2022/04/24/linux%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="1、用户组管理"><a href="#1、用户组管理" class="headerlink" title="1、用户组管理"></a>1、用户组管理</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">用户组的管理包括用户组的添加、删除和修改。</span><br><span class="line"></span><br><span class="line">为什么要建立用户组</span><br><span class="line"></span><br><span class="line">人事部有20名员工，我们要建立一个组，叫 hr，这样就不用分别给20个员工设置权限了。</span><br></pre></td></tr></table></figure><h3 id="①-用户组添加"><a href="#①-用户组添加" class="headerlink" title="① 用户组添加"></a>① 用户组添加</h3><p>命令：groupadd</p><p>作用：添加组</p><p>语法：# groupadd  [参数选项  选项值]  用户组名</p><p>选项：-g：设置用户组ID 数字，如果不指定，则默认从1000 之后递增（1-999系统组）</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">用法一：groupadd 组名</span><br><span class="line">示例代码：</span><br><span class="line"><span class="comment">#groupadd bigdata</span></span><br><span class="line">含义：新建一个组叫做bigdata</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/04/25/npRaHjm9SbX6M1k.png" alt="image-20220425210757721"></p><p>提示：linux下我们执行完命令，有时候会没有任何提示，直接回到#提示符，这种状态表明，命令执行成功，没有报错。&#x3D;&#x3D;“没有消息就是最好的消息”&#x3D;&#x3D;</p><p>存储用户组信息的文件：&#x2F;etc&#x2F;group<br> 使用cat命令，查看&#x2F;etc&#x2F;group文件</p><p>&#x2F;etc&#x2F;group文件结构：</p><p>特别说明：</p><p>1） 密码位<code>x</code>代表<code>占位符</code>，用户组可以设置密码，但是大部分情况下不需要设置</p><p>2）组内用户名：表示附加组是该组的用户名称。</p><p><img src="https://s2.loli.net/2022/04/25/YDOHwAhScxNpVGe.png" alt="image-20220425211850494"></p><h3 id="②-用户组修改"><a href="#②-用户组修改" class="headerlink" title="② 用户组修改"></a>② 用户组修改</h3><p>命令：groupmod</p><p>语法：# groupmod   [选项   选项值]   用户组名</p><p>选项：-g  ：gid缩写，设置一个自定义的用户组ID 数字</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-n  ：name缩写，设置新的用户组的名称</span><br></pre></td></tr></table></figure><p>示例代码：修改bigdata用户组，将组ID改成1100，将名称改为bigdata1</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">用法一：groupmod -g 新的组ID -n 新的组ID 原有组ID</span><br><span class="line">示例代码：</span><br><span class="line">#groupmod -g 1100 -n bigdata1 bigdata</span><br><span class="line">含义：将bigdata组的组ID改成1100，组名改成bigdata1</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/04/25/59HpA2iK1Cvdxb6.png" alt="image-20220425220023602"></p><h3 id="③-用户组删除"><a href="#③-用户组删除" class="headerlink" title="③ 用户组删除"></a>③ 用户组删除</h3><p>命令：groupdel</p><p>语法：# groupdel  用户组名</p><p>案例：删除bigdata1组</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">用法一：groupdel 组名</span><br><span class="line">示例代码：</span><br><span class="line"><span class="comment">#groupdel bigdata1</span></span><br><span class="line">含义：将bigdata1组删除</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/04/25/EtZWhnz6brfRJxg.png" alt="image-20220425221937616"></p><h2 id="2、用户管理"><a href="#2、用户管理" class="headerlink" title="2、用户管理"></a>2、用户管理</h2><p>用户的管理涉及用户的添加、删除和修改。</p><p>与用户相关的文件：&#x2F;etc&#x2F;passwd</p><h3 id="①useradd添加用户"><a href="#①useradd添加用户" class="headerlink" title="①useradd添加用户"></a>①useradd添加用户</h3><p>命令：useradd</p><p>作用：添加用户</p><p>语法：# useradd   [选项  选项的值]   …   用户名</p><p>选项：-g：表示指定用户的用户主（主要）组，选项值可以是用户组ID，也可以是组名</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-G：表示指定用户的用户附加（额外）组，选项值可以是用户组ID，也可以是组名</span><br><span class="line"></span><br><span class="line">-u ：uid，用户的id（用户的标识符），系统默认会从500 /或1000之后按顺序分配uid，如果不想使用系统分配的，可以通过该选项自定义【类似于腾讯QQ 的自选靓号情况】</span><br><span class="line"></span><br><span class="line">-c：comment，添加注释（选择是否添加）</span><br><span class="line"></span><br><span class="line">-s：指定用户登入后所使用的shell 解释器，默认/bin/bash【专门的接待员】，如果不想让其登录，则可以设置为/sbin/nologin   （重要）</span><br><span class="line"></span><br><span class="line">-d：指定用户登入时的启始目录（家目录位置）</span><br><span class="line"></span><br><span class="line">    -n：取消建立以用户名称为名的群组（了解）</span><br><span class="line"></span><br><span class="line"> 当我新建一个账户叫user01, 同时，系统会自动建立一个组也叫user01</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">用法一：useradd 用户名</span><br><span class="line">示例代码：</span><br><span class="line">#useradd hzh</span><br><span class="line">含义：创建用户hzh，不带任何选项。</span><br></pre></td></tr></table></figure><p>注意：不用任何参数，创建用户，系统会默认执行以下操作：</p><p>1）在 &#x2F;etc&#x2F;passwd 文件中创建一行关于hzh用户的数据</p><p><img src="https://s2.loli.net/2022/04/25/l149argXGmFtVoK.png" alt="image-20220425222337183"></p><p> 2）在 &#x2F;etc&#x2F;shadow 文件中新增了一行关于wyh密码的数据</p><p><img src="https://s2.loli.net/2022/04/25/oqDQihSmxaUlYCJ.png" alt="image-20220425224109139"></p><p> 3）在 &#x2F;etc&#x2F;group 文件中创建一行与用户名相同的组，例如wyh</p><p><img src="https://s2.loli.net/2022/04/25/V1fFiYKbUw5eumD.png" alt="image-20220425224156762"></p><p> 4）在 &#x2F;etc&#x2F;gshadow 文件中新增一行与新增群组相关的密码信息，例如wyh</p><p><img src="https://s2.loli.net/2022/04/25/mgCprBDyfAshOKx.png" alt="image-20220425224224338"></p><p> 5）自动创建用户的家目录，默认在&#x2F;home下，与用户名同名</p><p><img src="https://s2.loli.net/2022/04/25/k4gRO2fNYBuovF9.png" alt="image-20220425224328709"></p><p>验证是否成功：</p><p>1）使用tail文件查看&#x2F;etc&#x2F;passwd文件</p><p>2）使用tail文件查看&#x2F;etc&#x2F;group文件</p><p>3）验证是否存在家目录（在Centos 下创建好用户之后随之产生一个同名家目录）</p><h3 id="②etc-x2F-passwd存储用户信息的文件"><a href="#②etc-x2F-passwd存储用户信息的文件" class="headerlink" title="②etc&#x2F;passwd存储用户信息的文件"></a>②etc&#x2F;passwd存储用户信息的文件</h3><p>使用vim命令打开&#x2F;etc&#x2F;passwd文件</p><figure class="highlight jsx"><table><tr><td class="code"><pre><span class="line">root : x : <span class="number">0</span> : <span class="number">0</span> : root : <span class="regexp">/root : /</span>bin/bash</span><br><span class="line">用户名 : 密码 : 用户<span class="variable constant_">ID</span> : 用户组<span class="variable constant_">ID</span> : 注释 : 家目录 : 解释器shell</span><br></pre></td></tr></table></figure><p><strong>用户名</strong>：登录linux时使用的用户名<br> <strong>密码</strong>：此密码位置一般情况都是”x”，表示密码的占位，真实密码存储在&#x2F;etc&#x2F;shadow<br> <strong>用户ID</strong>：用户的识别符，每个用户都有唯一的UID【-u】<br> <strong>用户组ID</strong>：该用户所属的主组ID；【-g】</p><p><strong>注释</strong>：解释该用户是做什么用的；【-c】<br> <strong>家目录</strong>：用户登录进入系统之后默认的位置；【-d】<br> <strong>解释器shell</strong>：等待用户进入系统之后，用户输入指令之后，该解释器会收集用户输入的指令，转换成机器语言，传递给内核处理；如果解释器是&#x3D;&#x3D;&#x2F;bin&#x2F;bash 表示用户可以登录到系统&#x3D;&#x3D;，&#x3D;&#x3D;&#x2F;sbin&#x2F;nologin表示该用户不能登录到系统&#x3D;&#x3D;【-s】</p><p>下面我们来看一下对于useradd参数的使用</p><p><strong>企业场景1：</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">     公司新员工lisi，属于bigdata部门，用户ID1200，不允许登录系统</span><br><span class="line">思路：</span><br><span class="line">    创建用户lisi，默认lisi属于自己同名的主组，让lisi 属于附加组bigdata1，用户ID 1200，注释为<span class="string">&quot;数据工程师lisi&quot;</span>，解释器为/sbin/nologin</span><br></pre></td></tr></table></figure><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">用法二：useradd <span class="literal">-G</span> 附加组名 <span class="literal">-u</span> 用户ID <span class="literal">-s</span> /sbin/nologin <span class="literal">-c</span> <span class="string">&quot;shuser lisi&quot;</span> 用户名</span><br><span class="line">示例代码：</span><br><span class="line">[<span class="type">root</span>@<span class="type">master</span> <span class="type">hzh</span>]<span class="comment"># useradd -G bigdata -u 1200 -s /sbin/nologin -c &quot;数据工程师lisi&quot; lisi</span></span><br><span class="line">useradd：“bigdata”组不存在</span><br><span class="line">[<span class="type">root</span>@<span class="type">master</span> <span class="type">hzh</span>]<span class="comment"># groupadd bigdata</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">master</span> <span class="type">hzh</span>]<span class="comment"># useradd -G bigdata -u 1200 -s /sbin/nologin -c &quot;数据工程师lisi&quot; lisi</span></span><br><span class="line"></span><br><span class="line">含义：创建用户lisi，不带任何选项。</span><br></pre></td></tr></table></figure><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">bigdata行的含义：在bigdata的组里（组<span class="type">id</span> 是<span class="number">1002</span>）有一个组内用户lisi（lisi 的附加组就是<span class="number">1002</span>，附加组的名字是bigdata1）。</span><br><span class="line">如果需要为一个用户指定多个附加组，只需要将多个附加组的<span class="type">id</span> 通过英文逗号“,”分割即可。</span><br><span class="line">例如-G <span class="number">500</span>,<span class="number">501</span>,<span class="number">502</span></span><br><span class="line"></span><br><span class="line">① 主组只能有<span class="number">1</span> 个（类似于亲生父母只有一对），附加组可以多个，也可以没有附加组（类似于认干爹干妈，可以有也可以没有，也可以有多个）</span><br><span class="line">② 主组必须有</span><br><span class="line">③ 后期将权限管理的时候，关于文档的属组指的是主组（了解）</span><br></pre></td></tr></table></figure><h3 id="③id查看用户信息"><a href="#③id查看用户信息" class="headerlink" title="③id查看用户信息"></a>③id查看用户信息</h3><p>命令：id</p><p>作用：查看一个用户的一些基本信息（包含用户id，用户组id，附加组id…），该指令如果不指定用户则默认当前用户。</p><p>语法1：# id  <code>默认显示当前执行该命令的用户的基本信息</code></p><p><img src="https://s2.loli.net/2022/04/25/vXxAD93nSeRUKhf.png" alt="image-20220425224939961"></p><p> 语法2：# id <code>用户名</code>， 显示指定用户的基本信息</p><p> 如何验证以上信息是否正确？</p><p>&#x3D;&#x3D;答：验证用户信息：通过文件&#x2F;etc&#x2F;passwd，验证用户组信息：通过文件&#x2F;etc&#x2F;group&#x3D;&#x3D;</p><h3 id="④usermod修改用户"><a href="#④usermod修改用户" class="headerlink" title="④usermod修改用户"></a>④usermod修改用户</h3><p>命令：usermod(user modify)</p><p>语法：# usermod   [选项  选项的值]   …  用户名</p><p>作用：修改用户的各种属性</p><p>选项：-g：表示指定用户的用户主组，选项的值可以是用户组的ID，也可以是组名</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">-G：表示指定用户的用户附加组，选项的值可以是用户组的ID，也可以是组名</span><br><span class="line"></span><br><span class="line">-u：uid，用户的id（用户的标识符），系统默认会从500 之后按顺序分配uid，如果不想使用系统分配的，可以通过该选项自定义【类似于腾讯QQ 的自选靓号情况】</span><br><span class="line"></span><br><span class="line"> -L：锁定用户，锁定后用户无法登陆系统lock</span><br><span class="line"></span><br><span class="line">     -U：解锁用户unlock</span><br><span class="line"></span><br><span class="line"> -c&lt;备注&gt;：修改用户帐号的备注文字</span><br><span class="line"></span><br><span class="line"> -d&lt;登入目录&gt;：修改用户登入时的目录</span><br><span class="line"></span><br><span class="line"> -s<span class="tag">&lt;<span class="name">shell</span>&gt;</span>：修改用户登入后所使用的shell</span><br></pre></td></tr></table></figure><p><strong>企业场景2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">公司员工wangwu，属于bigdata部门，现在要休产假，产假期间，暂时停止她登陆电脑的权限，同时原来属于wyhshujia部门的员工lisi，负责wangwu的工作，所以，需要把lisi加入到bigdata的组，同时，修改lisi的账户注释为“wyhshujia bigdata user”</span><br><span class="line"></span><br><span class="line">对于wangwu用户，我们要执行锁定和解锁操作</span><br><span class="line"></span><br><span class="line">对于lisi用户，我们要将lisi加入到bigdata的附加组，同时修改lisi账户的注释</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@master hzh]<span class="comment"># usermod -L wangwu</span></span><br><span class="line">usermod：用户“wangwu”不存在</span><br><span class="line">[root@master hzh]<span class="comment"># useradd -G bigdata -u 1300 -c &quot;大数据开发&quot; wangwuuseradd：“bigdata”组不存在</span></span><br><span class="line">[root@master hzh]<span class="comment"># groupadd bigdata</span></span><br><span class="line">[root@master hzh]<span class="comment"># useradd -G bigdata -u 1300 -c &quot;大数据开发&quot; wangwu</span></span><br><span class="line">[root@master hzh]<span class="comment"># usermod -L wangwu</span></span><br><span class="line"></span><br><span class="line">用法一：usermod -L 王五账户名</span><br><span class="line">示例代码：</span><br><span class="line"><span class="comment">#usermod -L wangwu</span></span><br><span class="line">含义：将王五账户暂时锁定</span><br><span class="line"></span><br><span class="line">用法二：usermod -U 王五账户名</span><br><span class="line">示例代码：</span><br><span class="line"><span class="comment">#usermod -U wangwu</span></span><br><span class="line">含义：将王五账户解锁</span><br></pre></td></tr></table></figure><figure class="highlight swift"><table><tr><td class="code"><pre><span class="line">用法三：usermod <span class="operator">-</span><span class="type">G</span> 组名 <span class="operator">-</span>c “注释内容” 李四用户账号</span><br><span class="line">示例代码：</span><br><span class="line">#usermod <span class="operator">-</span><span class="type">G</span> bigdata <span class="operator">-</span>c <span class="string">&quot;bigdata user&quot;</span> lisi</span><br><span class="line">含义：将李四的账户加入bigdata组，并修改注释内容为shhr user</span><br></pre></td></tr></table></figure><h3 id="⑤passwd修改用户密码"><a href="#⑤passwd修改用户密码" class="headerlink" title="⑤passwd修改用户密码"></a>⑤passwd修改用户密码</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Linux ==不允许没有密码的用户登录到系统==，因此前面创建的用户目前都处于锁定状态，需要设置密码之后才能登录计算机。</span><br></pre></td></tr></table></figure><p>命令：passwd</p><p>语法：# passwd  用户名 【如果不指定用户名则修改自己的密码】</p><p>作用：修改用户密码<br> <strong>企业场景3：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">王五产假休完回到公司上班，需要将王五的账户解锁，在使用usermod -U解锁时，我们看到一个错误信息如下：</span><br></pre></td></tr></table></figure><p>usermod: unlocking the user’s password would result in a passwordless account.</p><p>解锁这个账户，将导致一个没有密码的账户，因为之前王五的账户没有密码。这时候，我们就需要使用passwd命令，给王五的账户设置一个密码</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">用法一：passwd 账户名</span><br><span class="line">示例代码：</span><br><span class="line"><span class="comment">#passwd wangwu</span></span><br><span class="line">含义：为wangwu账户设置密码</span><br><span class="line">注意：</span><br><span class="line">    当密码过于简单时，系统会提示这是一个不好的密码，因为它太简单了，但是我们仍然可以坚持使用这个密码。</span><br><span class="line">    在我们输入密码时，屏幕不会有任何显示。</span><br><span class="line">    密码需要输入两次，请确保两次输入的密码是一样的。</span><br></pre></td></tr></table></figure><h3 id="⑥认识-x2F-etc-x2F-shadow文件"><a href="#⑥认识-x2F-etc-x2F-shadow文件" class="headerlink" title="⑥认识&#x2F;etc&#x2F;shadow文件"></a>⑥认识&#x2F;etc&#x2F;shadow文件</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">由于 /etc/passwd文件允许所有用户读取，易导致用户密码泄露，因此 Linux 系统将用户的密码信息从 /etc/passwd 文件中分离出来，并单独放到了shadow文件中。</span><br><span class="line"></span><br><span class="line">/etc/shadow 文件只有 root 用户拥有读权限，其他用户没有任何权限，这样就保证了用户密码的安全性。  </span><br></pre></td></tr></table></figure><p>与用户密码相关的文件：&#x2F;etc&#x2F;shadow</p><p>为用户设置密码之后，会自动在&#x2F;etc&#x2F;shadow文件中进行体现，使用vim编辑器打开</p><p>第一列为用户名，例如zhangsan</p><p>后面是加密后的密码，就是$开头的字符串</p><p>如果显示为!!,则表示这个用户&#x3D;&#x3D;没有&#x3D;&#x3D;设置密码。</p><p>由以上截图所知，wyh,lisi是没有设置密码的。wangwu我们刚刚设置了密码，所以显示为一个加密的字符创</p><p>任务：新建一个账户叫shujiaxiaoli</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">新建第二个账户叫shujiaxiaoli</span><br><span class="line"></span><br><span class="line">给shujiaxiaoli账户设置一个密码</span><br><span class="line"></span><br><span class="line">进入shadow文件，观察两个账户的区别</span><br></pre></td></tr></table></figure><h3 id="⑦su切换用户"><a href="#⑦su切换用户" class="headerlink" title="⑦su切换用户"></a>⑦su切换用户</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">在设置用户密码之后就可以使用此账号进行登录系统了，如果系统处于已登录状态，则可以使用su命令进行切换用户。</span><br><span class="line"></span><br><span class="line">为了系统安全，企业中通常不会允许root用户直接登录计算机，但是工作需要，我们又需要使用root权限，这时候，我们就可以先使用一个普通用户登录计算机，再通过su命令切换到root权限。</span><br></pre></td></tr></table></figure><p>命令：su</p><p>语法：# su  [-]   账号</p><p>作用：切换用户</p><figure class="highlight css"><table><tr><td class="code"><pre><span class="line">用法一：su 用户名</span><br><span class="line">示例代码：</span><br><span class="line"><span class="selector-id">#su</span> root</span><br><span class="line">含义：切换到root权限</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line"><span class="selector-tag">a</span>. 从root 往普通用户切换不需要密码，但是反之则需要root 密码；</span><br><span class="line"><span class="selector-tag">b</span>. 切换用户之后前后的工作路径是不变的，添加了选项<span class="selector-attr">[-]</span>会自动切换到用户的家；</span><br><span class="line">c. 普通用户没有办法访问root 用户家目录，但是反之则可以；</span><br></pre></td></tr></table></figure><p>启用wheel组设置（了解）</p><p>步骤1：使用vim编辑器 打开&#x2F;etc&#x2F;pam.d&#x2F;su文件</p><p>步骤2：编辑文件，去掉auth required pam_wheel.so use_uid这一行前面的#，使这一行配置生效</p><p>步骤3：下面是去掉#后的状态</p><p>步骤4：保存退出 ：wq<br>这时，只有在wheel组内的用户才可以su到root</p><p><img src="https://s2.loli.net/2022/04/25/9CUH3kfQtROpW5K.png" alt="image-20220425225804162"></p><p> ⑧userdel删除用户<br> 命令：userdel</p><p>语法：# userdel   选项   用户名</p><p>作用：删除账户及其对应家目录</p><p>选项：-r：表示删除用户的同时，删除其家目录&#x2F;home下的对应文件夹</p><p>权限:<br>    通过ll查看详细信息时:-rw-r–r–. 1 root root     11 4月  24 21:22 xiao.txt<br>    -:表示类型<br>    rw-:当前用户权限<br>    r–:当前用户组权限<br>    r–:其他用户权限</p><pre><code>r:读     4w:写     2x:执行   1u:当前用户g:当前组o:其他a:所有</code></pre><p>权限分配<br>    chmod:修改权限<br>        格式1:(使用相加减表达权限)<br>            chmod [选项] [权限修改] [文件]<br>        格式2:(使用数字表达权限)<br>            chmod [选项] [权限修改] [文件]<br>            4:读<br>            2:写<br>            1:执行<br>            7:全部<br>        注意:如果只给一个数字表示修改o,两个表示修改go<br>        选项:<br>            -R:迭代修改</p><pre><code>chgrp:修改用户组    格式:        chgrp [选项] [组名] [文件或目录]    选项:        -R:表达迭代修改    注意:文件或目录的所有用户或所有组,都是以编号来查询所有用户或所有组        如果不存在就显示编号,存在显示名称chown:修改所属用户    格式:        chown [选项] [组名] [文件或目录]    选项:        -R:表达迭代修改sudo:越权执行    格式:        sudo 命令    注意:sudo实际上去借root权限执行命令(root对普通用户分配了权限)sudo -l:查看当前权限visudo:修改配置文件进行权限分配(文件所在位置/etc/sudoers)    例如:普通用户拥有root所有权限        用户名 ALL=(ALL) ALL        用户名 ALL=(root) ALL    例如:普通用户只能执行一个命令        用户名 ALL=(root)/bin/rm可以同过vim /etc/sudoers修改权限分配</code></pre><p>查找<br>    find<br>        格式:<br>            find 开始查找路径 [选项] [条件]<br>        选项:<br>            -name<br>                <em>表示匹配所有<br>                ?表示匹配一个<br>                例如:从&#x2F;开始查找后缀为.txt<br>                    find &#x2F; -name “</em>.txt”<br>            -type<br>                d:表示目录<br>                f:表示文件<br>                例如:从&#x2F;开始查找文件<br>                    find &#x2F; -type f<br>            -size<br>                ll –block-size&#x3D;单位<br>                    例如:大小以k为单位进行显示<br>                        ll –block-size&#x3D;k<br>                注意:条件需要给上单位<br>                    +表示大于<br>                    -表示小于<br>                    不给就是等于<br>                例如:从&#x2F;开始查找文件大小大于2k<br>                    find &#x2F; -size +2k<br>            -user<br>            -group<br>Linux常见符号<br>    |:管道,把前面一部分的内容交给后面去处理<br>    例如:<br>        cat &#x2F;etc&#x2F;profile | more</p><pre><code>grep:筛选    格式:        grep 筛选条件&gt;&gt;:追加,把命令1的结果写入到命令2    格式:        命令1 &gt;&gt; 命令2     例如:cat profile &gt;&gt; test.txt&gt;:覆盖    格式:        命令1 &gt; 命令2     例如:cat profile &gt; test.txt</code></pre>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础</title>
      <link href="/2022/04/23/linux%E5%9F%BA%E7%A1%80/"/>
      <url>/2022/04/23/linux%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><h5 id="linux基础命令："><a href="#linux基础命令：" class="headerlink" title="linux基础命令："></a>linux基础命令：</h5><blockquote><p>1、ll 和 ls    查看当前目录下所有的文件和文件夹<br>2、cd    切换目录<br>3、pwd    查看当前目录的一个完整路径<br>4、ls -a    列出当前目录下的所有文件（包括隐藏文件）<br>5、stat 文件名    查看文件信息<br>6、ls –help    查看ls用法（–help查看命令使用手册）<br>7、mkdir    文件夹的名称<br>8、mkdir-p ….    创建多级文件夹<br>9、touch 文件名    创建文件<br>10、mv    移动一个文件，可以在移动的同时修改文件名<br>11、cp    复制文件<br>12、.    当前目录    ..    上一级目录<br>13、rm 文件的名称    删除一个文件，会进行提示，输入y则表示删除，输入n表示不删除<br>14、rm -f 文件的名称    不进行提示强制删除一个文件<br>15、rm -r    删除一个文件或文件夹，会进行提示，输入y则表示删除，输入n表示不删除<br>16、rm -rf    强制删除文件或文件夹，不进行提示   使用时一定要注意路径！！！<br>17、cat 文件名    不打开文件查看文件内容<br>18、tac 文件名    不打开文件倒序查看内容<br>19、cat file1 file2 &gt; file3    文件合并   ，注意的是file3目标文件可以不存在，会自动创建，如果存在则会覆盖原本内容<br>20、cat -b    显示行号输出<br>21、分屏显示 more    用一次显示一屏，没有显示完时最后一行显示进度。回车显示下一行，按b显示上一页，空格显示下一页，q退出。</p></blockquote><h5 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h5><blockquote><p>touch 文件名    创建一个文件<br>vi 文件名    创建打开并编辑</p></blockquote><h5 id="vi-打开文件"><a href="#vi-打开文件" class="headerlink" title="vi 打开文件"></a>vi 打开文件</h5><blockquote><p>按下i进入编辑模式</p><p>按下esc 退出编辑模式</p><p>英文输入状态下 输入：</p><p>输入wq    表示保存并退出</p><p>输入q!    表示不保存退出</p><p>进入编辑模式后退格一个一个字符删除</p><p>不进入编辑模式 连续两次 d表示删除一行</p><p>另外一种编辑模式 vim</p><p>centOS7并不自带这种编辑命令，需要额外下载安装</p><p>通过yum进行下载并安装 （类比python中的pip）</p><p>yum install vim</p><p>yum -y install vim   使用这个命令在安装过程中不需要手动输入y进行继续，默认都是y</p></blockquote><h5 id="linux安装jdk1-8"><a href="#linux安装jdk1-8" class="headerlink" title="linux安装jdk1.8"></a>linux安装jdk1.8</h5><blockquote><p>1、上传jdk压缩包到&#x2F;usr&#x2F;local&#x2F;soft&#x2F;目录下<br>2、使用解压命令进行解压  tar -zxvf jdk-8u171-linux-x64.tar.gz<br>3、配置环境变量<br>        在linux中环境的变量的文件是&#x2F;etc&#x2F;profile<br>4、打开并编辑环境变量文件<br>        vim &#x2F;etc&#x2F;profile<br>5、按下i进入编辑模式（注意：不要修改文件原本的内容，我们增加环境变量只需要额外增加即可，不要动原本的内容！！！！！！）<br>        export JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;soft&#x2F;jdk1.8.0_171<br>        export PATH&#x3D;.:$PATH:$JAVA_HOME&#x2F;bin<br>6、保存退出<br>7、在linux中环境变量修改完后需要使用命令让其生效<br>        source &#x2F;etc&#x2F;profile<br><strong>可能会出现的错误：</strong><br>1、发现交换文件 “&#x2F;etc&#x2F;.profile.swp”<br>            所有者: root    日期: Sat Apr 23 15:35:57 2022<br>            文件名: &#x2F;etc&#x2F;profile<br>            修改过: 是<br>            用户名: root      主机名: master<br>           进程 ID: 61454<br>正在打开文件 “&#x2F;etc&#x2F;profile”<br>              日期: Sat Apr 23 15:2<br>这是由于上一次打开的文件没有正确关闭导致的，需要删除交换文件： rm -rf &#x2F;etc&#x2F;.profile.swp</p></blockquote><h5 id="修改主机名（centOS7）"><a href="#修改主机名（centOS7）" class="headerlink" title="修改主机名（centOS7）"></a>修改主机名（centOS7）</h5><blockquote><p>vim &#x2F;etc&#x2F;hostname<br>修改后需要重启虚拟机<br>使用命令重启：reboot</p></blockquote><h5 id="centOS7关闭防火墙"><a href="#centOS7关闭防火墙" class="headerlink" title="centOS7关闭防火墙"></a>centOS7关闭防火墙</h5><blockquote><p>systemctl stop firewalld.service #停止firewall<br>systemctl disable firewalld.service #禁止firewall开机启动</p></blockquote><h5 id="查看防火墙状态"><a href="#查看防火墙状态" class="headerlink" title="查看防火墙状态"></a>查看防火墙状态</h5><blockquote><p>firewall-cmd –state<br>systemctl status firewalld.service</p></blockquote><h5 id="启动防火墙"><a href="#启动防火墙" class="headerlink" title="启动防火墙"></a>启动防火墙</h5><blockquote><p>systemctl start firewalld.service</p></blockquote><h5 id="关闭networkmanage服务（centOS7内置一个网络服务）"><a href="#关闭networkmanage服务（centOS7内置一个网络服务）" class="headerlink" title="关闭networkmanage服务（centOS7内置一个网络服务）"></a>关闭networkmanage服务（centOS7内置一个网络服务）</h5><blockquote><p>systemctl status NetworkManager    #查看NetworkManager状态<br>systemctl stop NetworkManager    #停止NetworkManager<br>systemctl disable NetworkManager    #禁止NetworkManager开机启动</p></blockquote><h3 id="克隆虚拟机"><a href="#克隆虚拟机" class="headerlink" title="克隆虚拟机"></a>克隆虚拟机</h3><blockquote><p>1、克隆之前不要开启被克隆的虚拟机<br>2、注意：克隆过程中需要选择完整克隆！！！<br>3、克隆完后不要立即启动，因为mac地址和被克隆的虚拟机一模一样，需要修改克隆后虚拟机的mac地址<br>4、需要修改克隆后虚拟机的主机名和ip地址，先不要启动被克隆的虚拟机<br>5、修改主机名：<br>        cd &#x2F;etc<br>        vim hostname<br>        修改后重启生效<br>6、修改ip地址<br>        cd &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts<br>        vim ifcfg-ens33<br>7、重启 reboot</p></blockquote><h5 id="基础命令进阶："><a href="#基础命令进阶：" class="headerlink" title="基础命令进阶："></a>基础命令进阶：</h5><h6 id="1、远程复制命令："><a href="#1、远程复制命令：" class="headerlink" title="1、远程复制命令："></a>1、远程复制命令：</h6><p>​        远程复制文件：scp test.txt 192.168.40.130:&#x2F;usr&#x2F;local&#x2F;soft&#x2F;<br>​        远程复制文件夹：scp -r aaaa 192.168.40.120:&#x2F;usr&#x2F;local&#x2F;soft&#x2F;</p><h6 id="2、配置ip映射"><a href="#2、配置ip映射" class="headerlink" title="2、配置ip映射"></a>2、配置ip映射</h6><p>​    2.1、vim &#x2F;etc&#x2F;hosts</p><p>​        添加如下内容：</p><p>​            192.168.40.110  master<br>​            192.168.40.120  node1<br>​            192.168.40.130  node2</p><p>​    2.2、远程复制到其他节点上，覆盖原来的hosts文件，每台虚拟机都要进行覆盖<br>​        scp &#x2F;etc&#x2F;hosts node1:&#x2F;etc&#x2F;hosts<br>​        scp &#x2F;etc&#x2F;hosts node2:&#x2F;etc&#x2F;hosts</p><p>​    2.3、覆盖完后，尝试ping其他虚拟机</p><h6 id="3、配置免密操作（每个虚拟机都要进行操作）"><a href="#3、配置免密操作（每个虚拟机都要进行操作）" class="headerlink" title="3、配置免密操作（每个虚拟机都要进行操作）"></a>3、配置免密操作（每个虚拟机都要进行操作）</h6><p>​     <strong>在任意目录都可以执行</strong><br>​        3.1 ssh-keygen -t rsa 然后三次回车<br>​        3.2  ssh-copy-id -i 主机名<br>​            注意：生成密钥的虚拟机复制密钥的时候，自己也要复制一份<br>​            举例：ssh-copy-id -i master<br>​                        ssh-copy-id -i node1<br>​                        ssh-copy-id -i node2</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构</title>
      <link href="/2022/04/19/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
      <url>/2022/04/19/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h2 id="栈和队列"><a href="#栈和队列" class="headerlink" title="栈和队列"></a>栈和队列</h2><p><img src="https://s2.loli.net/2022/04/06/asuo1fbJLt4mIRZ.png" alt="image-20220406205727748"></p><h2 id="数组和链表"><a href="#数组和链表" class="headerlink" title="数组和链表"></a>数组和链表</h2><p><img src="https://s2.loli.net/2022/04/06/oWvhCb9R1UucIVO.png" alt="image-20220406205910981"></p><h2 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h2><p><img src="https://s2.loli.net/2022/04/06/lQZNqR4ijroWKFc.png" alt="image-20220406210017338"></p><h2 id="图"><a href="#图" class="headerlink" title="图"></a>图</h2><p><img src="https://s2.loli.net/2022/04/06/1FmKhpXzNIdqu8x.png" alt="image-20220406210122779"></p><h2 id="树"><a href="#树" class="headerlink" title="树"></a>树</h2><p><img src="https://s2.loli.net/2022/04/06/j1lcME2yN3axgSG.png" alt="image-20220406210221158"></p><blockquote><p>给定N个权值作为N个<a href="https://baike.baidu.com/item/%E5%8F%B6%E5%AD%90%E7%BB%93%E7%82%B9/3620239">叶子结点</a>，构造一棵二叉树，若该树的带权路径长度达到最小，称这样的二叉树为最优二叉树，也称为哈夫曼树(Huffman Tree)</p></blockquote><blockquote><p>红黑树（Red Black Tree） 是一种自平衡二叉查找树</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>反射</title>
      <link href="/2022/04/17/%E5%8F%8D%E5%B0%84/"/>
      <url>/2022/04/17/%E5%8F%8D%E5%B0%84/</url>
      
        <content type="html"><![CDATA[<h3 id="类的加载"><a href="#类的加载" class="headerlink" title="类的加载"></a>类的加载</h3><blockquote><p>•当程序要使用某个类时，如果该类还未被加载到内存中，则系统会通过加载，连接，初始化三步来实现对这个类进行初始化。</p><h4 id="•加载"><a href="#•加载" class="headerlink" title="•加载"></a>•加载</h4><p>•就是指将class文件读入内存，并为之创建一个Class对象。</p><p>•任何类被使用时系统都会建立一个Class对象。</p><h4 id="•连接"><a href="#•连接" class="headerlink" title="•连接"></a>•连接</h4><p>•验证 是否有正确的内部结构，并和其他类协调一致</p><p>•准备 负责为类的静态成员分配内存，并设置默认初始化值</p><p>•解析 将类的二进制数据中的符号引用替换为直接引用</p><h4 id="•初始化"><a href="#•初始化" class="headerlink" title="•初始化"></a>•初始化</h4><p>就是我们以前讲过的初始化步骤</p></blockquote><h3 id="类加载器"><a href="#类加载器" class="headerlink" title="类加载器"></a>类加载器</h3><h4 id="类加载器的组成"><a href="#类加载器的组成" class="headerlink" title="类加载器的组成"></a>类加载器的组成</h4><h5 id="•Bootstrap-ClassLoader-根类加载器"><a href="#•Bootstrap-ClassLoader-根类加载器" class="headerlink" title="•Bootstrap ClassLoader 根类加载器"></a>•Bootstrap ClassLoader 根类加载器</h5><blockquote><p>也被称为引导类加载器，负责Java核心类的加载</p><p>比如System,String等。在JDK中JRE的lib目录下rt.jar文件中</p></blockquote><h5 id="•Extension-ClassLoader-扩展类加载器"><a href="#•Extension-ClassLoader-扩展类加载器" class="headerlink" title="•Extension ClassLoader 扩展类加载器"></a>•Extension ClassLoader 扩展类加载器</h5><blockquote><p>负责JRE的扩展目录中jar包的加载。</p><p>在JDK中JRE的lib目录下ext目录</p></blockquote><h5 id="•Sysetm-ClassLoader-系统类加载器"><a href="#•Sysetm-ClassLoader-系统类加载器" class="headerlink" title="•Sysetm ClassLoader 系统类加载器"></a>•Sysetm ClassLoader 系统类加载器</h5><blockquote><p>负责在JVM启动时加载来自java命令的class文件，以及classpath环境变量所指定的jar包和类路径</p></blockquote><h3 id="反射"><a href="#反射" class="headerlink" title="反射"></a>反射</h3><blockquote><p>JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为java语言的反射机制。</p></blockquote><h4 id="通过反射获取构造方法并使用"><a href="#通过反射获取构造方法并使用" class="headerlink" title="通过反射获取构造方法并使用"></a>通过反射获取构造方法并使用</h4><blockquote><h5 id="获取构造方法"><a href="#获取构造方法" class="headerlink" title="获取构造方法"></a>获取构造方法</h5><p>getConstructors</p><p>getDeclaredConstructors</p><h5 id="创建对象"><a href="#创建对象" class="headerlink" title="创建对象"></a>创建对象</h5><p>newInstance()</p><p>con.newInstance(“zhangsan”, 20);</p></blockquote><h4 id="通过反射获取成员变量并使用"><a href="#通过反射获取成员变量并使用" class="headerlink" title="通过反射获取成员变量并使用"></a>通过反射获取成员变量并使用</h4><blockquote><h5 id="获取所有成员"><a href="#获取所有成员" class="headerlink" title="获取所有成员"></a>获取所有成员</h5><p>getFields,getDeclaredFields</p><h5 id="获取单个成员"><a href="#获取单个成员" class="headerlink" title="获取单个成员"></a>获取单个成员</h5><p>getField,getDeclaredField</p><h5 id="修改成员的值"><a href="#修改成员的值" class="headerlink" title="修改成员的值"></a>修改成员的值</h5><p>set(Object obj,Object value)<br>将指定对象变量上此 Field 对象表示的字段设置为指定的新值。</p></blockquote><h4 id="通过反射获取成员方法并使用"><a href="#通过反射获取成员方法并使用" class="headerlink" title="通过反射获取成员方法并使用"></a>通过反射获取成员方法并使用</h4><blockquote><h5 id="获取所有方法"><a href="#获取所有方法" class="headerlink" title="获取所有方法"></a>获取所有方法</h5><p>getMethods</p><p>getDeclaredMethods</p><h5 id="获取单个方法"><a href="#获取单个方法" class="headerlink" title="获取单个方法"></a>获取单个方法</h5><p>getMethod</p><p>getDeclaredMethod</p><h5 id="暴力访问"><a href="#暴力访问" class="headerlink" title="暴力访问"></a>暴力访问</h5><p>method.setAccessible(true);</p></blockquote><h3 id="动态代理"><a href="#动态代理" class="headerlink" title="动态代理"></a>动态代理</h3><blockquote><p>在程序运行过程中产生的这个对象,而程序运行过程中产生对象其实就是我们刚才反射讲解的内容，所以，动态代理其实就是通过反射来生成一个代理</p></blockquote><p>在Java中<strong>java.lang.reflect</strong>包下提供了一个Proxy类和一个<strong>InvocationHandler</strong>接口，通过使用这个类和接口就可以生成动态代理对象。JDK提供的代理只能针对接口做代理。我们有更强大的代理cglib</p><p>Proxy类中的方法创建动态代理类对象</p><p>•public static Object newProxyInstance(ClassLoader loader,Class&lt;?&gt;[] interfaces,InvocationHandler h)</p><p>•最终会调用InvocationHandler的方法</p><p>InvocationHandler</p><p>Object invoke(Object proxy,Method method,Object[] args)</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多线程</title>
      <link href="/2022/04/16/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
      <url>/2022/04/16/%E5%A4%9A%E7%BA%BF%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h3 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h3><h4 id="进程："><a href="#进程：" class="headerlink" title="进程："></a>进程：</h4><blockquote><p>是指正在运行的程序，是系统进行资源分配和调用的独立单位，每一个进程都有它自己的内存空间和资源，在windows下通过任务管理器查看</p></blockquote><h4 id="线程："><a href="#线程：" class="headerlink" title="线程："></a>线程：</h4><blockquote><p>是进程的单个顺序控制流，或者就是说是一个单独执行的路径<br>如果一个进程只有一条执行路径，称之为单线程<br>如果一个进程有多条执行路径，称之为多线程<br><strong>线程</strong>包含在<strong>进程</strong>中</p></blockquote><h4 id="串行"><a href="#串行" class="headerlink" title="串行"></a>串行</h4><blockquote><p>指一个程序中所有的任务都是按照先后顺序执行的，在前一个任务还没有处理完的情况下，是不会进行处理下一个任务的。<br>举例：理发店只有一个理发师，很多人去理发，就需要排队，就有先后顺序，先等前面的人理完发，再轮到后面的人。</p></blockquote><h4 id="并行"><a href="#并行" class="headerlink" title="并行"></a>并行</h4><blockquote><p>指将任务分给不同的处理器去处理，每一个处理器中的任务再进行串行处理<br>举例：火车站上有很多卖票窗口，多个窗口同时卖票，但是呢，针对于某一个窗口来说，是一个接着一个去处理的。</p></blockquote><h4 id="并发"><a href="#并发" class="headerlink" title="并发"></a>并发</h4><blockquote><p>指一个现象，并发需要处理器的支持，比如在处理一个任务的时候，操作系统可以调用资源去处理其他的任务，这个任务并行还是串行都可以。<br>无论是串行还是并行，都需要处理支持并发。<br>举例：假设喝水是一个任务，每个火车站售票员，他再售票的同时也能喝水，这就表示支持并发。</p></blockquote><h5 id="在启动JVM的时候，最低的要求是需要启动两个线程，其中包含了主线程，垃圾回收线程所以JVM启动的时候是多线程程序。"><a href="#在启动JVM的时候，最低的要求是需要启动两个线程，其中包含了主线程，垃圾回收线程所以JVM启动的时候是多线程程序。" class="headerlink" title="在启动JVM的时候，最低的要求是需要启动两个线程，其中包含了主线程，垃圾回收线程所以JVM启动的时候是多线程程序。"></a><strong>在启动JVM的时候，最低的要求是需要启动两个线程，其中包含了主线程，垃圾回收线程所以JVM启动的时候是多线程程序。</strong></h5><h4 id="java中实现线程的方式"><a href="#java中实现线程的方式" class="headerlink" title="java中实现线程的方式"></a>java中实现线程的方式</h4><blockquote><p>有三种</p></blockquote><blockquote><p>第一种，继承Thread类，重写run方法，使用start启动线程；（thread默认是从0开始）<br>第一种，实现Runnable接口，实现run方法，借助Thread类创建线程对象，使用start方法启动；<br>第三种，实现Callable接口，实现call方法，需要结合线程池的方式创建线程对象，提交到线程池执行。（thread默认是从1开始）</p></blockquote><h5 id="start-启动线程"><a href="#start-启动线程" class="headerlink" title="start() 启动线程"></a>start() 启动线程</h5><h5 id="setName-给线程命名，还有一种方法是在创建线程对象的同时给线程命名（需要自己添加有参和无参构造方法）"><a href="#setName-给线程命名，还有一种方法是在创建线程对象的同时给线程命名（需要自己添加有参和无参构造方法）" class="headerlink" title="setName() 给线程命名，还有一种方法是在创建线程对象的同时给线程命名（需要自己添加有参和无参构造方法）"></a>setName() 给线程命名，还有一种方法是在创建线程对象的同时给线程命名（需要自己添加有参和无参构造方法）</h5><h5 id="getName-获取线程名字"><a href="#getName-获取线程名字" class="headerlink" title="getName() 获取线程名字"></a>getName() 获取线程名字</h5><h4 id="线程调度"><a href="#线程调度" class="headerlink" title="线程调度"></a>线程调度</h4><h5 id="线程有两种调度模型："><a href="#线程有两种调度模型：" class="headerlink" title="线程有两种调度模型："></a>线程有两种调度模型：</h5><blockquote><p>1、分时调度模型：所有线程轮流使用 CPU 的使用权，平均分配每个线程占用 CPU 的时间片<br>2、抢占式调度模型：优先让优先级高的线程使用 CPU，如果线程的优先级相同，那么会随机选择一个，优先级高的线程获取的 CPU 时间片相对多一些。</p></blockquote><h5 id="Java使用的是抢占式调度模型"><a href="#Java使用的是抢占式调度模型" class="headerlink" title="Java使用的是抢占式调度模型"></a>Java使用的是抢占式调度模型</h5><blockquote><p>1、线程的默认优先级是5<br>2、设置优先级的时候，范围是1-10<br>3、线程的优先级越高仅仅表示的是获取CPU时间片的机率会高一些，并不能保证一定会先执行。</p></blockquote><h5 id="获取线程中的优先级方法：public-final-int-getPriority-返回此线程的优先级。"><a href="#获取线程中的优先级方法：public-final-int-getPriority-返回此线程的优先级。" class="headerlink" title="获取线程中的优先级方法：public final int getPriority() 返回此线程的优先级。"></a>获取线程中的优先级方法：public final int getPriority() 返回此线程的优先级。</h5><p>设置线程优先级的方法：public final void setPriority(int newPriority) 更改此线程的优先级。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">int</span> <span class="variable">MIN_PRIORITY</span> <span class="operator">=</span> <span class="number">1</span>; 线程可以拥有的最小的优先级</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">int</span> <span class="variable">MAX_PRIORITY</span> <span class="operator">=</span> <span class="number">10</span>; 线程可以拥有的最大的优先级</span><br></pre></td></tr></table></figure><h4 id="线程控制"><a href="#线程控制" class="headerlink" title="线程控制"></a>线程控制</h4><h5 id="线程休眠"><a href="#线程休眠" class="headerlink" title="线程休眠"></a>线程休眠</h5><blockquote><p>public static void sleep(long millis)    休眠线程（睡眠线程）</p></blockquote><h5 id="线程加入"><a href="#线程加入" class="headerlink" title="线程加入"></a>线程加入</h5><blockquote><p>public final void join()    加入线程<br>线程对象调用该方法的时候，目的是让调用该方法的当前线程先执行完，执行完毕后，再让其他线程执行，其他没有调用join方法的线程，他们之间还是会抢CPU执行权的。</p></blockquote><p><strong>注意：join方法的调用，必须是紧跟着当前线程start()方法后调用，否则不起作用。</strong></p><h5 id="线程礼让"><a href="#线程礼让" class="headerlink" title="线程礼让"></a>线程礼让</h5><blockquote><p>public final void yield()    礼让线程</p></blockquote><blockquote><p>礼让线程的目的是暂停当前正在执行的线程，并让其他线程执行，它的作用实际上是为了让线程之间看起来更加和谐，它并不能保证多个线程之间一人一次。</p></blockquote><h5 id="后台线程"><a href="#后台线程" class="headerlink" title="后台线程"></a>后台线程</h5><blockquote><p>public final void setDaemon(boolean on)    后台线程(守护线程)</p><p><strong>Java中有两类：用户线程，守护线程</strong><br><strong>用户线程</strong>：在学习线程之前，运行起来的一个一个程序中的线程都是用户线程<br><strong>守护线程</strong>：所谓的守护线程，指的是程序运行的时候，在后台提供了一个通用的服务线程，比如说垃圾回收线程，就是一个守护线程。这种线程不一定是要存在的，但是可能程序会出问题。只要程序存在用户线程，程序就不会停止.</p><p><strong>守护线程进行设置</strong><br>   public final void setDaemon(boolean on)</p><p>注意事项：</p><p>​    <strong>守护线程必须在启动之前进行设置</strong></p></blockquote><h5 id="中断线程"><a href="#中断线程" class="headerlink" title="中断线程"></a>中断线程</h5><blockquote><p>public final void stop()    已弃用<br>public void interrupt()    打断线程</p></blockquote><h5 id="休眠线程"><a href="#休眠线程" class="headerlink" title="休眠线程"></a>休眠线程</h5><blockquote><p>public static void sleep(long millis)    休眠线程（睡眠线程）</p></blockquote><h4 id="线程的生命周期图"><a href="#线程的生命周期图" class="headerlink" title="线程的生命周期图"></a>线程的生命周期图</h4><p><img src="https://s2.loli.net/2022/04/11/SYz8thrRo27uw6N.png" alt="image-20220411225244933"></p><h4 id="同步代码块"><a href="#同步代码块" class="headerlink" title="同步代码块"></a>同步代码块</h4><p>•格式：</p><p><strong>synchronized</strong>(对象){</p><p>​        需要同步的代码;</p><p>}</p><blockquote><p>同步可以解决安全问题的根本原因就在那个对象上。该对象如同锁的功能。</p></blockquote><h5 id="同步的前提"><a href="#同步的前提" class="headerlink" title="同步的前提"></a>同步的前提</h5><blockquote><p>•多个线程</p></blockquote><blockquote><p>•多个线程使用的是同一个锁对象</p></blockquote><h5 id="同步的好处"><a href="#同步的好处" class="headerlink" title="同步的好处"></a>同步的好处</h5><blockquote><p>•同步的出现解决了多线程的安全问题。</p></blockquote><h5 id="同步的弊端"><a href="#同步的弊端" class="headerlink" title="同步的弊端"></a>同步的弊端</h5><blockquote><p>•当线程相当多时，因为每个线程都会去判断同步上的锁，这是很耗费资源的，无形中会降低程序的运行效率。</p></blockquote><h5 id="同步弊端"><a href="#同步弊端" class="headerlink" title="同步弊端"></a>同步弊端</h5><blockquote><p>•效率低</p></blockquote><blockquote><p>•如果出现了同步嵌套，就容易产生死锁问题</p></blockquote><h5 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h5><blockquote><p>•是指两个或者两个以上的线程在执行的过程中，因争夺资源产生的一种互相等待现象</p></blockquote><h4 id="线程的状态转换图"><a href="#线程的状态转换图" class="headerlink" title="线程的状态转换图"></a>线程的状态转换图</h4><p><img src="https://s2.loli.net/2022/04/18/CX7pwtn3ZgamueV.png" alt="屏幕截图 2022-04-18 194219"></p><h4 id="线程组"><a href="#线程组" class="headerlink" title="线程组"></a><strong>线程组</strong></h4><blockquote><p>Java中使用<strong>ThreadGroup</strong>来表示线程组，它可以对一批线程进行分类管理，Java允许程序直接对线程组进行控制。</p><p>•默认情况下，所有的线程都属于主线程组。</p><p>•<strong>public final ThreadGroup getThreadGroup()</strong></p><p>•我们也可以给线程设置分组</p><p>•<strong>Thread(ThreadGroup group, Runnable target, String name)</strong> </p></blockquote><h4 id="线程池"><a href="#线程池" class="headerlink" title="线程池"></a>线程池</h4><blockquote><p>•线程池里的每一个线程代码结束后，并不会死亡，而是再次回到线程池中成为空闲状态，等待下一个对象来使用。</p></blockquote><p>JDK5新增了一个Executors工厂类来产生线程池，有如下几个方法</p><p>•public static ExecutorService newCachedThreadPool()</p><p>•public static ExecutorService newFixedThreadPool(int nThreads)</p><p>•public static ExecutorService newSingleThreadExecutor()</p><p>•这些方法的返回值是ExecutorService对象，该对象表示一个线程池，可以执行Runnable对象或者Callable对象代表的线程。它提供了如下方法</p><p>•Future&lt;?&gt; submit(Runnable task)</p><p>•<T> Future<T> submit(Callable<T> task)</p><p>•案例演示</p><p>•创建线程池对象</p><p>•创建Runnable实例</p><p>•提交Runnable实例</p><p>•关闭线程池</p><h4 id="匿名内部类方式使用多线程"><a href="#匿名内部类方式使用多线程" class="headerlink" title="匿名内部类方式使用多线程"></a><strong>匿名内部类方式使用多线程</strong></h4><blockquote><p>匿名内部类方式使用多线程</p><p>new Thread(){代码…}.start();</p><p>New Thread(new Runnable(){代码…}).start();</p></blockquote><h4 id="定时器"><a href="#定时器" class="headerlink" title="定时器"></a>定时器</h4><blockquote><p>在java中可以通过Timer和TimerTask类来实现定义调度的功能</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Timer;</span><br><span class="line"><span class="keyword">import</span> java.util.TimerTask;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TimerDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">//创建定时对象</span></span><br><span class="line">        <span class="type">Timer</span> <span class="variable">timer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Timer</span>();</span><br><span class="line">        timer.schedule(<span class="keyword">new</span> <span class="title class_">MyTask</span>(timer),<span class="number">2000</span>,<span class="number">3000</span>);<span class="comment">//2秒后开始打印，并且每隔3秒执行一次打印</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MyTask</span> <span class="keyword">extends</span> <span class="title class_">TimerTask</span> &#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">final</span> Timer timer;</span><br><span class="line">        MyTask(Timer timer)&#123;</span><br><span class="line">            <span class="built_in">this</span>.timer=timer;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;哈哈哈&quot;</span>);</span><br><span class="line"><span class="comment">//            timer.cancel();</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>AzKaBan是一个完全由java编写的开源调度框架</p><h4 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h4><blockquote><p>1、创建型    创建对象</p><p>2、结构型    对象的组成</p><p>3、行为型    对象的功能</p></blockquote><h5 id="创建型模式"><a href="#创建型模式" class="headerlink" title="创建型模式"></a>创建型模式</h5><p><strong>1、单例模式</strong>    指的是在程序运行过程中，内存只允许有一个对象存在</p><blockquote><p>保障类在内存中只有一个对象</p><p>1、构造方法私有化</p><p>2、在类的内部成员变量位置上创建一个对象</p><p>3、提供公共的方法将唯一的对象返回出来获取</p></blockquote><p>​    饿汉式：类一加载，对象就创建好了（线程安全）</p><p>​    懒汉式：用的时候，再创建对象</p><p><strong>2、简单工厂模式</strong></p><p><strong>3、工厂方法模式</strong> </p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>递归</title>
      <link href="/2022/04/14/%E9%80%92%E5%BD%92/"/>
      <url>/2022/04/14/%E9%80%92%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<h4 id="递归"><a href="#递归" class="headerlink" title="递归"></a>递归</h4><h5 id="方法定义中调用方法本身的现象"><a href="#方法定义中调用方法本身的现象" class="headerlink" title="方法定义中调用方法本身的现象"></a>方法定义中调用方法本身的现象</h5><p> 递归的注意事项：<br>        1、递归一定要有一个出口，结束条件，否则就是死递归<br>        2、递归的次数不能太多，否则就会造成栈内存溢出<br>        3、构造方法不能初始化</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span> <span class="title function_">jieCheng</span><span class="params">(<span class="type">int</span> i)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (i == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">//5 * jieCheng(4)</span></span><br><span class="line">            <span class="comment">//5 * 4 * jieCheng(3)</span></span><br><span class="line">            <span class="comment">//5 * 4 * 3 * jieCheng(2)</span></span><br><span class="line">            <span class="comment">//5 * 4 * 3 * 2 * jieCheng(1)</span></span><br><span class="line">            <span class="keyword">return</span> i * jieCheng(i - <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IO流</title>
      <link href="/2022/04/13/Io%E6%B5%81/"/>
      <url>/2022/04/13/Io%E6%B5%81/</url>
      
        <content type="html"><![CDATA[<h2 id="IO流"><a href="#IO流" class="headerlink" title="IO流"></a>IO流</h2><blockquote><p>按照流向：输入流、输出流</p></blockquote><h4 id="字节流"><a href="#字节流" class="headerlink" title="字节流"></a>字节流</h4><h5 id="字节输入流"><a href="#字节输入流" class="headerlink" title="字节输入流"></a>字节输入流</h5><p>InputStream（抽象父类）</p><p>FileInputStream（具体继承子类）</p><p>FileInputStream fis &#x3D; new FileInputStream(“文件名”)</p><blockquote><p>读取数据的两种方式：</p></blockquote><h6 id="1、一次读取一个字节"><a href="#1、一次读取一个字节" class="headerlink" title="1、一次读取一个字节"></a>1、一次读取一个字节</h6><p>int b &#x3D; 0；</p><p>while(b &#x3D; fis.read() !&#x3D; -1){</p><p>​        Systeam.out.println((char) b );</p><p>}</p><h6 id="2、一次读取一个字节数组"><a href="#2、一次读取一个字节数组" class="headerlink" title="2、一次读取一个字节数组"></a>2、一次读取一个字节数组</h6><p>byte[] bytes &#x3D; new byte[1024];</p><p>int   length &#x3D; 0;</p><p>while((length &#x3D; fis.read(bytes)) !&#x3D; -1){</p><p>​        String s &#x3D; new String(bytes,0,length);</p><p>​        Systeam.out.print(s);</p><p>}</p><h5 id="字节缓冲输入流"><a href="#字节缓冲输入流" class="headerlink" title="字节缓冲输入流"></a>字节缓冲输入流</h5><p>BufferedInputStream</p><p>BufferedInputStteam bis &#x3D; new BufferedInputStream(new FileInputStream(“文件名”))；</p><blockquote><p>读取数据的两种方式：</p></blockquote><h6 id="1、一次读取一个字节-1"><a href="#1、一次读取一个字节-1" class="headerlink" title="1、一次读取一个字节"></a>1、一次读取一个字节</h6><p>​        int b &#x3D; 0;<br>​        while((b&#x3D;bis.read()) !&#x3D; -1){<br>​                System.out.print((char)b);<br>​        }</p><h6 id="2、一次读取一个字节数组-1"><a href="#2、一次读取一个字节数组-1" class="headerlink" title="2、一次读取一个字节数组"></a>2、一次读取一个字节数组</h6><p>​        byte[] bytes &#x3D; new byte[1024];<br>​        int length &#x3D; 0;<br>​        while((length &#x3D; bis.read(bytes)) !&#x3D; -1){<br>​                String s &#x3D; new String(bytes,0,length);<br>​                System.out.print(s);</p><p>}</p><h5 id="字节输出流"><a href="#字节输出流" class="headerlink" title="字节输出流"></a>字节输出流</h5><p>InputStream（抽象父类）</p><p>FileInputStream（具体继承子类）</p><p>如果目标文件不存在，会自动创建</p><p>FileInputStream fis &#x3D; new FileInputStream(“文件名”)</p><blockquote><p>1、一次写一个int类型的数据<br>    fos.write(97);<br>2、一次写一个字节数组<br>    fos.write(bytes);<br>3、一次写一个字节数组的一部分<br>    fos.write(bytes,1,3);</p></blockquote><h5 id="字节缓冲输出流"><a href="#字节缓冲输出流" class="headerlink" title="字节缓冲输出流"></a>字节缓冲输出流</h5><p>BufferedOutputStream</p><p>BufferedOutputStream bos &#x3D; new BufferedOutputStream(new FileOutputStream(“..”));</p><blockquote><p>1、一次写一个int类型的数据<br>    bos.write(97);<br>2、一次写一个字节数组<br>    bos.write(bytes);<br>3、一次写一个字节数组的一部分<br>    bos.write(bytes,1,3);</p></blockquote><h4 id="字符流-x3D-字节流-编码表"><a href="#字符流-x3D-字节流-编码表" class="headerlink" title="字符流 &#x3D; 字节流 + 编码表"></a>字符流 &#x3D; 字节流 + 编码表</h4><h5 id="字符输入流"><a href="#字符输入流" class="headerlink" title="字符输入流"></a>字符输入流</h5><p>Reader – InputStreamReader</p><p>InputStreamReader isr &#x3D; new InputStreamReader(new FileInputStream(“…”));</p><h6 id="1、一次读取一个字符"><a href="#1、一次读取一个字符" class="headerlink" title="1、一次读取一个字符"></a>1、一次读取一个字符</h6><p>​    int ch &#x3D; 0;<br>​    while((ch&#x3D;isr.read())!&#x3D;-1){<br>​            System.out.print((char)ch); &#x2F;&#x2F;这里不会出现乱码<br>​    }</p><h6 id="2、一次读取一个字符数组"><a href="#2、一次读取一个字符数组" class="headerlink" title="2、一次读取一个字符数组"></a>2、一次读取一个字符数组</h6><p>​    char[] chars &#x3D; new char[1024];</p><p>​    int length &#x3D; 0;</p><p>​    while((length&#x3D;isr.read(chars))!&#x3D;-1){<br>​            String s &#x3D; new String(chars,0,length);<br>​            System.out.print(s);</p><p>}</p><h6 id="简化写法："><a href="#简化写法：" class="headerlink" title="简化写法："></a>简化写法：</h6><p>fileReader</p><p>fileReader fr &#x3D; new FileReader(“…”);<br>1、一次读取一个字符<br>              int ch &#x3D; 0;<br>              while((ch&#x3D;fr.read())!&#x3D;-1){<br>                 System.out.print((char)ch); &#x2F;&#x2F;这里不会出现乱码<br>              }</p><p> 2）一次读取一个字符数组<br>    char[] chars &#x3D; new char[1024];<br>    int length &#x3D; 0;<br>    while((length&#x3D;fr.read(chars))!&#x3D;-1){<br>            String s &#x3D; new String(chars,0,length);</p><p>​        System.out.print(s);</p><p>}</p><h5 id="字符缓冲输入流"><a href="#字符缓冲输入流" class="headerlink" title="字符缓冲输入流"></a>字符缓冲输入流</h5><p>BufferedReader</p><p>BufferedReader br &#x3D; new BufferedReader(new OutputStreamReader(new FileOutputStream(“文件”)))</p><p> &#x2F;&#x2F;简化写法改进：<br>BufferedReader br &#x3D; new BufferedReader(new FileReader(“…”));<br>1、一次读取一个字符<br>    int ch &#x3D; 0;<br>    while((ch&#x3D;br.read())!&#x3D;-1){<br>            System.out.print((char)ch); &#x2F;&#x2F;这里不会出现乱码<br>    }</p><p>2、一次读取一个字符数组<br>    char[] chars &#x3D; new char[1024];<br>    int length &#x3D; 0;<br>    while((length&#x3D;br.read(chars))!&#x3D;-1){<br>            String s &#x3D; new String(chars,0,length);<br>            System.out.print(s);<br>    }</p><p>3、使用特殊方法一次读取一行</p><p>​    String line &#x3D; null;<br>​    while((line &#x3D; br.readLine())!&#x3D;null){<br>​            System.out.print(line);</p><p>​    }</p><h5 id="字符输出流"><a href="#字符输出流" class="headerlink" title="字符输出流"></a>字符输出流</h5><p>Writer –OutputStreamWriter</p><p>OutputStreamWriter osw &#x3D; new OutputStreamWriter(new FileOutputStream(“文件”));</p><p>简化写法：FileWriter<br>FileWriter fw &#x3D; new FileWriter(“…”);</p><h5 id="字符缓冲输出流"><a href="#字符缓冲输出流" class="headerlink" title="字符缓冲输出流"></a>字符缓冲输出流</h5><p>BufferedWriter</p><p>BufferedWriter bw &#x3D; new BufferedWriter(new OutputStreamWriter(new FileOutputStream(“文件”)));<br>简化写法改进：BufferedWriter bw &#x3D; new BufferedWriter(new FileWriter(“文件”));</p><p>特殊的方法：写换行符：newLine();</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>File类</title>
      <link href="/2022/04/08/File%E7%B1%BB/"/>
      <url>/2022/04/08/File%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h4 id="File类的构造方法"><a href="#File类的构造方法" class="headerlink" title="File类的构造方法"></a>File类的构造方法</h4><blockquote><p>•public File(String pathname)</p><p>•public File(String parent,String child)</p><p>•public File(File parent,String child)</p></blockquote><h4 id="File类的成员方法"><a href="#File类的成员方法" class="headerlink" title="File类的成员方法"></a>File类的成员方法</h4><h5 id="创建功能"><a href="#创建功能" class="headerlink" title="创建功能"></a>创建功能</h5><p>•public boolean createNewFile()</p><p>•public boolean mkdir()</p><p>•public boolean mkdirs()</p><h5 id="删除功能"><a href="#删除功能" class="headerlink" title="删除功能"></a>删除功能</h5><p>•public boolean delete()</p><h5 id="重命名功能"><a href="#重命名功能" class="headerlink" title="重命名功能"></a>重命名功能</h5><p>•public boolean renameTo(File dest)</p><h5 id="判断功能"><a href="#判断功能" class="headerlink" title="判断功能"></a>判断功能</h5><p>•public boolean isDirectory()    判断是不是文件夹</p><p>•public boolean isFile()    判断是不是文件</p><p>•public boolean exists()    判断是否存在</p><p>•public boolean canRead()    判断是否可读</p><p>•public boolean canWrite()    判断是否可写</p><p>•public boolean isHidden()    判断是否隐藏</p><h5 id="基本获取功能"><a href="#基本获取功能" class="headerlink" title="基本获取功能"></a>基本获取功能</h5><p>•public String getAbsolutePath()    获取完整路径（绝对路径）</p><p>•public String getPath()    获取相对路径</p><p>•public String getName()    获取名称</p><p>•public long length()    获取文件或者文件夹的字节数</p><p>•public long lastModified()    获取文件最后一次修改时间，时间戳</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FileDemo1</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span>&#123;</span><br><span class="line">        <span class="type">File</span> <span class="variable">file</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">File</span>(<span class="string">&quot;测试文件&quot;</span>);</span><br><span class="line">        <span class="type">Date</span> <span class="variable">date</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Date</span>(file.lastModified());</span><br><span class="line">        <span class="type">SimpleDateFormat</span> <span class="variable">sdf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SimpleDateFormat</span>(<span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>);</span><br><span class="line">        <span class="type">String</span> <span class="variable">s</span> <span class="operator">=</span> sdf.format(date);</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="高级获取功能"><a href="#高级获取功能" class="headerlink" title="高级获取功能"></a>高级获取功能</h5><p>•public String[] list()    将指定目录下的所有文件和文件夹的名称组成一个数组</p><p>•public File[] listFiles()    将指定目录下的所有文件和文件夹返回File对象数组，使用增强for循环遍历</p><h5 id="文件名称过滤器"><a href="#文件名称过滤器" class="headerlink" title="文件名称过滤器"></a>文件名称过滤器</h5><p>•public String[] list(FilenameFilter filter)    将指定目录下文件的名称过滤后组成一个数组</p><p>•public File[] listFiles(FilenameFilter filter)    将指定目录下文件的名称过滤后组成一个file对象数组</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Collection集合</title>
      <link href="/2022/04/04/collection%E9%9B%86%E5%90%88/"/>
      <url>/2022/04/04/collection%E9%9B%86%E5%90%88/</url>
      
        <content type="html"><![CDATA[<h3 id="Collection-接口"><a href="#Collection-接口" class="headerlink" title="Collection(接口)"></a>Collection(接口)</h3><h4 id="–-List-允许元素重复，且有序（存储和取出顺序一致）-接口"><a href="#–-List-允许元素重复，且有序（存储和取出顺序一致）-接口" class="headerlink" title="– List:允许元素重复，且有序（存储和取出顺序一致）(接口)"></a>– List:允许元素重复，且有序（存储和取出顺序一致）(接口)</h4><h5 id="–-ArrayList"><a href="#–-ArrayList" class="headerlink" title="– ArrayList"></a>– ArrayList</h5><blockquote><p>底层数据结构是数组，查询快，增删慢<br>线程不安全，效率高</p></blockquote><h5 id="–-Vector"><a href="#–-Vector" class="headerlink" title="– Vector"></a>– Vector</h5><blockquote><p>底层数据结构是数组，查询快，增删慢<br>线程安全，效率低</p></blockquote><h5 id="–-LinkedList"><a href="#–-LinkedList" class="headerlink" title="– LinkedList"></a>– LinkedList</h5><blockquote><p>底层数据结构是双链表，查询慢，增删快<br>线程不安全，效率高</p></blockquote><h5 id="List相关集合特点："><a href="#List相关集合特点：" class="headerlink" title="List相关集合特点："></a>List相关集合特点：</h5><blockquote><p>1、存在索引下标的概念，可以通过get()方法，通过索引获取集合中的元素<br>2、存储的元素和取出的顺序一致</p></blockquote><h4 id="–-Set-元素唯一，且无序（存储和取出的顺序不能保证）-接口"><a href="#–-Set-元素唯一，且无序（存储和取出的顺序不能保证）-接口" class="headerlink" title="– Set:元素唯一，且无序（存储和取出的顺序不能保证）(接口)"></a>– Set:元素唯一，且无序（存储和取出的顺序不能保证）(接口)</h4><h5 id="–-HashSet"><a href="#–-HashSet" class="headerlink" title="– HashSet"></a>– HashSet</h5><blockquote><p>底层数据结构是哈希表（元素是以链表节点的形式存在）<br>哈希表保证了元素的唯一性<br>线程不安全，效率高</p></blockquote><h5 id="–-LinkedHashSet"><a href="#–-LinkedHashSet" class="headerlink" title="– LinkedHashSet"></a>– LinkedHashSet</h5><blockquote><p>底层数据结构是双链表和哈希表共同决定的<br>哈希表保证元素的唯一性<br>双链表保证元素的顺序</p></blockquote><h5 id="–-TreeSet"><a href="#–-TreeSet" class="headerlink" title="– TreeSet"></a>– TreeSet</h5><blockquote><p>底层数据结构是红黑树（自平衡二叉树）<br>提供了两种排序方案：<br>1）自然排序<br>要求元素类实现Comparable<T>接口，并重写compareTo()方法<br>2）比较器排序<br>实现起来有两种方式，第一种是写具体的子类实现Comparator<T>接口，并重写compare(..,..)方法<br>第二种是匿名内部类当作TreeSet构造方法参数传入。</p></blockquote><h4 id="注意：遍历Collection集合的方式："><a href="#注意：遍历Collection集合的方式：" class="headerlink" title="注意：遍历Collection集合的方式："></a>注意：遍历Collection集合的方式：</h4><blockquote><p>1、迭代器遍历<br>2、增强for循环<br>3、注意，其中我们说过的get()和size()方法使用普通for循环仅仅适用于List相关集合，因为只有List相关集合才存在索引下标。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>正则表达式</title>
      <link href="/2022/04/01/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
      <url>/2022/04/01/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><blockquote><p>需求：编写一个java程序检验qq号是否符合规范</p><p>​    1、必须全部都是数字</p><p>​    2、必须是5-10</p><p>​    3、不能以0开头</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shujia.wyh.day17;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">需求：编写一个java程序检验qq号是否符合规范</span></span><br><span class="line"><span class="comment">    1、必须是5-10</span></span><br><span class="line"><span class="comment">    2、不能以0开头</span></span><br><span class="line"><span class="comment">3、必须全部都是数字</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RegularDemo1</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">s</span> <span class="operator">=</span> <span class="string">&quot;1165872335&quot;</span>;</span><br><span class="line">        System.out.println(checkQQ(s));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//感受一下使用正则表达式来解决这个需求</span></span><br><span class="line">        System.out.println(checkQQ2(s));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">checkQQ2</span><span class="params">(String s)</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//正则表达式可以很容易地完成字符串地查找匹配替换等工作</span></span><br><span class="line">        <span class="comment">//正则表达式实现</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">regex</span> <span class="operator">=</span> <span class="string">&quot;[1-9][0-9]&#123;4,9&#125;&quot;</span>;</span><br><span class="line">        <span class="keyword">return</span> s.matches(regex);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">checkQQ</span><span class="params">(String s)</span>&#123;</span><br><span class="line">        <span class="type">boolean</span> <span class="variable">flag</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//必须是5-10位</span></span><br><span class="line">        <span class="keyword">if</span>(s.length()&gt;=<span class="number">5</span> &amp;&amp; s.length()&lt;=<span class="number">10</span>)&#123;</span><br><span class="line">            <span class="comment">//不能以0开头</span></span><br><span class="line">            <span class="keyword">if</span>(!s.startsWith(<span class="string">&quot;0&quot;</span>))&#123;</span><br><span class="line">                flag = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//将字符串转成字符数组</span></span><br><span class="line">                <span class="type">char</span>[] chars = s.toCharArray();</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;chars.length;i++)&#123;</span><br><span class="line">                    <span class="comment">//包装类Character类中有一个方法可以进行判断该字符是否是数字</span></span><br><span class="line">                    <span class="comment">//public static boolean isDigit(char ch)确定指定的字符是否是数字。</span></span><br><span class="line">                    <span class="keyword">if</span>(!Character.isDigit(chars[i]))&#123;</span><br><span class="line">                        flag = <span class="literal">false</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;长度不符合规范，不是qq号&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(flag)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>运行结果：</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">长度不符合规范，不是qq号</span><br><span class="line"><span class="literal">false</span></span><br><span class="line"><span class="literal">false</span></span><br></pre></td></tr></table></figure><h4 id="学习正则表达式-正则表达式本身是一个字符串"><a href="#学习正则表达式-正则表达式本身是一个字符串" class="headerlink" title="学习正则表达式(正则表达式本身是一个字符串)"></a>学习正则表达式(正则表达式本身是一个字符串)</h4><blockquote><p>学习正则表达式的目的：通过正则表达式进行处理字符串的复杂的查找&#x2F;替换&#x2F;匹配&#x2F;分割等工作。</p><p>正则表达式是一个独立于任何一门语言的技术，不依附于java,但是它可以在java中进行使用，也可以在python&#x2F;Js等语言中进行使用</p></blockquote><h3 id="正则表达式的概述"><a href="#正则表达式的概述" class="headerlink" title="正则表达式的概述"></a>正则表达式的概述</h3><blockquote><p>概念：使用单个字符串来描述或者匹配一系列符合某种语法规则的字符串</p><p>正则表达式的使用步骤：</p><p>​    1、使用大量的字符串来寻找规律使用正则语法来定义规则<br>​    2、使用这种规则区匹配新的字符串</p><p>​    3、匹配成功后的相应的操作</p></blockquote><p><a href="mailto:&#49;&#49;&#54;&#x35;&#x38;&#x37;&#x32;&#51;&#x33;&#x35;&#64;&#x71;&#113;&#46;&#x63;&#111;&#x6d;">&#49;&#49;&#54;&#x35;&#x38;&#x37;&#x32;&#51;&#x33;&#x35;&#64;&#x71;&#113;&#46;&#x63;&#111;&#x6d;</a></p><h3 id="正则表达式语法规则"><a href="#正则表达式语法规则" class="headerlink" title="正则表达式语法规则"></a>正则表达式语法规则</h3><h4 id="1、原义字符（字符本身就可以当作一个正则表达式）"><a href="#1、原义字符（字符本身就可以当作一个正则表达式）" class="headerlink" title="1、原义字符（字符本身就可以当作一个正则表达式）"></a>1、原义字符（字符本身就可以当作一个正则表达式）</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">a\b\c\...\z \t \r \n</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">        原义字符（字符本身就可以当作一个正则表达式）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RegularDemo2</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">regex</span> <span class="operator">=</span> <span class="string">&quot;a&quot;</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="string">&quot;abc12342121sadsa&amp;.;123!&quot;</span>;</span><br><span class="line">        <span class="comment">//String replaceAll(String regex, String replacement)</span></span><br><span class="line">        <span class="comment">//用给定的替换替换与给定的 regular expression匹配的此字符串的每个子字符串。</span></span><br><span class="line">        System.out.println(str.replaceAll(regex,<span class="string">&quot;_&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>运行结果</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">_bc12342121s_ds_&amp;.;<span class="number">123</span>!</span><br></pre></td></tr></table></figure><h4 id="正则表达是元字符高级用法"><a href="#正则表达是元字符高级用法" class="headerlink" title="正则表达是元字符高级用法"></a>正则表达是元字符高级用法</h4><table><thead><tr><th align="left">字符</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">\</td><td align="left">将下一个字符标记为一个特殊字符、或一个原义字符、或一个 向后引用、或一个八进制转义符。例如，’n’ 匹配字符 “n”。’\n’ 匹配一个换行符。序列 ‘\‘ 匹配 “&quot; 而 “(“ 则匹配 “(“。</td></tr><tr><td align="left">^</td><td align="left">匹配输入字符串的开始位置。如果设置了 RegExp 对象的 Multiline 属性，^ 也匹配 ‘\n’ 或 ‘\r’ 之后的位置。</td></tr><tr><td align="left">$</td><td align="left">匹配输入字符串的结束位置。如果设置了RegExp 对象的 Multiline 属性，$ 也匹配 ‘\n’ 或 ‘\r’ 之前的位置。</td></tr><tr><td align="left">*</td><td align="left">匹配前面的子表达式零次或多次。例如，zo* 能匹配 “z” 以及 “zoo”。* 等价于{0,}。</td></tr><tr><td align="left">+</td><td align="left">匹配前面的子表达式一次或多次。例如，’zo+’ 能匹配 “zo” 以及 “zoo”，但不能匹配 “z”。+ 等价于 {1,}。</td></tr><tr><td align="left">?</td><td align="left">匹配前面的子表达式零次或一次。例如，”do(es)?” 可以匹配 “do” 或 “does” 。? 等价于 {0,1}。</td></tr><tr><td align="left">{n}</td><td align="left">n 是一个非负整数。匹配确定的 n 次。例如，’o{2}’ 不能匹配 “Bob” 中的 ‘o’，但是能匹配 “food” 中的两个 o。</td></tr><tr><td align="left">{n,}</td><td align="left">n 是一个非负整数。至少匹配n 次。例如，’o{2,}’ 不能匹配 “Bob” 中的 ‘o’，但能匹配 “foooood” 中的所有 o。’o{1,}’ 等价于 ‘o+’。’o{0,}’ 则等价于 ‘o*’。</td></tr><tr><td align="left">{n,m}</td><td align="left">m 和 n 均为非负整数，其中n &lt;&#x3D; m。最少匹配 n 次且最多匹配 m 次。例如，”o{1,3}” 将匹配 “fooooood” 中的前三个 o。’o{0,1}’ 等价于 ‘o?’。请注意在逗号和两个数之间不能有空格。</td></tr><tr><td align="left">?</td><td align="left">当该字符紧跟在任何一个其他限制符 (*, +, ?, {n}, {n,}, {n,m}) 后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串 “oooo”，’o+?’ 将匹配单个 “o”，而 ‘o+’ 将匹配所有 ‘o’。</td></tr><tr><td align="left">.</td><td align="left">匹配除换行符（\n、\r）之外的任何单个字符。要匹配包括 ‘\n’ 在内的任何字符，请使用像”<strong>(.|\n)</strong>“的模式。</td></tr><tr><td align="left">(pattern)</td><td align="left">匹配 pattern 并获取这一匹配。所获取的匹配可以从产生的 Matches 集合得到，在VBScript 中使用 SubMatches 集合，在JScript 中则使用 $0…$9 属性。要匹配圆括号字符，请使用 ‘(‘ 或 ‘)‘。</td></tr><tr><td align="left">(?:pattern)</td><td align="left">匹配 pattern 但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用 “或” 字符 (|) 来组合一个模式的各个部分是很有用。例如， ‘industr(?:y|ies) 就是一个比 ‘industry|industries’ 更简略的表达式。</td></tr><tr><td align="left">(?&#x3D;pattern)</td><td align="left">正向肯定预查（look ahead positive assert），在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，”Windows(?&#x3D;95|98|NT|2000)”能匹配”Windows2000”中的”Windows”，但不能匹配”Windows3.1”中的”Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。</td></tr><tr><td align="left">(?!pattern)</td><td align="left">正向否定预查(negative assert)，在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如”Windows(?!95|98|NT|2000)”能匹配”Windows3.1”中的”Windows”，但不能匹配”Windows2000”中的”Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。</td></tr><tr><td align="left">(?&lt;&#x3D;pattern)</td><td align="left">反向(look behind)肯定预查，与正向肯定预查类似，只是方向相反。例如，”&#96;(?&lt;&#x3D;95</td></tr><tr><td align="left">(?&lt;!pattern)</td><td align="left">反向否定预查，与正向否定预查类似，只是方向相反。例如”&#96;(?&lt;!95</td></tr><tr><td align="left">x|y</td><td align="left">匹配 x 或 y。例如，’z|food’ 能匹配 “z” 或 “food”。’(z|f)ood’ 则匹配 “zood” 或 “food”。</td></tr><tr><td align="left">[xyz]</td><td align="left">字符集合。匹配所包含的任意一个字符。例如， ‘[abc]’ 可以匹配 “plain” 中的 ‘a’。</td></tr><tr><td align="left">[^xyz]</td><td align="left">负值字符集合。匹配未包含的任意字符。例如， ‘[^abc]’ 可以匹配 “plain” 中的’p’、’l’、’i’、’n’。</td></tr><tr><td align="left">[a-z]</td><td align="left">字符范围。匹配指定范围内的任意字符。例如，’[a-z]’ 可以匹配 ‘a’ 到 ‘z’ 范围内的任意小写字母字符。</td></tr><tr><td align="left">[^a-z]</td><td align="left">负值字符范围。匹配任何不在指定范围内的任意字符。例如，’[^a-z]’ 可以匹配任何不在 ‘a’ 到 ‘z’ 范围内的任意字符。</td></tr><tr><td align="left">\b</td><td align="left">匹配一个单词边界，也就是指单词和空格间的位置。例如， ‘er\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’。</td></tr><tr><td align="left">\B</td><td align="left">匹配非单词边界。’er\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’。</td></tr><tr><td align="left">\cx</td><td align="left">匹配由 x 指明的控制字符。例如， \cM 匹配一个 Control-M 或回车符。x 的值必须为 A-Z 或 a-z 之一。否则，将 c 视为一个原义的 ‘c’ 字符。</td></tr><tr><td align="left">\d</td><td align="left">匹配一个数字字符。等价于 [0-9]。</td></tr><tr><td align="left">\D</td><td align="left">匹配一个非数字字符。等价于 [^0-9]。</td></tr><tr><td align="left">\f</td><td align="left">匹配一个换页符。等价于 \x0c 和 \cL。</td></tr><tr><td align="left">\n</td><td align="left">匹配一个换行符。等价于 \x0a 和 \cJ。</td></tr><tr><td align="left">\r</td><td align="left">匹配一个回车符。等价于 \x0d 和 \cM。</td></tr><tr><td align="left">\s</td><td align="left">匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。</td></tr><tr><td align="left">\S</td><td align="left">匹配任何非空白字符。等价于 [^ \f\n\r\t\v]。</td></tr><tr><td align="left">\t</td><td align="left">匹配一个制表符。等价于 \x09 和 \cI。</td></tr><tr><td align="left">\v</td><td align="left">匹配一个垂直制表符。等价于 \x0b 和 \cK。</td></tr><tr><td align="left">\w</td><td align="left">匹配字母、数字、下划线。等价于’[A-Za-z0-9_]’。</td></tr><tr><td align="left">\W</td><td align="left">匹配非字母、数字、下划线。等价于 ‘[^A-Za-z0-9_]’。</td></tr><tr><td align="left">\xn</td><td align="left">匹配 n，其中 n 为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，’\x41’ 匹配 “A”。’\x041’ 则等价于 ‘\x04’ &amp; “1”。正则表达式中可以使用 ASCII 编码。</td></tr><tr><td align="left">\num</td><td align="left">匹配 num，其中 num 是一个正整数。对所获取的匹配的引用。例如，’(.)\1’ 匹配两个连续的相同字符。</td></tr><tr><td align="left">\n</td><td align="left">标识一个八进制转义值或一个向后引用。如果 \n 之前至少 n 个获取的子表达式，则 n 为向后引用。否则，如果 n 为八进制数字 (0-7)，则 n 为一个八进制转义值。</td></tr><tr><td align="left">\nm</td><td align="left">标识一个八进制转义值或一个向后引用。如果 \nm 之前至少有 nm 个获得子表达式，则 nm 为向后引用。如果 \nm 之前至少有 n 个获取，则 n 为一个后跟文字 m 的向后引用。如果前面的条件都不满足，若 n 和 m 均为八进制数字 (0-7)，则 \nm 将匹配八进制转义值 nm。</td></tr><tr><td align="left">\nml</td><td align="left">如果 n 为八进制数字 (0-3)，且 m 和 l 均为八进制数字 (0-7)，则匹配八进制转义值 nml。</td></tr><tr><td align="left">\un</td><td align="left">匹配 n，其中 n 是一个用四个十六进制数字表示的 Unicode 字符。例如， \u00A9 匹配版权符号 (?)。</td></tr></tbody></table><h5 id="字符类："><a href="#字符类：" class="headerlink" title="字符类："></a>字符类：</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">        正则表达式的语法规则：</span></span><br><span class="line"><span class="comment">        字符类：</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RegularDemo3</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">//[]作用是将字符进行分类，可以匹配到中括号中的任意一个字符</span></span><br><span class="line">        <span class="comment">//[abc]将来会匹配到abc中任意一个字符</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">regex</span> <span class="operator">=</span> <span class="string">&quot;[abc]&quot;</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="string">&quot;abc1234cc21c21sadcsa&amp;.;123!&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex,<span class="string">&quot;_&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>运行结果</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">___1234__21_21s_d_s_&amp;.;<span class="number">123</span>!</span><br></pre></td></tr></table></figure><h5 id="范围类："><a href="#范围类：" class="headerlink" title="范围类："></a>范围类：</h5><blockquote><p>其实就是在字符类的基础之上增加了一个范围</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shujia.wyh.day17;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">        范围类：</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RegularDemo4</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">//需求1：我想要匹配所有的小写字母</span></span><br><span class="line"><span class="comment">//        String regex = &quot;[abcdefghijklmnopqrstuvwxyz]&quot;;</span></span><br><span class="line">        <span class="comment">//[a-z]表示匹配a到z中的所有小写字母</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">regex</span> <span class="operator">=</span> <span class="string">&quot;[a-z]&quot;</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="string">&quot;abcABC1234ERQcc21c21sDASadcsABCCa&amp;.;123!&quot;</span>;</span><br><span class="line">        System.out.println(<span class="string">&quot;要匹配的字符串为：\n&quot;</span> + str);</span><br><span class="line">        System.out.println(<span class="string">&quot;==========================================&quot;</span>);</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//需求2：我想要匹配所有的大写字母</span></span><br><span class="line">        regex = <span class="string">&quot;[A-Z]&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//需求3：我即想要匹配小写字母，也想要匹配大写字母</span></span><br><span class="line"><span class="comment">//        regex = &quot;[a-zA-Z]&quot;;</span></span><br><span class="line">        regex = <span class="string">&quot;[A-z]&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//需求4：我想要匹配所有的数字</span></span><br><span class="line">        regex = <span class="string">&quot;[0-9]&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//需求5：我想匹配不仅大小写数字还有感叹号，分号咋办？</span></span><br><span class="line">        regex = <span class="string">&quot;[0-9a-zA-Z!;&amp;.]&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//需求6：我想匹配除了大小写字母和数字以外的符号</span></span><br><span class="line">        regex = <span class="string">&quot;[^0-9a-zA-Z]&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>运行结果：</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">要匹配的字符串为：</span><br><span class="line">abcABC1234ERQcc21c21sDASadcsABCCa&amp;.;<span class="number">123</span>!</span><br><span class="line">==========================================</span><br><span class="line">___ABC1234ERQ__21_21_DAS____ABCC_&amp;.;<span class="number">123</span>!</span><br><span class="line">abc___1234___cc21c21s___adcs____a&amp;.;<span class="number">123</span>!</span><br><span class="line">______1234_____21_21_____________&amp;.;<span class="number">123</span>!</span><br><span class="line">abcABC____ERQcc__c__sDASadcsABCCa&amp;.;___!</span><br><span class="line">________________________________________</span><br><span class="line">abcABC1234ERQcc21c21sDASadcsABCCa___123_</span><br></pre></td></tr></table></figure><h5 id="预定义类："><a href="#预定义类：" class="headerlink" title="预定义类："></a>预定义类：</h5><blockquote><p>我们在上面的范围类的情况下，在实际开发中我们可能会遇见一些常见的需求：判断是否是数字，是否是小写字母，是否是大写字母等等这些情况，用上面范围类的写法的话正则会比较长，所以在正则表达式中会给出一些含有特殊含义的表达式，这些表达式更加简化了我们使用范围类的用法，统称为预定义类，具体我们来探讨一下有哪些：</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\d == [0-9] 代表的是数字</span><br><span class="line">\D == [^0-9]代表的是非数字</span><br><span class="line">\s == 空白字符</span><br><span class="line">\w == [a-zA-Z0-9]</span><br><span class="line">\W == [^a-zA-Z0-9]</span><br><span class="line">.  == 任意字符</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shujia.wyh.day17;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">        预定义类</span></span><br><span class="line"><span class="comment">        \d == [0-9] 代表的是数字</span></span><br><span class="line"><span class="comment">        \D == [^0-9]代表的是非数字</span></span><br><span class="line"><span class="comment">        \s == 空白字符</span></span><br><span class="line"><span class="comment">        \w == [a-zA-Z0-9]</span></span><br><span class="line"><span class="comment">        \W == [^a-zA-Z0-9]</span></span><br><span class="line"><span class="comment">        .  == 任意字符</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RegularDemo5</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">regex</span> <span class="operator">=</span> <span class="string">&quot;\\d&quot;</span>;  <span class="comment">// \\d代表转义，使用它原本的意思</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="string">&quot;abcABC 1234  ERQcc2 1c21sDA Sadcs ABC Ca&amp;.;12  3!&quot;</span>;</span><br><span class="line">        System.out.println(<span class="string">&quot;要匹配的字符串为：\n&quot;</span> + str);</span><br><span class="line">        System.out.println(<span class="string">&quot;==========================================&quot;</span>);</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        regex = <span class="string">&quot;\\D&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        regex = <span class="string">&quot;\\s&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        regex = <span class="string">&quot;\\w&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        regex = <span class="string">&quot;\\W&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        regex = <span class="string">&quot;.&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//仅仅单纯要匹配一个字符.</span></span><br><span class="line">        regex = <span class="string">&quot;[.]&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line">        regex = <span class="string">&quot;\\.&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>运行结果</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">要匹配的字符串为：</span><br><span class="line">abcABC <span class="number">1234</span>  ERQcc2 1c21sDA Sadcs ABC Ca&amp;.;<span class="number">12</span>  <span class="number">3</span>!</span><br><span class="line">==========================================</span><br><span class="line">abcABC ____  ERQcc_ _c__sDA Sadcs ABC Ca&amp;.;__  _!</span><br><span class="line">_______1234_______2_1_21___________________12__3_</span><br><span class="line">abcABC_1234__ERQcc2_1c21sDA_Sadcs_ABC_Ca&amp;.;<span class="number">12__3</span>!</span><br><span class="line">______ ____  ______ _______ _____ ___ __&amp;.;__  _!</span><br><span class="line">abcABC_1234__ERQcc2_1c21sDA_Sadcs_ABC_Ca___12__3_</span><br><span class="line">_________________________________________________</span><br><span class="line">abcABC <span class="number">1234</span>  ERQcc2 1c21sDA Sadcs ABC Ca&amp;_;<span class="number">12</span>  <span class="number">3</span>!</span><br><span class="line">abcABC <span class="number">1234</span>  ERQcc2 1c21sDA Sadcs ABC Ca&amp;_;<span class="number">12</span>  <span class="number">3</span>!</span><br></pre></td></tr></table></figure><h5 id="边界类字符"><a href="#边界类字符" class="headerlink" title="边界类字符"></a>边界类字符</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">^:以xxx开头</span><br><span class="line">$:以xxx结尾</span><br><span class="line">\b:单词边界</span><br><span class="line">\B:非单词边界</span><br></pre></td></tr></table></figure><h5 id="量词分类"><a href="#量词分类" class="headerlink" title="量词分类"></a>量词分类</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">?:出现了<span class="number">0</span>次或者<span class="number">1</span>次</span><br><span class="line">+:代表出现了<span class="number">1</span>次或者多次</span><br><span class="line">*:代表出现了任意次</span><br><span class="line">&#123;n&#125;:代表出现了n次</span><br><span class="line">&#123;n,m&#125;:出现了n-m次</span><br><span class="line">&#123;n,&#125;:代表出现了至少n次</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shujia.wyh.day17;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">        量词分类：</span></span><br><span class="line"><span class="comment">        ?:出现了0次或者1次</span></span><br><span class="line"><span class="comment">        +:代表出现了1次或者多次</span></span><br><span class="line"><span class="comment">        *:代表出现了任意次</span></span><br><span class="line"><span class="comment">        &#123;n&#125;:代表出现了n次</span></span><br><span class="line"><span class="comment">        &#123;n,m&#125;:出现了n-m次</span></span><br><span class="line"><span class="comment">        &#123;n,&#125;:代表出现了至少n次</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RegularDemo7</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">regex</span> <span class="operator">=</span> <span class="string">&quot;^b?&quot;</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="string">&quot;aaaaaabaaacdeaaaafg&quot;</span>;</span><br><span class="line">        System.out.println(<span class="string">&quot;要匹配的字符串为：\n&quot;</span> + str);</span><br><span class="line">        System.out.println(<span class="string">&quot;==========================================&quot;</span>);</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        regex = <span class="string">&quot;^b+&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        regex = <span class="string">&quot;^a+&quot;</span>; <span class="comment">//匹配连续出现1次以上并且是开头的a</span></span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        regex = <span class="string">&quot;a+&quot;</span>; <span class="comment">//匹配连续出现1次以上的a</span></span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        regex = <span class="string">&quot;^a&#123;2&#125;&quot;</span>; <span class="comment">//匹配连续出现2次的a</span></span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        regex = <span class="string">&quot;^a&#123;2,5&#125;&quot;</span>; <span class="comment">//匹配连续出现2次到5次的a开头</span></span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        regex = <span class="string">&quot;a&#123;4,&#125;&quot;</span>; <span class="comment">//匹配连续出现4次以上</span></span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>运行结果</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">要匹配的字符串为：</span><br><span class="line">aaaaaabaaacdeaaaafg</span><br><span class="line">==========================================</span><br><span class="line">_aaaaaabaaacdeaaaafg</span><br><span class="line">aaaaaabaaacdeaaaafg</span><br><span class="line">_baaacdeaaaafg</span><br><span class="line">_b_cde_fg</span><br><span class="line">_aaaabaaacdeaaaafg</span><br><span class="line">_abaaacdeaaaafg</span><br><span class="line">_baaacde_fg</span><br><span class="line"></span><br><span class="line">Process finished with exit code <span class="number">0</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">分组的符号：()</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shujia.wyh.day17;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">        正则表达式语法：</span></span><br><span class="line"><span class="comment">        分组：()</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RegularDemo8</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">//将连续出现了3次以上的abc替换成_</span></span><br><span class="line"><span class="comment">//        String regex = &quot;abc&#123;3,&#125;&quot;; //ab后面接着3次以上的c</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">regex</span> <span class="operator">=</span> <span class="string">&quot;(abc)&#123;3,&#125;&quot;</span>; <span class="comment">//使用小括号将abc看作是一组，然后匹配这组出现了3次以上</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="string">&quot;abccccccccABC123ABC123abcABCabcabcabc123ABC123123&quot;</span>;</span><br><span class="line">        System.out.println(<span class="string">&quot;要匹配的字符串为：\n&quot;</span> + str);</span><br><span class="line">        System.out.println(<span class="string">&quot;==========================================&quot;</span>);</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//需求2：ABC后面跟上出现1次以上的123为一个整体进行匹配</span></span><br><span class="line">        <span class="comment">//ABC123123123</span></span><br><span class="line">        regex = <span class="string">&quot;ABC(123)&#123;1,&#125;&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//需求3：ABC后面跟上出现1次以上的123或者abc为一个整体进行匹配</span></span><br><span class="line">        <span class="comment">//ABC123123</span></span><br><span class="line">        <span class="comment">//ABCabcabc</span></span><br><span class="line">        regex = <span class="string">&quot;ABC(123|abc)&#123;1,&#125;&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>运行结果：</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">要匹配的字符串为：</span><br><span class="line">abccccccccABC123ABC123abcABCabcabcabc123ABC123123</span><br><span class="line">==========================================</span><br><span class="line">abccccccccABC123ABC123abcABC_123ABC123123</span><br><span class="line">abcccccccc__abcABCabcabcabc123_</span><br><span class="line">abcccccccc____</span><br></pre></td></tr></table></figure><h5 id="反向引用"><a href="#反向引用" class="headerlink" title="反向引用"></a>反向引用</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shujia.wyh.day17;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">        反向引用</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RegularDemo9</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">//日期案例</span></span><br><span class="line">        <span class="comment">// 2022-03-28  ---&gt; 03/28/2022</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">regex</span> <span class="operator">=</span> <span class="string">&quot;(\\d&#123;4&#125;)-(\\d&#123;2&#125;)-(\\d&#123;2&#125;)&quot;</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="string">&quot;2022-03-28 dasdas 2022-04-05&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex,<span class="string">&quot;$2/$3/$1&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//需求2：我不想你取出月份</span></span><br><span class="line">        regex = <span class="string">&quot;(\\d&#123;4&#125;)-(?:\\d&#123;2&#125;)-(\\d&#123;2&#125;)&quot;</span>;</span><br><span class="line">        System.out.println(str.replaceAll(regex,<span class="string">&quot;$2/$1&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>运行结果：</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="number">03</span>/<span class="number">28</span>/<span class="number">2022</span> dasdas <span class="number">04</span>/<span class="number">05</span>/<span class="number">2022</span></span><br><span class="line"><span class="number">28</span>/<span class="number">2022</span> dasdas <span class="number">05</span>/<span class="number">2022</span></span><br></pre></td></tr></table></figure><h4 id="正则表达式在java中的应用"><a href="#正则表达式在java中的应用" class="headerlink" title="正则表达式在java中的应用"></a>正则表达式在java中的应用</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、字符串的查找操作：Pattern和Matcher</span><br><span class="line">2、字符串的匹配操作：可以使用字符串String类中matches()方法</span><br><span class="line">3、字符串的分割操作：可以使用字符串String类中的split()方法</span><br><span class="line">4、字符串的替换工作：字符串中的replaceAll()方法和replaceFirst()方法</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shujia.wyh.day17;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.regex.Matcher;</span><br><span class="line"><span class="keyword">import</span> java.util.regex.Pattern;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">        正则表达式在Java中的应用：</span></span><br><span class="line"><span class="comment">        1、字符串的查找操作：Pattern和Matcher</span></span><br><span class="line"><span class="comment">        2、字符串的匹配操作：可以使用字符串String类中matches()方法</span></span><br><span class="line"><span class="comment">        3、字符串的分割操作：可以使用字符串String类中的split()方法</span></span><br><span class="line"><span class="comment">        4、字符串的替换工作：字符串中的replaceAll()方法和replaceFirst()方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RegularDemo10</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">regex</span> <span class="operator">=</span> <span class="string">&quot;\\w&#123;3,&#125;&quot;</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="string">&quot;abcdef123;!&quot;</span>;</span><br><span class="line">        <span class="comment">//public boolean matches(String regex)告诉这个字符串是否匹配给定的regular expression 。</span></span><br><span class="line">        System.out.println(str.matches(regex));</span><br><span class="line">        System.out.println(<span class="string">&quot;======================================&quot;</span>);</span><br><span class="line"></span><br><span class="line">        regex = <span class="string">&quot;[a-z]&#123;2,&#125;&quot;</span>;</span><br><span class="line">        str = <span class="string">&quot;abc def hello 123dsa&quot;</span>;</span><br><span class="line">        System.out.println(str.matches(regex));</span><br><span class="line">        System.out.println(str.replaceAll(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line">        System.out.println(str.replaceFirst(regex, <span class="string">&quot;_&quot;</span>));</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;=======================================&quot;</span>);</span><br><span class="line">        str = <span class="string">&quot;hllo wdw worel spark&quot;</span>;</span><br><span class="line">        <span class="comment">//public String[] split(String regex)将此字符串拆分为给定的regular expression的匹配。</span></span><br><span class="line">        String[] strings = str.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; strings.length; i++) &#123;</span><br><span class="line">            System.out.println(strings[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">&quot;=======================================&quot;</span>);</span><br><span class="line">        String[] ws = str.split(<span class="string">&quot;w&quot;</span>);</span><br><span class="line">        <span class="type">String</span> <span class="variable">s</span> <span class="operator">=</span> Arrays.toString(ws);</span><br><span class="line">        System.out.println(s);</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;=======================================&quot;</span>);</span><br><span class="line">        <span class="comment">//字符串的查找操作：Pattern和Matcher</span></span><br><span class="line">        regex = <span class="string">&quot;\\w&#123;3,7&#125;&quot;</span>;</span><br><span class="line">        str = <span class="string">&quot;abcd123qqqq122321dddd44&quot;</span>;</span><br><span class="line">        <span class="comment">//创建一个java对应的正则表达式对象</span></span><br><span class="line">        <span class="comment">//public static Pattern compile(String regex)将给定的正则表达式编译为模式</span></span><br><span class="line">        <span class="type">Pattern</span> <span class="variable">compile</span> <span class="operator">=</span> Pattern.compile(regex);</span><br><span class="line">        <span class="comment">//public Matcher matcher(CharSequence input)创建一个匹配器，匹配给定的输入与此模式。</span></span><br><span class="line">        <span class="type">Matcher</span> <span class="variable">matcher</span> <span class="operator">=</span> compile.matcher(str);</span><br><span class="line">        <span class="comment">//boolean matches()</span></span><br><span class="line">        <span class="comment">//尝试将整个区域与模式进行匹配。</span></span><br><span class="line"><span class="comment">//        System.out.println(matcher.matches());</span></span><br><span class="line">        System.out.println(matcher.find());</span><br><span class="line">        <span class="comment">//int end()</span></span><br><span class="line">        <span class="comment">//返回最后一个字符匹配后的偏移量。</span></span><br><span class="line">        <span class="comment">//public int start()返回上一个匹配的起始索引。</span></span><br><span class="line"><span class="comment">//        System.out.println(matcher.start());</span></span><br><span class="line">        System.out.println(matcher.end());</span><br><span class="line">        <span class="comment">//String group()</span></span><br><span class="line">        <span class="comment">//返回与上一个匹配匹配的输入子序列。</span></span><br><span class="line">        System.out.println(matcher.group());</span><br><span class="line">        System.out.println(<span class="string">&quot;====================================&quot;</span>);</span><br><span class="line">        <span class="comment">//boolean find()</span></span><br><span class="line">        <span class="comment">//尝试找到匹配模式的输入序列的下一个子序列。</span></span><br><span class="line">        System.out.println(matcher.find());</span><br><span class="line">        <span class="comment">//int end()</span></span><br><span class="line">        <span class="comment">//返回最后一个字符匹配后的偏移量。</span></span><br><span class="line">        <span class="comment">//public int start()返回上一个匹配的起始索引。</span></span><br><span class="line"><span class="comment">//        System.out.println(matcher.start());</span></span><br><span class="line">        System.out.println(matcher.end());</span><br><span class="line">        <span class="comment">//String group()</span></span><br><span class="line">        <span class="comment">//返回与上一个匹配匹配的输入子序列。</span></span><br><span class="line">        System.out.println(matcher.group());</span><br><span class="line"><span class="comment">//        System.out.println(matcher.group());</span></span><br><span class="line">        System.out.println(<span class="string">&quot;================================&quot;</span>);</span><br><span class="line">        System.out.println(matcher.find());</span><br><span class="line">        <span class="comment">//int end()</span></span><br><span class="line">        <span class="comment">//返回最后一个字符匹配后的偏移量。</span></span><br><span class="line">        <span class="comment">//public int start()返回上一个匹配的起始索引。</span></span><br><span class="line"><span class="comment">//        System.out.println(matcher.start());</span></span><br><span class="line">        System.out.println(matcher.end());</span><br><span class="line">        <span class="comment">//String group()</span></span><br><span class="line">        <span class="comment">//返回与上一个匹配匹配的输入子序列。</span></span><br><span class="line">        System.out.println(matcher.group());</span><br><span class="line">        System.out.println(<span class="string">&quot;==================================&quot;</span>);</span><br><span class="line">        System.out.println(matcher.find());</span><br><span class="line">        <span class="comment">//int end()</span></span><br><span class="line">        <span class="comment">//返回最后一个字符匹配后的偏移量。</span></span><br><span class="line">        <span class="comment">//public int start()返回上一个匹配的起始索引。</span></span><br><span class="line"><span class="comment">//        System.out.println(matcher.start());</span></span><br><span class="line">        System.out.println(matcher.end());</span><br><span class="line">        <span class="comment">//String group()</span></span><br><span class="line">        <span class="comment">//返回与上一个匹配匹配的输入子序列。</span></span><br><span class="line">        System.out.println(matcher.group());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>运行结果</p></blockquote><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="literal">false</span></span><br><span class="line">======================================</span><br><span class="line"><span class="literal">false</span></span><br><span class="line">_ _ _ 123_</span><br><span class="line">_ def hello 123dsa</span><br><span class="line">=======================================</span><br><span class="line">hllo</span><br><span class="line">wdw</span><br><span class="line"><span class="type">worel</span></span><br><span class="line"><span class="variable">spark</span></span><br><span class="line"><span class="operator">=</span>======================================</span><br><span class="line">[hllo , d,  , orel spark]</span><br><span class="line">=======================================</span><br><span class="line"><span class="literal">true</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line">abcd123</span><br><span class="line">====================================</span><br><span class="line"><span class="literal">true</span></span><br><span class="line"><span class="number">14</span></span><br><span class="line">qqqq122</span><br><span class="line">================================</span><br><span class="line"><span class="literal">true</span></span><br><span class="line"><span class="number">21</span></span><br><span class="line">321dddd</span><br><span class="line">==================================</span><br><span class="line"><span class="literal">false</span></span><br><span class="line">Exception in thread <span class="string">&quot;main&quot;</span> java.lang.IllegalStateException: No match available</span><br><span class="line">at java.util.regex.Matcher.end(Matcher.java:<span class="number">415</span>)</span><br><span class="line">at com.shujia.wyh.day17.RegularDemo10.main(RegularDemo10.java:<span class="number">90</span>)</span><br><span class="line"></span><br><span class="line">Process finished with exit code <span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="正则表达式练习："><a href="#正则表达式练习：" class="headerlink" title="正则表达式练习："></a>正则表达式练习：</h5><p>题目：治疗口吃</p><p>将字符串“我我我我我我我……….我……..要要要要要………….要要要要….学习习习……习习习习习习习习习编程程程程程程程程程程程…..程程程程程程程程” —-&gt; “我要学习编程”</p><p>分析：1、先将.去掉      2、再将叠词变成一个</p><p>帮助理解正则表达式的网址：<a href="https://regexper.com/">https://regexper.com/</a></p>]]></content>
      
      
      <categories>
          
          <category> 正则表达式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 正则表达式 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
