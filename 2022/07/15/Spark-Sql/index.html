<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Spark-SQL | Notes</title><meta name="keywords" content="Spark-2.4.5 大数据 分布式"><meta name="author" content="秋水一色"><meta name="copyright" content="秋水一色"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="对学习Spark-Sql的一些知识笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark-SQL">
<meta property="og:url" content="http://example.com/2022/07/15/Spark-Sql/index.html">
<meta property="og:site_name" content="Notes">
<meta property="og:description" content="对学习Spark-Sql的一些知识笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2022/07/25/dyYNo7VTn5Bw6lc.jpg">
<meta property="article:published_time" content="2022-07-14T16:00:00.000Z">
<meta property="article:modified_time" content="2022-08-10T02:49:31.547Z">
<meta property="article:author" content="秋水一色">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2022/07/25/dyYNo7VTn5Bw6lc.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2022/07/15/Spark-Sql/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark-SQL',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-08-10 10:49:31'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="Notes" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2022/03/30/mD5ZV7wdUBGl62S.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">43</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2022/07/25/dyYNo7VTn5Bw6lc.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Notes</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark-SQL</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-07-14T16:00:00.000Z" title="发表于 2022-07-15 00:00:00">2022-07-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-08-10T02:49:31.547Z" title="更新于 2022-08-10 10:49:31">2022-08-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark-SQL"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h5 id="1、sparkWordCount"><a href="#1、sparkWordCount" class="headerlink" title="1、sparkWordCount"></a>1、sparkWordCount</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo1WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 1、读取数据构建DataFrame，DF相当于一张表</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> linesDF: <span class="type">DataFrame</span> = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">&quot;csv&quot;</span>) <span class="comment">//指定读取数据的文件格式</span></span><br><span class="line">      .schema(<span class="string">&quot;line STRING&quot;</span>) <span class="comment">//指定字段名和字段类型</span></span><br><span class="line">      .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;\t&quot;</span>) <span class="comment">//指定分隔符，csv格式默认逗号分隔</span></span><br><span class="line">      .load(<span class="string">&quot;data/words.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    linesDF.printSchema() <span class="comment">//打印表结构</span></span><br><span class="line">    linesDF.show() <span class="comment">//打印数据</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 2、将DataFrame注册成一个视图，才能写sql</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    linesDF.createOrReplaceTempView(<span class="string">&quot;lines&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 3、写Spark的SQl来统计单词的数量</span></span><br><span class="line"><span class="comment">     * sparkSQl的语法完全兼容hive sql</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> result: <span class="type">DataFrame</span> = spark.sql(</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |</span></span><br><span class="line"><span class="string">        |select word,count(1) as c from</span></span><br><span class="line"><span class="string">        |(select explode(split(line,&#x27;,&#x27;)) as word</span></span><br><span class="line"><span class="string">        |from</span></span><br><span class="line"><span class="string">        |lines) as a</span></span><br><span class="line"><span class="string">        |group by word</span></span><br><span class="line"><span class="string">        |</span></span><br><span class="line"><span class="string">        |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 4、将数据保存到本地</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    result</span><br><span class="line">      .write</span><br><span class="line">      .format(<span class="string">&quot;csv&quot;</span>)<span class="comment">//指定文件类型</span></span><br><span class="line">      .option(<span class="string">&quot;sep&quot;</span>,<span class="string">&quot;,&quot;</span>)<span class="comment">//数据分隔符</span></span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)<span class="comment">//覆盖数据</span></span><br><span class="line">      .save(<span class="string">&quot;data/wc1&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2、WslWordCount"><a href="#2、WslWordCount" class="headerlink" title="2、WslWordCount"></a>2、WslWordCount</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo2DSLWC</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 1、读取数据构建DataFrame，DF相当于一张表</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> linesDF: <span class="type">DataFrame</span> = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">&quot;csv&quot;</span>) <span class="comment">//指定读取数据的文件格式</span></span><br><span class="line">      .schema(<span class="string">&quot;line STRING&quot;</span>) <span class="comment">//指定字段名和字段类型</span></span><br><span class="line">      .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;\t&quot;</span>) <span class="comment">//指定分隔符，csv格式默认逗号分隔</span></span><br><span class="line">      .load(<span class="string">&quot;data/words.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * DSL:类sql api，介于代码和SQL之间的一种写法</span></span><br><span class="line"><span class="comment">     * 在写DSL之前需要导入sparkSql的函数和隐式转换</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//$&quot;line&quot; 获取列对象</span></span><br><span class="line">    linesDF</span><br><span class="line">      <span class="comment">//将一行中的多个单词拆分成多行</span></span><br><span class="line">      .select(explode(split($<span class="string">&quot;line&quot;</span>, <span class="string">&quot;,&quot;</span>)) as <span class="string">&quot;word&quot;</span>)</span><br><span class="line">      <span class="comment">//按照单词分组</span></span><br><span class="line">      .groupBy($<span class="string">&quot;word&quot;</span>)</span><br><span class="line">      <span class="comment">//统计单词的数量</span></span><br><span class="line">      .agg(count($<span class="string">&quot;word&quot;</span>) as <span class="string">&quot;c&quot;</span>)</span><br><span class="line">      .write <span class="comment">//保存数据</span></span><br><span class="line">      .format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;sep&quot;</span>,<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">      .save(<span class="string">&quot;data/wc2&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="3、DslAPI"><a href="#3、DslAPI" class="headerlink" title="3、DslAPI"></a>3、DslAPI</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo3DSLAPI</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 1、创建spark连接</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> session: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;Demo3DslAPI&quot;</span>)</span><br><span class="line">      .config(<span class="string">&quot;spark.sql.shuffle.partition&quot;</span>,<span class="number">1</span>) <span class="comment">//指定在shuffle之后的分区数，默认是200，类似hive中设置reduce的数量</span></span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * DSL API</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//读取一个json格式文件,spark会自动识别json中的列名</span></span><br><span class="line">    <span class="keyword">val</span> dataFrame: <span class="type">DataFrame</span> = session</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">&quot;json&quot;</span>) <span class="comment">//指定读取数据的格式为json</span></span><br><span class="line">      .load(<span class="string">&quot;data/students.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    dataFrame.printSchema()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * show：查看df中数据，相当于rdd的action算子，会触发任务的执行</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    dataFrame.show()</span><br><span class="line">    <span class="comment">//指定打印多少行</span></span><br><span class="line">    dataFrame.show(<span class="number">20</span>)</span><br><span class="line">    <span class="comment">//完整打印每一列的数据</span></span><br><span class="line">    dataFrame.show(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * select:选择数据，和sql中的select用法基本一致</span></span><br><span class="line"><span class="comment">     * dsl中select 不能使用聚合函数需要在agg中使用聚合函数</span></span><br><span class="line"><span class="comment">     * select相当于rdd中的转换算子</span></span><br><span class="line"><span class="comment">     * selectExpr可以传一个sql的表达式</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    dataFrame.select(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line">    dataFrame.selectExpr(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;age+1 as age&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//需要导入spark sql的函数才能在DSl中使用函数</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    <span class="keyword">import</span> session.implicits._</span><br><span class="line">    <span class="comment">//使用列对象的方式</span></span><br><span class="line">    dataFrame.select($<span class="string">&quot;name&quot;</span>, $<span class="string">&quot;age&quot;</span> + <span class="number">1</span> as <span class="string">&quot;age&quot;</span>).show()</span><br><span class="line">    <span class="comment">//在sql中使用spark函数</span></span><br><span class="line">    dataFrame.select($<span class="string">&quot;id&quot;</span>, substring($<span class="string">&quot;clazz&quot;</span>, <span class="number">0</span>, <span class="number">2</span>) as <span class="string">&quot;zz&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * where：过滤数据 也相当于一个转换算子</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//字符串的sql表达式</span></span><br><span class="line">    dataFrame.where(<span class="string">&quot;gender = &#x27;女&#x27; and age = 23&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用列对象</span></span><br><span class="line">    dataFrame.where($<span class="string">&quot;gender&quot;</span> =!= <span class="string">&quot;男&quot;</span> and $<span class="string">&quot;age&quot;</span> === <span class="number">22</span>).show()</span><br><span class="line">    <span class="comment">//这个里面只能用函数不能写scala代码</span></span><br><span class="line">    dataFrame.where(substring($<span class="string">&quot;clazz&quot;</span>, <span class="number">0</span>, <span class="number">2</span>) === <span class="string">&quot;文科&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * groupBy agg ：分组聚合要一起使用不能单独使用</span></span><br><span class="line"><span class="comment">     * 分组聚合之后返回的DF只包含分组字段和聚合字段</span></span><br><span class="line"><span class="comment">     * 分组不能在select中聚合</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    dataFrame</span><br><span class="line">      .groupBy($<span class="string">&quot;clazz&quot;</span>)</span><br><span class="line">      .agg(count($<span class="string">&quot;clazz&quot;</span>) as <span class="string">&quot;num&quot;</span>, floor(avg($<span class="string">&quot;age&quot;</span>)) as <span class="string">&quot;avg_age1&quot;</span>, ceil(avg($<span class="string">&quot;age&quot;</span>)) as <span class="string">&quot;avg_age2&quot;</span>)</span><br><span class="line">      .show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * orderBy : 排序</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    dataFrame</span><br><span class="line">      .groupBy($<span class="string">&quot;clazz&quot;</span>)</span><br><span class="line">      .agg(count($<span class="string">&quot;clazz&quot;</span>) as <span class="string">&quot;num&quot;</span>)</span><br><span class="line">      .orderBy($<span class="string">&quot;num&quot;</span>.desc)</span><br><span class="line">      .show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * join:表关联</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//读取分数表</span></span><br><span class="line">    <span class="keyword">val</span> scoreDF: <span class="type">DataFrame</span> = session</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;,&quot;</span>)</span><br><span class="line">      .schema(<span class="string">&quot;id STRING,cid STRING,Sco DOUBLE&quot;</span>) <span class="comment">//指定列名</span></span><br><span class="line">      .load(<span class="string">&quot;data/score.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//关联字段名不一样时</span></span><br><span class="line">    <span class="comment">//    scoreDF.join(dataFrame,$&quot;id&quot;===$&quot;sid&quot;,&quot;inner&quot;).show()</span></span><br><span class="line">    <span class="comment">//关联字段名一样时,直接写&quot;id&quot;</span></span><br><span class="line">    <span class="keyword">val</span> joinDF: <span class="type">DataFrame</span> = scoreDF.join(dataFrame, <span class="string">&quot;id&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 开窗函数</span></span><br><span class="line"><span class="comment">     * 统计每个班级总分前十的学生</span></span><br><span class="line"><span class="comment">     * withColumn():在DF的基础上增加新的列</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    joinDF</span><br><span class="line">      .groupBy($<span class="string">&quot;id&quot;</span>,$<span class="string">&quot;clazz&quot;</span>)<span class="comment">//按照学号和班级分组</span></span><br><span class="line">      .agg(sum($<span class="string">&quot;sco&quot;</span>) as <span class="string">&quot;sumSco&quot;</span>)<span class="comment">//计算总分</span></span><br><span class="line">      <span class="comment">//简写，在前面的基础上增加列</span></span><br><span class="line">      .withColumn(<span class="string">&quot;r&quot;</span>,row_number() over <span class="type">Window</span>.partitionBy($<span class="string">&quot;clazz&quot;</span>).orderBy($<span class="string">&quot;sumSco&quot;</span>.desc))</span><br><span class="line">      .where($<span class="string">&quot;r&quot;</span>&lt;=<span class="number">10</span>)</span><br><span class="line">      .show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * sql</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    joinDF.createOrReplaceTempView(<span class="string">&quot;student_score&quot;</span>)</span><br><span class="line"></span><br><span class="line">    session.sql(</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |select * from (</span></span><br><span class="line"><span class="string">        |select id as sid,clazz,sumSco,row_number() over(partition by clazz order by sumSco desc) as r from (</span></span><br><span class="line"><span class="string">        |select id,clazz,sum(sco) as  sumSco</span></span><br><span class="line"><span class="string">        |from student_score</span></span><br><span class="line"><span class="string">        |group by id,clazz</span></span><br><span class="line"><span class="string">        |) as a</span></span><br><span class="line"><span class="string">        |) as b</span></span><br><span class="line"><span class="string">        |where r&lt;=10</span></span><br><span class="line"><span class="string">        |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="4、DataSource"><a href="#4、DataSource" class="headerlink" title="4、DataSource"></a>4、DataSource</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo4DataSource</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">&quot;source&quot;</span>)</span><br><span class="line">      .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">      .config(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="number">1</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * csv格式的数据</span></span><br><span class="line"><span class="comment">     * 选哟指定表结构，分隔符（默认时逗号），文件路径</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> csvDF: <span class="type">DataFrame</span> = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">      .schema(<span class="string">&quot;id STRING,name STRING,age INT,gender STRING, clazz STRING&quot;</span>) <span class="comment">//指定列名和类的类型</span></span><br><span class="line">      .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;,&quot;</span>)</span><br><span class="line">      .load(<span class="string">&quot;data/students.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    csvDF.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将数据保持为csv格式</span></span><br><span class="line">    csvDF</span><br><span class="line">      .groupBy($<span class="string">&quot;clazz&quot;</span>)</span><br><span class="line">      .agg(count($<span class="string">&quot;clazz&quot;</span>) as <span class="string">&quot;num&quot;</span>)</span><br><span class="line">      .write <span class="comment">//保持数据</span></span><br><span class="line">      .format(<span class="string">&quot;csv&quot;</span>) <span class="comment">//指定保持数据的格式</span></span><br><span class="line">      .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;,&quot;</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">      .save(<span class="string">&quot;data/clazz_num1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * json格式</span></span><br><span class="line"><span class="comment">     * spark 会自动将json中字段名和字段类型解析出来</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * json格式比csv格式占用的空间更大，在大数据场景下不适用json</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取json格式的数据</span></span><br><span class="line">    <span class="keyword">val</span> jsonDF: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">      .format(<span class="string">&quot;json&quot;</span>)</span><br><span class="line">      .load(<span class="string">&quot;data/students.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    jsonDF.show()</span><br><span class="line">    <span class="comment">//将数据保存为json格式</span></span><br><span class="line">    jsonDF</span><br><span class="line">      .groupBy($<span class="string">&quot;gender&quot;</span>)</span><br><span class="line">      .agg(count($<span class="string">&quot;gender&quot;</span>) as <span class="string">&quot;c&quot;</span>)</span><br><span class="line">      .write</span><br><span class="line">      .format(<span class="string">&quot;json&quot;</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">      .save(<span class="string">&quot;data/gender_num&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * parquet: 带表结构的压缩格式</span></span><br><span class="line"><span class="comment">     * 压缩：时间换空间</span></span><br><span class="line"><span class="comment">     * 压缩比取决于《信息熵》</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//1、将数据保存为parquet</span></span><br><span class="line">    jsonDF</span><br><span class="line">      .write</span><br><span class="line">      .format(<span class="string">&quot;parquet&quot;</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">      .save(<span class="string">&quot;data/students&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 读取parquet</span></span><br><span class="line"><span class="comment">     * parquet格式的数据自带了表结构，不需要手动指定</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> parquetDF: <span class="type">DataFrame</span> = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">&quot;parquet&quot;</span>)</span><br><span class="line">      .load(<span class="string">&quot;data/students&quot;</span>)</span><br><span class="line"></span><br><span class="line">    parquetDF.printSchema()</span><br><span class="line">    parquetDF.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 读取JDBC中的数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> jdbcDF: <span class="type">DataFrame</span> = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://master:3306&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;bigdata.students&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;123456&quot;</span>)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    jdbcDF.printSchema()</span><br><span class="line">    jdbcDF.show()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="5、RDDToDF"><a href="#5、RDDToDF" class="headerlink" title="5、RDDToDF"></a>5、RDDToDF</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo5RDDToDF</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">&quot;rdd&quot;</span>)</span><br><span class="line">      .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取sparkContext，使用rdd ppi</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> linesRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;data/students.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> studnetRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)] = linesRDD</span><br><span class="line">      .map(_.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">      .map &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Array</span>(id: <span class="type">String</span>, name: <span class="type">String</span>, age: <span class="type">String</span>, gender: <span class="type">String</span>, clazz: <span class="type">String</span>) =&gt;</span><br><span class="line">          (id, name, age, gender, clazz)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 将rdd转换成DF</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> studentDF: <span class="type">DataFrame</span> = studnetRDD.toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>, <span class="string">&quot;gender&quot;</span>, <span class="string">&quot;clazz&quot;</span>)</span><br><span class="line"></span><br><span class="line">    studentDF.printSchema()</span><br><span class="line">    studentDF.show()</span><br><span class="line"></span><br><span class="line">    spark</span><br><span class="line">      .read</span><br><span class="line">      .json()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="6、DFToRDD"><a href="#6、DFToRDD" class="headerlink" title="6、DFToRDD"></a>6、DFToRDD</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo6DFToRDD</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">&quot;rdd&quot;</span>)</span><br><span class="line">      .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> studentDF: <span class="type">DataFrame</span> = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">&quot;json&quot;</span>)</span><br><span class="line">      .load(<span class="string">&quot;data/students.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    studentDF.printSchema()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将DF转换成RDD</span></span><br><span class="line">    <span class="keyword">val</span> studentRDD: <span class="type">RDD</span>[<span class="type">Row</span>] = studentDF.rdd</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stuRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">Long</span>, <span class="type">String</span>, <span class="type">String</span>)] = studentRDD.map((row: <span class="type">Row</span>) =&gt; &#123;</span><br><span class="line">      <span class="comment">//通过列名获取数据</span></span><br><span class="line">      <span class="keyword">val</span> id: <span class="type">String</span> = row.getAs[<span class="type">String</span>](<span class="string">&quot;id&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> name: <span class="type">String</span> = row.getAs[<span class="type">String</span>](<span class="string">&quot;name&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> age: <span class="type">Long</span> = row.getAs[<span class="type">Long</span>](<span class="string">&quot;age&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> gender: <span class="type">String</span> = row.getAs[<span class="type">String</span>](<span class="string">&quot;gender&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> clazz: <span class="type">String</span> = row.getAs[<span class="type">String</span>](<span class="string">&quot;clazz&quot;</span>)</span><br><span class="line">      (id, name, age, gender, clazz)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//stuRDD.foreach(println)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> caseRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">Long</span>, <span class="type">String</span>, <span class="type">String</span>)] = studentRDD.map &#123;</span><br><span class="line">      <span class="comment">//需要注意字段顺序</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(age: <span class="type">Long</span>, clazz: <span class="type">String</span>, gender: <span class="type">String</span>, id: <span class="type">String</span>, name: <span class="type">String</span>) =&gt;</span><br><span class="line">        (id, name, age, gender, clazz)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    caseRDD.foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="7、开窗"><a href="#7、开窗" class="headerlink" title="7、开窗"></a>7、开窗</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo7Window</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 窗口函数</span></span><br><span class="line"><span class="comment">     * row_number</span></span><br><span class="line"><span class="comment">     * rank</span></span><br><span class="line"><span class="comment">     * sum</span></span><br><span class="line"><span class="comment">     * count</span></span><br><span class="line"><span class="comment">     * avg</span></span><br><span class="line"><span class="comment">     * max</span></span><br><span class="line"><span class="comment">     * min</span></span><br><span class="line"><span class="comment">     * lag 取当前行前面</span></span><br><span class="line"><span class="comment">     * lead： 取当前行后面的</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;window&quot;</span>)</span><br><span class="line">      .config(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="number">1</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//学生表</span></span><br><span class="line">    <span class="keyword">val</span> studentDF: <span class="type">DataFrame</span> = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;,&quot;</span>)</span><br><span class="line">      .schema(<span class="string">&quot;id STRING,name STRING,age INT,gender STRING,clazz STRING&quot;</span>)</span><br><span class="line">      .load(<span class="string">&quot;data/students.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分数表</span></span><br><span class="line">    <span class="keyword">val</span> scoreDF: <span class="type">DataFrame</span> = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;,&quot;</span>)</span><br><span class="line">      .schema(<span class="string">&quot;sid STRING,cid STRING,sco DOUBLE&quot;</span>)</span><br><span class="line">      .load(<span class="string">&quot;data/score.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//科目表</span></span><br><span class="line">    <span class="keyword">val</span> subjectDF: <span class="type">DataFrame</span> = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;,&quot;</span>)</span><br><span class="line">      .schema(<span class="string">&quot;cid STRING,cname STRING,ssco DOUBLE&quot;</span>)</span><br><span class="line">      .load(<span class="string">&quot;data/subject.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> joinDF: <span class="type">DataFrame</span> = studentDF.join(scoreDF, $<span class="string">&quot;id&quot;</span> === $<span class="string">&quot;sid&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 1、统计总分年级排名前十学生各科的分数</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * sum over 由两种用法</span></span><br><span class="line"><span class="comment">     * 1、之分区不拍戏----总和</span></span><br><span class="line"><span class="comment">     * 2、分区也排序-----累计求和</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    joinDF</span><br><span class="line">      <span class="comment">//计算总分</span></span><br><span class="line">      .withColumn(<span class="string">&quot;sumSco&quot;</span>, sum($<span class="string">&quot;sco&quot;</span>) over <span class="type">Window</span>.partitionBy($<span class="string">&quot;id&quot;</span>))</span><br><span class="line">      <span class="comment">//按总分排名</span></span><br><span class="line">      .orderBy($<span class="string">&quot;sumSco&quot;</span>.desc)</span><br><span class="line">      <span class="comment">//取top</span></span><br><span class="line">      .limit(<span class="number">60</span>)</span><br><span class="line">    <span class="comment">//.show(60)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 3、统计每科都及格的学生</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    scoreDF</span><br><span class="line">      <span class="comment">//关联科目表</span></span><br><span class="line">      .join(subjectDF, <span class="string">&quot;cid&quot;</span>)</span><br><span class="line">      <span class="comment">//1过滤不及格的分数</span></span><br><span class="line">      .where($<span class="string">&quot;sco&quot;</span> &gt;= $<span class="string">&quot;ssco&quot;</span> * <span class="number">0.6</span>)</span><br><span class="line">      <span class="comment">//统计每个学生几个的科目数</span></span><br><span class="line">      .withColumn(<span class="string">&quot;jige&quot;</span>, count($<span class="string">&quot;sid&quot;</span>) over <span class="type">Window</span>.partitionBy($<span class="string">&quot;sid&quot;</span>))</span><br><span class="line">      <span class="comment">//取出都及格的学生</span></span><br><span class="line">      .where($<span class="string">&quot;jige&quot;</span> === <span class="number">6</span>)</span><br><span class="line">    <span class="comment">//.show(100)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 2、统计总分大于年级平均分的学生</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    joinDF</span><br><span class="line">      <span class="comment">//计算总分</span></span><br><span class="line">      .withColumn(<span class="string">&quot;sumSco&quot;</span>, sum($<span class="string">&quot;sco&quot;</span>) over <span class="type">Window</span>.partitionBy($<span class="string">&quot;id&quot;</span>))</span><br><span class="line">      <span class="comment">//计算年级平均分</span></span><br><span class="line">      .withColumn(<span class="string">&quot;avgSco&quot;</span>, avg($<span class="string">&quot;sumSco&quot;</span>) over <span class="type">Window</span>.partitionBy(substring($<span class="string">&quot;clazz&quot;</span>, <span class="number">0</span>, <span class="number">2</span>)))</span><br><span class="line">      .where($<span class="string">&quot;sumSco&quot;</span> &gt; $<span class="string">&quot;avgSco&quot;</span>)</span><br><span class="line">    <span class="comment">//.show(1000)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 统计每个班级每个名次分数差</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * lag:取当前行前面的数据，需要分区和排序</span></span><br><span class="line"><span class="comment">     * lead 取当前行后面的数据，需要分区和排序</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    joinDF</span><br><span class="line">      <span class="comment">//按照学号和班级分组</span></span><br><span class="line">      .groupBy($<span class="string">&quot;id&quot;</span>, $<span class="string">&quot;clazz&quot;</span>)</span><br><span class="line">      <span class="comment">//计算总分</span></span><br><span class="line">      .agg(sum($<span class="string">&quot;sco&quot;</span>) as <span class="string">&quot;sumSco&quot;</span>)</span><br><span class="line">      <span class="comment">//增加排名</span></span><br><span class="line">      .withColumn(<span class="string">&quot;r&quot;</span>, row_number() over <span class="type">Window</span>.partitionBy($<span class="string">&quot;clazz&quot;</span>).orderBy($<span class="string">&quot;sumSco&quot;</span>.desc))</span><br><span class="line">      <span class="comment">//取前一个名次的总分</span></span><br><span class="line">      .withColumn(<span class="string">&quot;headSumSco&quot;</span>, lag($<span class="string">&quot;sumSco&quot;</span>, <span class="number">1</span>, <span class="number">750</span>) over <span class="type">Window</span>.partitionBy($<span class="string">&quot;clazz&quot;</span>).orderBy($<span class="string">&quot;sumSco&quot;</span>.desc))</span><br><span class="line">      <span class="comment">//计算分数差</span></span><br><span class="line">      .withColumn(<span class="string">&quot;cha&quot;</span>, $<span class="string">&quot;headSumSco&quot;</span> - $<span class="string">&quot;sumSco&quot;</span>)</span><br><span class="line">      .show(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="8、练习"><a href="#8、练习" class="headerlink" title="8、练习"></a>8、练习</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Column</span>, <span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo8Burks</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">&quot;burk&quot;</span>)</span><br><span class="line">      .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">      .config(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="number">1</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取数据</span></span><br><span class="line">    <span class="keyword">val</span> burksDF: <span class="type">DataFrame</span> = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;,&quot;</span>)</span><br><span class="line">      .schema(<span class="string">&quot;burk STRING,year STRING,tsl01 DOUBLE,tsl02 DOUBLE,tsl03 DOUBLE,tsl04 DOUBLE,tsl05 DOUBLE,tsl06 DOUBLE,tsl07 DOUBLE,tsl08 DOUBLE,tsl09 DOUBLE,tsl10 DOUBLE,tsl11 DOUBLE,tsl12 DOUBLE&quot;</span>)</span><br><span class="line">      .load(<span class="string">&quot;data/burks.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    burksDF.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 1、统计每个公司每年按月累计收入  行转列 --&gt; sum窗口函数</span></span><br><span class="line"><span class="comment">     * 输出结果</span></span><br><span class="line"><span class="comment">     * 公司代码,年度,月份,当月收入,累计收入</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * sql</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    burksDF.createOrReplaceTempView(<span class="string">&quot;burks&quot;</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |select</span></span><br><span class="line"><span class="string">        |burk,year,month,plc,</span></span><br><span class="line"><span class="string">        |sum(plc) over(partition by burk,year order by month) as leiji</span></span><br><span class="line"><span class="string">        | from (</span></span><br><span class="line"><span class="string">        |select burk,year,month,plc</span></span><br><span class="line"><span class="string">        |from burks</span></span><br><span class="line"><span class="string">        |lateral view explode(map(1,tsl01,2,tsl02,3,tsl03,4,tsl04,5,tsl05,6,tsl06,7,tsl07,8,tsl08,9,tsl09,10,tsl10,11,tsl11,12,tsl12)) T as month,plc</span></span><br><span class="line"><span class="string">        |) as a</span></span><br><span class="line"><span class="string">        |</span></span><br><span class="line"><span class="string">        |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line">    <span class="comment">// .show(100)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * DSL</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> m: <span class="type">Column</span> = map(</span><br><span class="line">      expr(<span class="string">&quot;1&quot;</span>), $<span class="string">&quot;tsl01&quot;</span>,</span><br><span class="line">      expr(<span class="string">&quot;2&quot;</span>), $<span class="string">&quot;tsl02&quot;</span>,</span><br><span class="line">      expr(<span class="string">&quot;3&quot;</span>), $<span class="string">&quot;tsl03&quot;</span>,</span><br><span class="line">      expr(<span class="string">&quot;4&quot;</span>), $<span class="string">&quot;tsl04&quot;</span>,</span><br><span class="line">      expr(<span class="string">&quot;5&quot;</span>), $<span class="string">&quot;tsl05&quot;</span>,</span><br><span class="line">      expr(<span class="string">&quot;6&quot;</span>), $<span class="string">&quot;tsl06&quot;</span>,</span><br><span class="line">      expr(<span class="string">&quot;7&quot;</span>), $<span class="string">&quot;tsl07&quot;</span>,</span><br><span class="line">      expr(<span class="string">&quot;8&quot;</span>), $<span class="string">&quot;tsl08&quot;</span>,</span><br><span class="line">      expr(<span class="string">&quot;9&quot;</span>), $<span class="string">&quot;tsl09&quot;</span>,</span><br><span class="line">      expr(<span class="string">&quot;10&quot;</span>), $<span class="string">&quot;tsl10&quot;</span>,</span><br><span class="line">      expr(<span class="string">&quot;11&quot;</span>), $<span class="string">&quot;tsl11&quot;</span>,</span><br><span class="line">      expr(<span class="string">&quot;12&quot;</span>), $<span class="string">&quot;tsl12&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    burksDF</span><br><span class="line">      <span class="comment">//一行转换成多行</span></span><br><span class="line">      .select($<span class="string">&quot;burk&quot;</span>, $<span class="string">&quot;year&quot;</span>, explode(m) as <span class="type">Array</span>(<span class="string">&quot;month&quot;</span>, <span class="string">&quot;plc&quot;</span>))</span><br><span class="line">      <span class="comment">//计算按月累计</span></span><br><span class="line">      .withColumn(<span class="string">&quot;leiji&quot;</span>, sum($<span class="string">&quot;plc&quot;</span>) over <span class="type">Window</span>.partitionBy($<span class="string">&quot;burk&quot;</span>, $<span class="string">&quot;year&quot;</span>).orderBy($<span class="string">&quot;month&quot;</span>))</span><br><span class="line">      .show(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h5 id="9、Spark-Sql"><a href="#9、Spark-Sql" class="headerlink" title="9、Spark Sql"></a>9、Spark Sql</h5><p><strong>spark-sql  写代码方式</strong></p>
<h6 id="1、idea里面将代码编写好打包上传到集群中运行，上线使用"><a href="#1、idea里面将代码编写好打包上传到集群中运行，上线使用" class="headerlink" title="1、idea里面将代码编写好打包上传到集群中运行，上线使用"></a>1、idea里面将代码编写好打包上传到集群中运行，上线使用</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--conf spark.sql.shuffle.partitions=1 -- 设置spark sqlshuffle之后分区数据马，和代码里面设置是一样的，代码中优先级高</span><br><span class="line">spark-submit提交</span><br><span class="line">spark-submit --master yarn-client --class com.shujia.spark.sql.Demo9Submit --conf spark.sql.shuffle.partitions=1 spark-1.0.jar </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h6 id="2、spark-shell-repl-里面使用sqlContext-测试使用，简单任务使用"><a href="#2、spark-shell-repl-里面使用sqlContext-测试使用，简单任务使用" class="headerlink" title="2、spark shell  (repl) 里面使用sqlContext     测试使用，简单任务使用"></a>2、spark shell  (repl) 里面使用sqlContext     测试使用，简单任务使用</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">spark-shell --master yarn-client</span><br><span class="line">不能使用yarn-cluster Driver必须再本地启动</span><br></pre></td></tr></table></figure>

<h6 id="3、spark-sql-spark-sql-–master-yarn-client-不能使用yarn-cluster-和hive的命令行一样，直接写sql"><a href="#3、spark-sql-spark-sql-–master-yarn-client-不能使用yarn-cluster-和hive的命令行一样，直接写sql" class="headerlink" title="3、spark-sql    spark-sql –master yarn-client   不能使用yarn-cluster    和hive的命令行一样，直接写sql"></a>3、spark-sql    spark-sql –master yarn-client   不能使用yarn-cluster    和hive的命令行一样，直接写sql</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">在spark-sql时完全兼容hive sql的</span><br><span class="line">spark-sql底层使用的时spark进行计算的</span><br><span class="line">hive 底层使用的是MR进行计算的</span><br></pre></td></tr></table></figure>

<h5 id="10、spark-sql整合hive"><a href="#10、spark-sql整合hive" class="headerlink" title="10、spark sql整合hive"></a>10、spark sql整合hive</h5><blockquote>
<p>在spark sql中使用hive的元数据</p>
<p>spark sql是使用spark进行计算的，hive使用MR进行计算的</p>
</blockquote>
<h6 id="1、在hive的hive-site-xml修改一行配置，增加了这一行配置之后，以后在使用hive之前都需要先启动元数据服务"><a href="#1、在hive的hive-site-xml修改一行配置，增加了这一行配置之后，以后在使用hive之前都需要先启动元数据服务" class="headerlink" title="1、在hive的hive-site.xml修改一行配置，增加了这一行配置之后，以后在使用hive之前都需要先启动元数据服务"></a>1、在hive的hive-site.xml修改一行配置，增加了这一行配置之后，以后在使用hive之前都需要先启动元数据服务</h6><p>cd &#x2F;usr&#x2F;local&#x2F;soft&#x2F;hive-1.2.1&#x2F;conf&#x2F;</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://master:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h6 id="2、启动hive元数据服务-将hvie的元数据暴露给第三方使用"><a href="#2、启动hive元数据服务-将hvie的元数据暴露给第三方使用" class="headerlink" title="2、启动hive元数据服务, 将hvie的元数据暴露给第三方使用"></a>2、启动hive元数据服务, 将hvie的元数据暴露给第三方使用</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nohup  hive --service metastore &gt;&gt; metastore.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<h6 id="3、将hive-site-xml-复制到spark-conf目录下"><a href="#3、将hive-site-xml-复制到spark-conf目录下" class="headerlink" title="3、将hive-site.xml  复制到spark conf目录下"></a>3、将hive-site.xml  复制到spark conf目录下</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp hive-site.xml /usr/local/soft/spark-2.4.5/conf/</span><br></pre></td></tr></table></figure>

<h6 id="4、-将mysql-驱动包复制到spark-jars目录下"><a href="#4、-将mysql-驱动包复制到spark-jars目录下" class="headerlink" title="4、 将mysql 驱动包复制到spark jars目录下"></a>4、 将mysql 驱动包复制到spark jars目录下</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /usr/local/soft/hive-1.2.1/lib</span><br><span class="line">cp mysql-connector-java-5.1.49.jar /usr/local/soft/spark-2.4.5/jars/</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h6 id="5、整合好之后在spark-sql-里面就可以使用hive的表了"><a href="#5、整合好之后在spark-sql-里面就可以使用hive的表了" class="headerlink" title="5、整合好之后在spark-sql 里面就可以使用hive的表了"></a>5、整合好之后在spark-sql 里面就可以使用hive的表了</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">模式是<span class="built_in">local</span>模式</span></span><br><span class="line">spark-sql -conf  spark.sql.shuffle.partitions=2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用yarn-client模式</span></span><br><span class="line">spark-sql --master yarn-client  --conf  spark.sql.shuffle.partitions=2</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">在spark-sql中设置运行参数</span></span><br><span class="line">set spark.sql.shuffle.partitions=2;</span><br></pre></td></tr></table></figure>

<h6 id="spark-sql-e"><a href="#spark-sql-e" class="headerlink" title="spark-sql -e"></a>spark-sql -e</h6><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 执行一条sql语句，执行完，自动退出</span></span><br><span class="line">spark<span class="operator">-</span><span class="keyword">sql</span> <span class="operator">-</span>e &quot;select * from student&quot;</span><br></pre></td></tr></table></figure>

<h6 id="spark-sql-f"><a href="#spark-sql-f" class="headerlink" title="spark-sql -f"></a>spark-sql -f</h6><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">vim a.sql</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student</span><br><span class="line"><span class="comment">-- 执行一个sql文件</span></span><br><span class="line">spark<span class="operator">-</span><span class="keyword">sql</span> <span class="operator">-</span>f a.sql</span><br></pre></td></tr></table></figure>

<h6 id="当spark-sql-和hive整合好之后再代码中也可以直接使用hive的表"><a href="#当spark-sql-和hive整合好之后再代码中也可以直接使用hive的表" class="headerlink" title="当spark-sql 和hive整合好之后再代码中也可以直接使用hive的表"></a>当spark-sql 和hive整合好之后再代码中也可以直接使用hive的表</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">.builder()</span><br><span class="line">.appName(<span class="string">&quot;onhive&quot;</span>)</span><br><span class="line">.enableHiveSupport() <span class="comment">//开启hive的元数据支持，在代码中读取hive的元数据</span></span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">//读取hie的表</span></span><br><span class="line"><span class="keyword">val</span> studentDF = spark.talbe(<span class="string">&quot;studnet&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//写好的代码不能再本地运行， 需要打包上传到集群运行</span></span><br></pre></td></tr></table></figure>

<h6 id="spark-sql和hvie的建表语句一样"><a href="#spark-sql和hvie的建表语句一样" class="headerlink" title="spark sql和hvie的建表语句一样"></a>spark sql和hvie的建表语句一样</h6><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student</span><br><span class="line">(</span><br><span class="line">id  string,</span><br><span class="line">name string,</span><br><span class="line">age <span class="type">int</span>,</span><br><span class="line">gender string,</span><br><span class="line">clazz string</span><br><span class="line">)</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">STORED <span class="keyword">AS</span> textfile</span><br><span class="line">location <span class="string">&#x27;/data/student/&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score</span><br><span class="line">(</span><br><span class="line">student_id  string,</span><br><span class="line">cource_id string,</span><br><span class="line">sco <span class="type">int</span></span><br><span class="line">)</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">STORED <span class="keyword">AS</span> textfile</span><br><span class="line">location <span class="string">&#x27;/data/score/&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h6 id="禁用集群spark日志"><a href="#禁用集群spark日志" class="headerlink" title="禁用集群spark日志"></a>禁用集群spark日志</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /usr/local/soft/spark-2.4.5/conf</span><br><span class="line">mv log4j.properties.template log4j.properties</span><br><span class="line">vim log4j.properties</span><br><span class="line">修改配置</span><br><span class="line">log4j.rootCategory=ERROR, console</span><br></pre></td></tr></table></figure>

<h5 id="11、spark-sql和hive区别"><a href="#11、spark-sql和hive区别" class="headerlink" title="11、spark sql和hive区别"></a>11、spark sql和hive区别</h5><h6 id="1、spark-sql缓存"><a href="#1、spark-sql缓存" class="headerlink" title="1、spark sql缓存"></a>1、spark sql缓存</h6><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 进入spark sql命令行</span></span><br><span class="line">spark<span class="operator">-</span><span class="keyword">sql</span></span><br><span class="line"><span class="comment">-- 可以通过一个网址访问spark任务</span></span><br><span class="line">http:<span class="operator">/</span><span class="operator">/</span>master:<span class="number">4040</span></span><br><span class="line"><span class="comment">-- 设置并行度</span></span><br><span class="line"><span class="keyword">set</span> spark.sql.shuffle.partitions<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 再spark-sql中对同一个表进行多次查询的时候可以将表缓存起来</span></span><br><span class="line">cache <span class="keyword">table</span> student;</span><br><span class="line"><span class="comment">-- 删除缓存</span></span><br><span class="line">uncache <span class="keyword">table</span> student;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 再代码中也可以缓存DF</span></span><br><span class="line"> studentDF.persist(StorageLevel.MEMORY_ONLY)</span><br></pre></td></tr></table></figure>

<h6 id="2、spark-sql-mapjoin-—-广播变量"><a href="#2、spark-sql-mapjoin-—-广播变量" class="headerlink" title="2、spark sql mapjoin    — 广播变量"></a>2、spark sql mapjoin    — 广播变量</h6><h5 id="Reduce-Join"><a href="#Reduce-Join" class="headerlink" title="Reduce Join"></a>Reduce Join</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> </span><br><span class="line">student <span class="keyword">as</span> a</span><br><span class="line"><span class="keyword">join</span> </span><br><span class="line">score <span class="keyword">as</span> b</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">a.id<span class="operator">=</span>b.student_id</span><br></pre></td></tr></table></figure>

<h5 id="MapJoin"><a href="#MapJoin" class="headerlink" title="MapJoin"></a>MapJoin</h5><blockquote>
<p>当一个大表关联小表的时候可以将小表加载到内存中进行关联—- 广播变量</p>
<p>在map端进行表关联，不会产生shuffle</p>
</blockquote>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+broadcast(a)  */</span> <span class="operator">*</span> <span class="keyword">from</span> </span><br><span class="line">student <span class="keyword">as</span> a</span><br><span class="line"><span class="keyword">join</span> </span><br><span class="line">score <span class="keyword">as</span> b</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">a.id<span class="operator">=</span>b.student_id</span><br></pre></td></tr></table></figure>

<blockquote>
<p>&#x2F;*+broadcast(a)  *&#x2F;   HINT:给sql加提示的语法</p>
</blockquote>
<h5 id="12、Spaark-JDBC"><a href="#12、Spaark-JDBC" class="headerlink" title="12、Spaark JDBC"></a>12、Spaark JDBC</h5><h6 id="1、开启hive的元数据服务"><a href="#1、开启hive的元数据服务" class="headerlink" title="1、开启hive的元数据服务"></a>1、开启hive的元数据服务</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nohup hive --service metastore &gt;&gt; metastore.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<h6 id="2、开启spark-jdbc-服务"><a href="#2、开启spark-jdbc-服务" class="headerlink" title="2、开启spark jdbc  服务"></a>2、开启spark jdbc  服务</h6><blockquote>
<p>saprkjdbc服务使用的就是hive的jdbc服务</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /usr/local/soft/spark-2.4.5/sbin/</span><br><span class="line">./start-thriftserver.sh --master yarn-client</span><br></pre></td></tr></table></figure>

<h6 id="3、使用命令链接spark-sql-jdbc服务"><a href="#3、使用命令链接spark-sql-jdbc服务" class="headerlink" title="3、使用命令链接spark sql  jdbc服务"></a>3、使用命令链接spark sql  jdbc服务</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /usr/local/soft/spark-2.4.5/bin/</span><br><span class="line">./beeline </span><br><span class="line">输入</span><br><span class="line">!connect jdbc:hive2://master:10000</span><br><span class="line"></span><br><span class="line">设置sparkshuffle并行度</span><br><span class="line">set spark.sql.shuffle.partitions=2;</span><br></pre></td></tr></table></figure>

<h6 id="4、使用scala代码远程链接spark-sql-jdbc服务"><a href="#4、使用scala代码远程链接spark-sql-jdbc服务" class="headerlink" title="4、使用scala代码远程链接spark sql jdbc服务"></a>4、使用scala代码远程链接spark sql jdbc服务</h6><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">-- 1、在maven增加增加jdbc驱动</span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">-- 2、编写java jdbc代码</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">秋水一色</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2022/07/15/Spark-Sql/">http://example.com/2022/07/15/Spark-Sql/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Notes</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2022/07/25/dyYNo7VTn5Bw6lc.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/07/18/FLink%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2022/07/25/BPEbZelx9kQV5Sc.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">FLink集群搭建</div></div></a></div><div class="next-post pull-right"><a href="/2022/07/12/spark%E6%90%AD%E5%BB%BA/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2022/07/25/BPEbZelx9kQV5Sc.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Spark搭建</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/07/11/spark%E4%BB%A3%E7%A0%81/" title="Spark代码"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2022/07/25/dyYNo7VTn5Bw6lc.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-11</div><div class="title">Spark代码</div></div></a></div><div><a href="/2022/07/20/spark%E4%BC%98%E5%8C%96/" title="spark优化"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2022/07/25/dyYNo7VTn5Bw6lc.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-20</div><div class="title">spark优化</div></div></a></div><div><a href="/2022/07/12/spark%E6%90%AD%E5%BB%BA/" title="Spark搭建"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2022/07/25/BPEbZelx9kQV5Sc.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-12</div><div class="title">Spark搭建</div></div></a></div><div><a href="/2022/07/31/spark%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/" title="spark参数优化"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2022/07/25/BPEbZelx9kQV5Sc.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-31</div><div class="title">spark参数优化</div></div></a></div><div><a href="/2022/07/10/spark%E6%A6%82%E8%BF%B0/" title="Spark概述"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2022/07/25/dyYNo7VTn5Bw6lc.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-10</div><div class="title">Spark概述</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2022/03/30/mD5ZV7wdUBGl62S.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">秋水一色</div><div class="author-info__description">春风得意马蹄疾,一日看尽长安花</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">43</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/hzh4015"><i class="fab fa-github"></i><span>一起学习，加油！</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/hzh4015" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:hzh4015@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=2597845702&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">这是我的第一个笔记</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%E3%80%81sparkWordCount"><span class="toc-number">1.</span> <span class="toc-text">1、sparkWordCount</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%E3%80%81WslWordCount"><span class="toc-number">2.</span> <span class="toc-text">2、WslWordCount</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%E3%80%81DslAPI"><span class="toc-number">3.</span> <span class="toc-text">3、DslAPI</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4%E3%80%81DataSource"><span class="toc-number">4.</span> <span class="toc-text">4、DataSource</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5%E3%80%81RDDToDF"><span class="toc-number">5.</span> <span class="toc-text">5、RDDToDF</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#6%E3%80%81DFToRDD"><span class="toc-number">6.</span> <span class="toc-text">6、DFToRDD</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#7%E3%80%81%E5%BC%80%E7%AA%97"><span class="toc-number">7.</span> <span class="toc-text">7、开窗</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#8%E3%80%81%E7%BB%83%E4%B9%A0"><span class="toc-number">8.</span> <span class="toc-text">8、练习</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9%E3%80%81Spark-Sql"><span class="toc-number">9.</span> <span class="toc-text">9、Spark Sql</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1%E3%80%81idea%E9%87%8C%E9%9D%A2%E5%B0%86%E4%BB%A3%E7%A0%81%E7%BC%96%E5%86%99%E5%A5%BD%E6%89%93%E5%8C%85%E4%B8%8A%E4%BC%A0%E5%88%B0%E9%9B%86%E7%BE%A4%E4%B8%AD%E8%BF%90%E8%A1%8C%EF%BC%8C%E4%B8%8A%E7%BA%BF%E4%BD%BF%E7%94%A8"><span class="toc-number">9.1.</span> <span class="toc-text">1、idea里面将代码编写好打包上传到集群中运行，上线使用</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2%E3%80%81spark-shell-repl-%E9%87%8C%E9%9D%A2%E4%BD%BF%E7%94%A8sqlContext-%E6%B5%8B%E8%AF%95%E4%BD%BF%E7%94%A8%EF%BC%8C%E7%AE%80%E5%8D%95%E4%BB%BB%E5%8A%A1%E4%BD%BF%E7%94%A8"><span class="toc-number">9.2.</span> <span class="toc-text">2、spark shell  (repl) 里面使用sqlContext     测试使用，简单任务使用</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3%E3%80%81spark-sql-spark-sql-%E2%80%93master-yarn-client-%E4%B8%8D%E8%83%BD%E4%BD%BF%E7%94%A8yarn-cluster-%E5%92%8Chive%E7%9A%84%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%B8%80%E6%A0%B7%EF%BC%8C%E7%9B%B4%E6%8E%A5%E5%86%99sql"><span class="toc-number">9.3.</span> <span class="toc-text">3、spark-sql    spark-sql –master yarn-client   不能使用yarn-cluster    和hive的命令行一样，直接写sql</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#10%E3%80%81spark-sql%E6%95%B4%E5%90%88hive"><span class="toc-number">10.</span> <span class="toc-text">10、spark sql整合hive</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1%E3%80%81%E5%9C%A8hive%E7%9A%84hive-site-xml%E4%BF%AE%E6%94%B9%E4%B8%80%E8%A1%8C%E9%85%8D%E7%BD%AE%EF%BC%8C%E5%A2%9E%E5%8A%A0%E4%BA%86%E8%BF%99%E4%B8%80%E8%A1%8C%E9%85%8D%E7%BD%AE%E4%B9%8B%E5%90%8E%EF%BC%8C%E4%BB%A5%E5%90%8E%E5%9C%A8%E4%BD%BF%E7%94%A8hive%E4%B9%8B%E5%89%8D%E9%83%BD%E9%9C%80%E8%A6%81%E5%85%88%E5%90%AF%E5%8A%A8%E5%85%83%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1"><span class="toc-number">10.1.</span> <span class="toc-text">1、在hive的hive-site.xml修改一行配置，增加了这一行配置之后，以后在使用hive之前都需要先启动元数据服务</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2%E3%80%81%E5%90%AF%E5%8A%A8hive%E5%85%83%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1-%E5%B0%86hvie%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE%E6%9A%B4%E9%9C%B2%E7%BB%99%E7%AC%AC%E4%B8%89%E6%96%B9%E4%BD%BF%E7%94%A8"><span class="toc-number">10.2.</span> <span class="toc-text">2、启动hive元数据服务, 将hvie的元数据暴露给第三方使用</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3%E3%80%81%E5%B0%86hive-site-xml-%E5%A4%8D%E5%88%B6%E5%88%B0spark-conf%E7%9B%AE%E5%BD%95%E4%B8%8B"><span class="toc-number">10.3.</span> <span class="toc-text">3、将hive-site.xml  复制到spark conf目录下</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#4%E3%80%81-%E5%B0%86mysql-%E9%A9%B1%E5%8A%A8%E5%8C%85%E5%A4%8D%E5%88%B6%E5%88%B0spark-jars%E7%9B%AE%E5%BD%95%E4%B8%8B"><span class="toc-number">10.4.</span> <span class="toc-text">4、 将mysql 驱动包复制到spark jars目录下</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#5%E3%80%81%E6%95%B4%E5%90%88%E5%A5%BD%E4%B9%8B%E5%90%8E%E5%9C%A8spark-sql-%E9%87%8C%E9%9D%A2%E5%B0%B1%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8hive%E7%9A%84%E8%A1%A8%E4%BA%86"><span class="toc-number">10.5.</span> <span class="toc-text">5、整合好之后在spark-sql 里面就可以使用hive的表了</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#spark-sql-e"><span class="toc-number">10.6.</span> <span class="toc-text">spark-sql -e</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#spark-sql-f"><span class="toc-number">10.7.</span> <span class="toc-text">spark-sql -f</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%BD%93spark-sql-%E5%92%8Chive%E6%95%B4%E5%90%88%E5%A5%BD%E4%B9%8B%E5%90%8E%E5%86%8D%E4%BB%A3%E7%A0%81%E4%B8%AD%E4%B9%9F%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8hive%E7%9A%84%E8%A1%A8"><span class="toc-number">10.8.</span> <span class="toc-text">当spark-sql 和hive整合好之后再代码中也可以直接使用hive的表</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#spark-sql%E5%92%8Chvie%E7%9A%84%E5%BB%BA%E8%A1%A8%E8%AF%AD%E5%8F%A5%E4%B8%80%E6%A0%B7"><span class="toc-number">10.9.</span> <span class="toc-text">spark sql和hvie的建表语句一样</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%A6%81%E7%94%A8%E9%9B%86%E7%BE%A4spark%E6%97%A5%E5%BF%97"><span class="toc-number">10.10.</span> <span class="toc-text">禁用集群spark日志</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#11%E3%80%81spark-sql%E5%92%8Chive%E5%8C%BA%E5%88%AB"><span class="toc-number">11.</span> <span class="toc-text">11、spark sql和hive区别</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1%E3%80%81spark-sql%E7%BC%93%E5%AD%98"><span class="toc-number">11.1.</span> <span class="toc-text">1、spark sql缓存</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2%E3%80%81spark-sql-mapjoin-%E2%80%94-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="toc-number">11.2.</span> <span class="toc-text">2、spark sql mapjoin    — 广播变量</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Reduce-Join"><span class="toc-number">12.</span> <span class="toc-text">Reduce Join</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapJoin"><span class="toc-number">13.</span> <span class="toc-text">MapJoin</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#12%E3%80%81Spaark-JDBC"><span class="toc-number">14.</span> <span class="toc-text">12、Spaark JDBC</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1%E3%80%81%E5%BC%80%E5%90%AFhive%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1"><span class="toc-number">14.1.</span> <span class="toc-text">1、开启hive的元数据服务</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2%E3%80%81%E5%BC%80%E5%90%AFspark-jdbc-%E6%9C%8D%E5%8A%A1"><span class="toc-number">14.2.</span> <span class="toc-text">2、开启spark jdbc  服务</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3%E3%80%81%E4%BD%BF%E7%94%A8%E5%91%BD%E4%BB%A4%E9%93%BE%E6%8E%A5spark-sql-jdbc%E6%9C%8D%E5%8A%A1"><span class="toc-number">14.3.</span> <span class="toc-text">3、使用命令链接spark sql  jdbc服务</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#4%E3%80%81%E4%BD%BF%E7%94%A8scala%E4%BB%A3%E7%A0%81%E8%BF%9C%E7%A8%8B%E9%93%BE%E6%8E%A5spark-sql-jdbc%E6%9C%8D%E5%8A%A1"><span class="toc-number">14.4.</span> <span class="toc-text">4、使用scala代码远程链接spark sql jdbc服务</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/08/19/SQL%E7%AC%94%E8%AF%95%E9%A2%98/" title="无题"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2022/07/25/dyYNo7VTn5Bw6lc.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2022/08/19/SQL%E7%AC%94%E8%AF%95%E9%A2%98/" title="无题">无题</a><time datetime="2022-08-19T00:57:49.909Z" title="发表于 2022-08-19 08:57:49">2022-08-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/02/DolphinScheduler%E6%90%AD%E5%BB%BA/" title="DolphinScheduler搭建"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2022/07/05/3zJOcWHCiKjMVZT.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="DolphinScheduler搭建"/></a><div class="content"><a class="title" href="/2022/08/02/DolphinScheduler%E6%90%AD%E5%BB%BA/" title="DolphinScheduler搭建">DolphinScheduler搭建</a><time datetime="2022-08-01T16:00:00.000Z" title="发表于 2022-08-02 00:00:00">2022-08-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/31/spark%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/" title="spark参数优化"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2022/07/25/BPEbZelx9kQV5Sc.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="spark参数优化"/></a><div class="content"><a class="title" href="/2022/07/31/spark%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/" title="spark参数优化">spark参数优化</a><time datetime="2022-07-30T16:00:00.000Z" title="发表于 2022-07-31 00:00:00">2022-07-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/25/kafka%E4%BB%A3%E7%A0%81/" title="Kafka代码"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2022/07/25/BPEbZelx9kQV5Sc.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Kafka代码"/></a><div class="content"><a class="title" href="/2022/07/25/kafka%E4%BB%A3%E7%A0%81/" title="Kafka代码">Kafka代码</a><time datetime="2022-07-25T14:44:58.432Z" title="发表于 2022-07-25 22:44:58">2022-07-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/25/Kafka%E6%90%AD%E5%BB%BA/" title="Kafka搭建"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2022/07/25/dyYNo7VTn5Bw6lc.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Kafka搭建"/></a><div class="content"><a class="title" href="/2022/07/25/Kafka%E6%90%AD%E5%BB%BA/" title="Kafka搭建">Kafka搭建</a><time datetime="2022-07-25T14:24:58.265Z" title="发表于 2022-07-25 22:24:58">2022-07-25</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://s2.loli.net/2022/07/25/dyYNo7VTn5Bw6lc.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 秋水一色</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>